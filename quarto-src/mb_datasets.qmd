---
title: "MB Datasets"
format: html
---

Here you'll find the MB (Pacific Ocean) processing scripts for both chlorophyll-a and SST. 

They ingest raw Level-2 swath files, apply global coverage and quality filters, and grid valid observations into daily NetCDF products. These daily outputs are then averaged over rolling multi-day windows—automatically rebuilding composites whenever new data arrive.

Below are the individual scripts that form the MB workflow. Use the expandable code blocks to explore each stage of the pipeline.

## Chlorophyll
### makeChla1daynewMB

`makeChla1daynewMB.py` ingests Level-2 chlorophyll-a swath NetCDFs for the target day (HOD > 10) and early hours of the next day (HOD ≤ 10), filters to “Day” pixels within longitude 120–320° and latitude –45–65°, and interpolates onto a 0.025° × 0.025°regular grid. The resulting 1-day composite is land-masked, written as a CF-compliant NetCDF, and deployed automatically to a remote server directory. Find the 1-day product on ERDDAP [here](modisa_scripts.qmd#modisa-products-on-swfsc-erddap-server).

The following chart summarizes the script's workflow:
```{mermaid}
%%{init: {"flowchart":{"htmlLabels":true}}}%%
flowchart LR
  I("<b>Inputs</b><br/>• Raw L2 Chla NetCDF swaths (dataDir)<br/>• workDir<br/>• Year & DOY")
    --> P("<b>Processing</b><br/>• Compute dates & directories<br/>• Load land mask<br/>• Discover swaths for day & next-day<br/>• Stage & filter swaths (day-only, region overlap)<br/>• Extract, reshape & filter variables<br/>• Interpolate onto regular grid<br/>• Apply mask")
    --> O("<b>Output</b><br/>• CF-compliant 1-day composite Chla NetCDF<br/>• Deployed to /MB/chla/1day/")
```


```{python}
#| eval: false
#| code-fold: true
#| code-summary: "View makeChla1daynewMB.py"

"""
Overview
--------
Generate a one-day ocean-color (Chla) grid for the MODIS “MB” (Pacific Ocean) product by
combining swaths from Level-2 chlorophyll-a NetCDF files and gridding them with PyGMT. The
script produces a CF-compliant NetCDF file of daily Chla for the region (lon 120-320°, lat -45-65°).

Usage
-----
::
  
     python makeChla1daynewMB.py <dataDirBase> <workDir> <year> <doy>

Where:

- ``dataDir``

  Root folder containing raw Level-2 ocean-color NetCDF swath files, organized as ``<dataDirBase>/<YYYY><MM>/``. Each swath must match: ``AQUA_MODIS.<YYYY><MM><DD>*L2.OC.NRT.nc``.

- ``workDir``

  Temporary working directory for staging swath copies and intermediate files.

- ``year``

  Four-digit year string (e.g., ``"2025"``).

- ``doy``

  Three-digit day-of-year string (zero-padded, e.g., ``"082"`` for March 23).

Description
-----------
1. **Date and Directory Setup**  

     - Computes calendar date from ``year`` + ``doy``, zero-pads month/day, and sets ``datadir = dataDirBase + YYYY + MM + "/"``.  
     
     - Also computes the next day's directory for early-morning swaths (``HOD ≤ 10``).

2. **Load Land Mask**  

     - Reads a static GRD land-mask from ``/u00/ref/landmasks/LM_120_320_0.025_-45_65_0.025_gridline.grd`` into ``my_mask``.

3. **Swath Discovery & Staging**

     - Globs ``AQUA_MODIS.<YYYY><MM><DD>*.L2.OC.NRT.nc`` in both current and next-day folders.

     - Copies swaths into ``workDir``, removing any old ``AQUA_MODIS.*L2.OC*`` or ``MB20*`` files.

4. **Swath Processing**

     For each swath:

         - Extracts navigation (``lon``, ``lat``), converts negative longitudes to 0-360°, and tests overlap with the region (lon 120-320°, lat -45-65°).

     - Filters only daytime pixels (``day_night_flag == "Day"``).

     - Reads chlorophyll-a (``chlor_a``), reshapes into column vectors, and stacks into ``(lon, lat, Chla)``.

     - Applies sequential filters:  

         • ``Chla > 0``

         • ``lonmin ≤ lon ≤ lonmax``

         • ``latmin ≤ lat ≤ latmax``

         • Discards any obviously invalid longitudes (``> -400``).

     - Accumulates valid points into ``temp_data`` and tracks provenance in ``filesUsed``.

5. **Gridding & NetCDF Generation**  

     - Uses ``pygmt.xyz2grd`` on ``temp_data`` with region ``120/320/-45/65`` and spacing ``0.025/0.025`` to produce a GMT ``.grd`` file named ``MB<YYYY><DDD>_<YYYY><DDD>_chla.grd``.
     
     - Converts the grid to a CF-compliant NetCDF via ``roylib.grd2netcdf1``, applying ``my_mask``.
     
     - Sends the final NetCDF into ``/MB/chla/1day/`` via ``roylib.send_to_servers``.

Dependencies
------------
- **Python 3.x**

- **Standard library**: ``os``, ``sys``, ``datetime``, ``timedelta``, ``glob``, ``re``, ``shutil``

- **Third-party**: ``netCDF4.Dataset``, ``numpy``, ``numpy.ma``, ``pygmt``  

- **Custom roylib functions**:  

     - ``myReshape(array)``

     - ``grd2netcdf1(grd, outName, filesUsed, mask, fType)``

     - ``safe_remove(filePath)``  

     - ``send_to_servers(ncFile, destDir, interval)``

     - ``isleap(year)``

     - ``makeNetcdf(mean, nobs, interval, outFile, filesUsed, workDir)``

     - ``meanVar(mean, num, obs)``

Land Mask
---------
A static GRD mask is required at:

 ``/u00/ref/landmasks/LM_120_320_0.025_-45_65_0.025_gridline.grd``  

to zero-out land pixels in the daily Chla grid.

Directory Structure
-------------------
- **Input directory** (datadir):

  ``<dataDirBase>/<YYYY><MM>/`` containing raw L2 OC files: ``AQUA_MODIS.<YYYY><MM><DD>*.L2.OC.NRT.nc``

- **Working directory** (workDir):

  Temporary staging area; cleared of ``AQUA_MODIS.*L2.OC*`` and ``MB20*`` at start.

- **Output grid** (fileOut):

  GMT “.grd” file named ``MB<YYYY><DDD>_<YYYY><DDD>_chla.grd``

- **Final NetCDF**:

  ``MB<YYYY><DDD>_<YYYY><DDD>_chla.nc``, copied to: ``/path/to/modis_data/modisgf/chla/1day/``

Usage Example
-------------
::  
  
     python makeChla1daynewMB.py /Users/you/modis_data/netcdf/ /Users/you/modis_work/ 2025 082

This will:

  - Download and stage all Level-2 OC swaths for March 23, 2025 (``HOD > 10`` and next day ``HOD ≤ 10``).

  - Build and grid the daily chlorophyll-a point cloud for the region.

  - Generate CF-compliant NetCDF and deploy to ``/modisgf/chla/1day/``.
"""
from __future__ import print_function
from builtins import str

if __name__ == "__main__":
    from datetime import datetime, timedelta
    import glob
    from itertools import chain
    from netCDF4 import Dataset
    import numpy as np
    import numpy.ma as ma
    import pygmt
    import os
    import re
    import shutil
    import sys

    # Ensure 'roylib' is on the import path
    sys.path.append('/home/cwatch/pythonLibs')
    from roylib import *

    # Geographic bounds for MB region
    latmax = 65.
    latmin = -45.
    lonmax = 320.
    lonmin = 120.

    outFile = 'modisgfChlatemp'

    # Set data directory
    datadirBase = sys.argv[1]

    # Set work directory
    workdir = sys.argv[2]

    # Get the year and doy from the command line
    year = sys.argv[3]
    doy = sys.argv[4]
    print(year)
    print(doy)

    # Convert year/doy to a calendar date and zero-pad month/day
    myDate = datetime(int(year), 1, 1) + timedelta(int(doy) - 1)
    myMon = str(myDate.month)
    myMon = myMon.rjust(2, '0')
    myDay = str(myDate.day).rjust(2, '0')

    # Construct the directory path where raw swaths are stored for this date
    datadir = datadirBase + year + myMon + '/'

    # Next calendar day (for hours ≤ 10)
    myDate1 = myDate + timedelta(days=1)
    year1 = str(myDate1.year)
    doy1 = myDate1.strftime("%j").zfill(3)
    myMon1 = str(myDate1.month)
    myMon1 = myMon1.rjust(2, '0')
    myDay1 = str(myDate1.day).rjust(2, '0')
    datadir1 = datadirBase + year1 + myMon1 + '/'

    # Load static land mask from GRD
    mask_root = Dataset('/u00/ref/landmasks/LM_120_320_0.025_-45_65_0.025_gridline.grd')
    my_mask = mask_root.variables['z'][:, :]
    mask_root.close()

    # Now move to the data directory
    os.chdir(datadir)

    # Set up the string for the file search in the data directory
    #myString = 'A' + year + doy + '*.L2_LAC_OC.nc'
    myString = 'AQUA_MODIS.' + year + myMon + myDay + '*.L2.OC.NRT.nc'
    print(myString)

    # Get list of files in the data directory that match with full path
    fileList = glob.glob(myString)
    fileList.sort()

    # Now move to the work directory and clear old files
    os.chdir(workdir)
    os.system('rm -f AQUA_MODIS.*L2.OC*')
    os.system('rm -f MB20*')

    # Initialize variables to accumulate data and track provenance
    filesUsed = ""
    temp_data = None

    # Loop over swaths for hod > 10 
    for fName in fileList:
        fileName = fName
        # Extract hour of day from filename
        datetime = re.search('AQUA_MODIS.(.+?).L2.OC.NRT.nc', fileName)
        hodstr = datetime.group(1)[9:11]
        hod = int(hodstr)

        if (hod > 10):
            print(hod)
            print(fileName)

            # Stage the swath in the working directory
            shutil.copyfile(datadir + fName, workdir + fName)

            # Try to open the NetCDF; skip if the file is unreadable
            try:
                rootgrp = Dataset(fileName, 'r')
            except IOError:
                print("bad file " + fileName)
                continue

            # Extract navigation-group data
            navDataGroup = rootgrp.groups['navigation_data']
            latitude = navDataGroup.variables['latitude'][:, :]
            longitude = navDataGroup.variables['longitude'][:, :]

            # Convert any negative longitudes to the 0-360° domain
            longitude[longitude < 0] = longitude[longitude < 0] + 360

            # Compute swath extents (min/max) for geographic filtering
            dataLonMin = np.nanmin(longitude[longitude >= 0])
            dataLonMax = np.nanmax(longitude[longitude <= 360])
            dataLatMin = np.nanmin(latitude[latitude >= -90])
            dataLatMax = np.nanmax(latitude[latitude <= 90])

            # Determine if swath overlaps our the MW region
            goodLon1 = (dataLonMin < lonmin) and (dataLonMax >= lonmin)
            goodLon2 = (dataLonMin >= lonmin) and (dataLonMin <= lonmax)
            goodLon = goodLon1 or goodLon2

            goodLat1 = (dataLatMin < latmin) and (dataLatMax >= latmin)
            goodLat2 = (dataLatMin >= latmin) and (dataLatMin <= latmax)
            goodLat = goodLat1 or goodLat2

            # Check if swath is daytime (only keep "Day" pixels)
            dayNightTest = (rootgrp.day_night_flag == 'Day')

            # Only proceed if geography and day-night tests pass
            if (goodLon and goodLat and dayNightTest):
            # if(goodLon and goodLat):
                if (len(filesUsed) == 0):
                    filesUsed = fileName
                else:
                    filesUsed = filesUsed + ', ' + fileName

                # Reshape latitude & longitude arrays into column vectors
                latitude = myReshape(latitude)
                longitude = myReshape(longitude)

                # Access geophysical data group
                geoDataGroup = rootgrp.groups['geophysical_data']

                # Extract chlor_a
                chlor_a = geoDataGroup.variables['chlor_a'][:, :]
                chlor_a = myReshape(chlor_a)

                # Stack (lon, lat, chlor_a) into a single 2D array with shape (N, 3)
                dataOut = np.hstack((longitude, latitude, chlor_a))

                # Filter rows to keep only valid chlor_a (>0)
                dataOut = dataOut[dataOut[:, 2] > 0]
                dataOut = dataOut[dataOut[:, 0] > -400]
                dataOut = dataOut[dataOut[:, 0] >= lonmin]
                dataOut = dataOut[dataOut[:, 0] <= lonmax]
                dataOut = dataOut[dataOut[:, 1] >= latmin]
                dataOut = dataOut[dataOut[:, 1] <= latmax]

                # Accumulate into temp_data
                if(dataOut.shape[0] > 0):
                    if(temp_data is None):
                        temp_data = dataOut
                    else:
                        temp_data = np.concatenate((temp_data, dataOut), axis=0)

            # Clean up this swath
            rootgrp.close()
            safe_remove(fileName)

    # Repeat for hod ≤ 10 on the next day

    # Move back to data dir
    os.chdir(datadir1)

    # Set up the string for file matching of doy+1
    myString = 'AQUA_MODIS.' + year1 + myMon1 + myDay1 + '*.L2.OC.NRT.nc'
    fileList = glob.glob(myString)
    fileList.sort()

    # Change back to work directory
    os.chdir(workdir)

    for fName in fileList:
        fileName = fName

        # Find the datatime group in the filename
        datetime = re.search('AQUA_MODIS.(.+?).L2.OC.NRT.nc', fileName)
        hodstr = datetime.group(1)[9:11]
        hod = int(hodstr)
        if (hod <= 10):
            print(hod)
            print(fileName)

            # cp file from work directory to
            shutil.copyfile(datadir1 + fName, workdir + fName)
            try:
                rootgrp = Dataset(fileName, 'r')
            except IOError:
                print("bad file " + fileName)
                continue

            #rootgrp = Dataset(fileName, 'r')

            navDataGroup = rootgrp.groups['navigation_data']
            latitude = navDataGroup.variables['latitude'][:, :]
            longitude = navDataGroup.variables['longitude'][:, :]

            longitude[longitude < 0] = longitude[longitude < 0] + 360

            dataLonMin = np.nanmin(longitude[longitude >= 0])
            dataLonMax = np.nanmax(longitude[longitude <= 360])
            dataLatMin = np.nanmin(latitude[latitude >= -90] )
            dataLatMax = np.nanmax(latitude[latitude <= 90] )

            goodLon1 = (dataLonMin < lonmin) and ( dataLonMax >= lonmin)
            goodLon2 = (dataLonMin >= lonmin) and (dataLonMin <= lonmax)
            goodLon = goodLon1 or goodLon2

            goodLat1 = (dataLatMin < latmin) and (dataLatMax >= latmin)
            goodLat2 = (dataLatMin >= latmin) and (dataLatMin <= latmax)
            goodLat = goodLat1 or goodLat2

            dayNightTest = (rootgrp.day_night_flag == 'Day')

            if (goodLon and goodLat and dayNightTest):
                if (len(filesUsed) == 0):
                    filesUsed = fileName
                else:
                    filesUsed = filesUsed + ', ' + fileName

                latitude = myReshape(latitude)
                longitude = myReshape(longitude)

                geoDataGroup = rootgrp.groups['geophysical_data']

                chlor_a = geoDataGroup.variables['chlor_a'][:, :]
                chlor_a = myReshape(chlor_a)

                dataOut = np.hstack((longitude, latitude, chlor_a))

                dataOut = dataOut[dataOut[:, 2] > 0]
                dataOut = dataOut[dataOut[:, 0] > -400]
                dataOut = dataOut[dataOut[:, 0] >= lonmin]
                dataOut = dataOut[dataOut[:, 0] <= lonmax]
                dataOut = dataOut[dataOut[:, 1] >= latmin]
                dataOut = dataOut[dataOut[:, 1] <= latmax]

                if(dataOut.shape[0] > 0):
                    if(temp_data is None):
                        temp_data = dataOut
                    else:
                        temp_data = np.concatenate((temp_data, dataOut), axis=0)

            rootgrp.close()
            safe_remove(fileName)

    # Grid the combined Chla point cloud and write outputs
    fileOut = 'MB' + year + doy + '_' + year + doy + '_chla.grd'
    range = '120/320/-45/65'
    increment = '0.025/0.025'
    temp_data1 = pygmt.xyz2grd(
        data=temp_data,
        region=range,
        spacing=increment,
    )

    # Convert the GMT grid to CF-compliant NetCDF, masking land
    ncFile = grd2netcdf1(temp_data1, fileOut, filesUsed, my_mask, 'MB')

    #myCmd = "mv " + ncFile + " /home/cwatch/pygmt_test/outfiles"
    #os.system(myCmd)

    # Copy to the MB server folder for chla (1-day product)
    send_to_servers(ncFile, '/MB/chla/' , '1')
    os.remove(ncFile)

```

### CompMBChla

`CompMBChla.py` reads the daily 1-day chlorophyll-a NetCDF files over a user-defined interval, accumulates running sum and count arrays on a 0.025° × 0.025° regular grid spanning longitude 120°–320° and latitude –45°–65°, then masks out cells with no observations and writes the averaged multi-day (3-, 5-, 8-, or 14-day) composite as a CF-compliant NetCDF. It finishes by uploading the final composite to a directory on a remote server. Find the multi-day products on ERDDAP [here](modisa_scripts.qmd#modisa-products-on-swfsc-erddap-server).

The following chart summarizes the script's workflow:
```{mermaid}
%%{init: {"flowchart":{"htmlLabels":true}}}%%
flowchart LR
  I("<b>Inputs</b><br/>• Daily 1-day Chla NetCDFs (dataDir)<br/>• workDir<br/>• endYear & endDoy<br/>• interval (days)")
    --> P("<b>Processing</b><br/>• Compute start & end dates<br/>• Initialize mean/num arrays<br/>• Gather daily files over interval<br/>• Loop: open each file, extract & squeeze variable<br/>• Update running mean & count<br/>• Mask pixels with zero observations")
    --> O("<b>Output</b><br/>• CF-compliant multi-day composite Chla NetCDF<br/>• Deployed to /MB/chla/")
```

```{python}
#| eval: false
#| code-fold: true
#| code-summary: "View CompMBChla.py"

"""
Overview
--------
Compute multi-day (m-day) chlorophyll-a composites for the MODIS “MB” (Pacific Ocean)
dataset by averaging 1-day NetCDF files over a specified range of days.

Usage
-----
::
  
      python CompMBChla.py <dataDir> <workDir> <endYear> <endDoy> <interval>

Where:

- ``dataDir``

  Directory containing daily 1-day NetCDFs named ``MB<YYYY><DDD>*chla.nc``.

- ``workDir``

  Temporary working directory for intermediate files.

- ``endYear``

  Four-digit year of the last day in the composite (e.g., ``2025``).

- ``endDoy``

  Three-digit day-of-year of the last day (e.g., ``082``).

- ``interval``

  Number of days to include (e.g., `3`, `5`, `8`, `14`).


Description
-----------
1. **Parse arguments & compute interval**

  - Read ``dataDir``, ``workDir``, ``endYear``, ``endDoy``, and ``interval`` from ``sys.argv``.

  - Compute ``startDoy = endDoy - interval + 1``.

2. **Compute start/end dates**

  - Convert ``endYear`` + ``endDoy`` to a ``datetime``.

  - Derive ``startDoy`` date by subtracting ``interval - 1`` days.

3. **Initialize accumulators**

  - Change to ``workDir`` and clear any old files.

  - Preallocate two arrays of shape (4401x8001):

     - ``mean`` (float32) for the running sum of chlorophyll-a.

     - ``num``  (int32) for the count of valid observations.

4. **Gather daily files**

  - Build a list of all ``MB<YYYY><DDD>*chla.nc`` files from ``startDoy`` to ``endDoy``.

  - Handle wrap-around year boundaries by splitting into two ranges if needed.

5. **Accumulate daily Chla**
  
  - For each file:

     - Open with ``Dataset()``, read the 4-D variable ``MBchla``, squeeze to 2-D.

     - Update ``mean`` and ``num`` via ``meanVar(mean, num, data2d)``.

6. **Mask & finalize**

  - Create a masked array where ``num == 0`` (no observations), fill value ``-9999999.``.

7. **Write & deploy** 

  - Change to ``workDir``.
  
  - Construct output filename ``MB<startYear><startDDD>_<endYear><endDDD>_chla.nc``.
  
  - Call ``makeNetcdf(mean, num, interval, outFile, filesUsed, workDir)`` to write the CF-compliant NetCDF.
  
  - Transfer it to ``/MB/chla/`` on the server via ``send_to_servers()``.

Dependencies
------------
- Python 3.x

- Standard library: ``os``, ``sys``, ``glob``, ``itertools.chain``, ``datetime``, ``timedelta``

- Third-party: ``numpy``, ``numpy.ma``, ``netCDF4``

- Custom roylib functions:

  - ``isleap(year)``

  - ``meanVar(mean, num, array)``

  - ``makeNetcdf(mean, num, interval, outFile, filesUsed, workDir)``

  - ``send_to_servers(ncFile, remote_dir, interval_flag)``

Directory Structure
-------------------
- **Input Directory** (``dataDir``):

  ``MB<YYYY><DDD>*chla.nc`` daily files.

- **Working Directory** (``workDir``):

  Temporary staging and output location.

- **Output location**:

  Copied to ``/MB/chla/`` with the interval flag.

Usage Example
-------------
Create a 5-day chlorophyll composite DOY 001-005 of 2025
::
 
   python CompMBChla.py /data/modisgf/1day/ /tmp/mw_work/ 2025 005 5

This averages files for DOY 001…005, writes ``MB2025001_2025005_chla.nc`` in /tmp/mw_work/, and uploads to /MB/chla/.
"""
from __future__ import print_function
from builtins import str
from builtins import range

if __name__ == "__main__":
    from datetime import datetime, timedelta
    import glob
    from itertools import chain
    from netCDF4 import Dataset
    import numpy as np
    import numpy.ma as ma
    import os
    import sys

    # Ensure 'roylib' is on the import path
    sys.path.append('/home/cwatch/pythonLibs')
    from roylib import *

    dataDir = sys.argv[1]

    # Temporary working directory
    workDir = sys.argv[2]

    # Composite end year (YYYY)
    endyearC = sys.argv[3]

    # Composite end day-of-year (DDD)
    endDoyC = sys.argv[4]
    endDoyC = endDoyC.rjust(3, '0')
    endDoy = int(endDoyC)

    # Composite length
    intervalC = sys.argv[5]
    interval = int(intervalC)

    # Convert end Doy to calendar date
    myDateEnd = datetime(int(endyearC), 1, 1) + timedelta(int(endDoyC)-1)

    # Start date = end date minus (interval-1) days
    myDateStart = myDateEnd + timedelta(days=-(interval - 1))

    # Zero-padded start Doy and year
    startDoyC = myDateStart.strftime("%j").zfill(3)
    startDoy = int(startDoyC)
    startYearC = str(myDateStart.year)

    # Prepare output directory
    outDir = '/ERDData1/modisa/data/modisgf/' + endyearC + '/'+ intervalC + 'day'

    print(dataDir)
    print(workDir)
    print(endyearC)
    print(endDoyC)
    print(intervalC)

    ###
    # dtypeList = ['chla']
    # for dtype in dtypeList:

    # Data type for composites
    dtype = 'chla'

    # Clean working directory
    os.chdir(workDir)
    os.system('rm -f *')

    # Change to data directory
    os.chdir(dataDir)

    # Preallocate sum (mean) and count arrays matching grid dims
    mean = np.zeros((4401, 8001), np.single)
    num = np.zeros((4401, 8001), dtype=np.int32)

    # Composite within the same calendar year
    if (endDoy > startDoy):
        # Build range of Doys
        doyRange = list(range(startDoy, endDoy + 1))
        fileList = []
        # Gather filenames for each DOY
        for doy in doyRange:
            doyC = str(doy)
            doyC = doyC.rjust(3, '0')
            myString = 'MB' + endyearC + doyC + '*' + dtype + '.nc'
            fileList.append(glob.glob(myString))

        # Flatten and sort file list
        fileList = list(chain.from_iterable(fileList))
        fileList.sort()
        filesUsed = ""
        print(fileList)

        # Loop through files and accumulate data
        for fName in fileList:
            if (len(filesUsed) == 0):
                filesUsed = fName
            else:
                filesUsed = filesUsed + ', ' + fName

            chlaFile = Dataset(fName)
            chla = chlaFile.variables["MBchla"][:, :, :, :]
            chlaFile.close()
            chla = np.squeeze(chla)
            mean, num = meanVar(mean, num, chla)
    else:
        # Composite spans year boundary
        # Determine directory for the start year
        dataDir1 = dataDir
        dataDir1 = dataDir1.replace(endyearC, startYearC)
        # Determine end of start year based on start year
        if (isleap):
            endday = 366
        else:
            endday = 365

        fileList = []
        os.chdir(dataDir1)
        doyRange = list(range(startDoy, endday + 1))
        for doy in doyRange:
            doyC = str(doy)
            doyC = doyC.rjust(3, '0')
            myString = 'MB' + startYearC + doyC + '*' + dtype + '.nc'
            fileList.append(glob.glob(myString))

        fileList = list(chain.from_iterable(fileList))
        fileList.sort()
        filesUsed = ""
        print(fileList)
        for fName in fileList:
            if (len(filesUsed) == 0):
                filesUsed = fName
            else:
                filesUsed = filesUsed + ', ' + fName

            chlaFile = Dataset(fName)
            chla = chlaFile.variables["MBchla"][:, :, :, :]
            chlaFile.close()
            chla = np.squeeze(chla)
            mean, num = meanVar(mean, num, chla)

        # DOY from 1 to endDoy of end year
        os.chdir(dataDir)
        fileList = []
        doyRange = list(range(1, endDoy + 1))
        for doy in doyRange:
            doyC = str(doy)
            doyC = doyC.rjust(3, '0')
            myString = 'MB' + endyearC + doyC + '*' + dtype + '.nc'
            fileList.append(glob.glob(myString))

        fileList = list(chain.from_iterable(fileList))
        fileList.sort()
        print(fileList)
        for fName in fileList:
            if (len(filesUsed) == 0):
                filesUsed = fName
            else:
                filesUsed = filesUsed + ', ' + fName

            # Read the NetCDF, variable 'MBchla' and squeeze to make 2D array (lat x lon)
            chlaFile = Dataset(fName)
            chla = chlaFile.variables["MBchla"][:, :, :, :]
            chlaFile.close()
            chla = np.squeeze(chla)

            # Update running sum and count arrays
            mean, num = meanVar(mean, num, chla)

    # Mask out any grid cells with zero observations, setting them to the fill value
    mean = ma.array(mean, mask=(num==0), fill_value=-9999999.)

    # Switch to the working directory for output operations
    os.chdir(workDir)

    # Construct the output filename with start and end dates plus data types
    outFile = 'MB' + startYearC + startDoyC + '_' + endyearC + endDoyC + '_' + dtype + '.nc'

    # Create multi-day NetCDF file using the mean and count arrays
    ncFile = makeNetcdf(mean, num, interval, outFile, filesUsed, workDir)

    # Upload composite NetCDF to remote MB chla directory, labeling it with the interval
    send_to_servers(ncFile, '/MB/chla/', str(interval))

```

### CompMBChlamday

`CompMBChlamday.py` ingests 1-day composite Chla NetCDF files over user-defined date range and maps them onto a 0.025° × 0.025° regular grid spanning longitude 120°–320° and latitude –45°–65°, accumulating sums and counts to compute the monthly mean. It then masks out points with no observations, writes the result as a CF-compliant NetCDF, and uploads it to a directory on a remote server. Find the monthly product on ERDDAP [here](modisa_scripts.qmd#modisa-products-on-swfsc-erddap-server).

The following chart summarizes the script's workflow:
```{mermaid}
%%{init: { "flowchart": { "htmlLabels": true } }}%%
flowchart LR
  I("<b>Inputs</b><br/>• 1-day Chla NetCDFs (dataDir)<br/>• workDir<br/>• endYear & endDoy<br/>• startDoy")
    --> P("<b>Processing</b><br/>• Parse args & compute interval<br/>• Compute start/end dates<br/>• Clear workDir<br/>• Initialize mean & count arrays<br/>• Gather files over date range<br/>• Loop: read each file & update mean/num<br/>• Mask pixels with no observations")
    --> O("<b>Output</b><br/>• Monthly composite Chla NetCDF<br/>• Deployed to /MB/chla/")
```

```{python}
#| eval: false
#| code-fold: true
#| code-summary: "View CompMBChlamday.py"

"""
Overview
--------
Compute monthly composites of chlorophyll-a (Chla) for the MODIS “MB” (Pacific Ocean)
dataset by averaging daily 1-day NetCDF files over a specified interval of days.

Usage
-----
::
  
    python CompMBChlamday.py <dataDir> <workDir> <endYear> <endDoy> <startDoy>

Where:

- ``dataDir``

  Directory containing daily 1-day NetCDF files named ``MB<YYYY><DDD>*chla.nc``.

- ``workDir``

  Temporary working directory for intermediate files

- ``endYear`` 

  Four-digit year of the last day in the composite (e.g., ``2025``).

- ``endDoy``

  Three-digit day-of-year of the last composite day (e.g., ``031``).

- ``startDoy``

  Three-digit day-of-year of the first composite day (e.g., ``001``).

Description
-----------
1. **Parse arguments & compute interval**

  - Read ``dataDir``, ``workDir``, ``endYear``, ``endDoy``, and ``startDoy``.

  - Compute ``interval = endDoy - startDoy + 1``.

2. **Compute calendar dates**

  - Convert ``endYear`` + ``endDoy`` to a ``datetime`` (``myDateEnd``).

  - Derive ``myDateStart = myDateEnd - (interval - 1) days``.

3. **Initialize accumulators**

  - Clear ``workDir``.

  - Preallocate two arrays of shape (4401x8001):

     - ``mean`` (float32) for the running sum of Chla.

     - ``num``  (int32) for the count of observations.

4. **Gather daily files**

  - Build a list of all ``MB<YYYY><DDD>*chla.nc`` files from ``startDoy`` to ``endDoy``.

  - Handle year-boundary spans by splitting into two date ranges if necessary.

5. **Accumulate daily Chla**

  - For each file:

     - Open via ``Dataset()``.

     - Read the 4-D variable ``MBchla``, squeeze to 2-D.

     - Update ``mean`` and ``num`` via ``meanVar(mean, num, data2d)``.

6. **Mask & finalize**

  - Create a masked array: mask pixels where ``num == 0`` (no observations) and set fill value ``-9999999.``.

7. **Write & deploy**

  - Change to ``workDir``.

  - Construct output filename ``MB<startYear><startDoy>_<endYear><endDoy>_chla.nc``.

  - Call ``makeNetcdfmDay(mean, num, interval, outFile, filesUsed, workDir)`` to produce a CF-compliant NetCDF.

  - Transfer it to ``/MB/chla/`` on the remote server via ``send_to_servers()``.

Dependencies
------------
- Python 3.x

- Standard library: ``os``, ``sys``, ``glob``, ``itertools.chain``, ``datetime``, ``timedelta``

- Third-party: ``numpy``, ``numpy.ma``, ``netCDF4.Dataset``

- Custom roylib functions:

  - ``isleap(year)``

  - ``meanVar(mean, num, data)``

  - ``makeNetcdfmDay(mean, num, interval, outFile, filesUsed, workDir)``

  - ``send_to_servers(ncFile, remote_dir, interval_flag)``


Directory Structure
-------------------
- **Input Directory** (dataDir):

  Contains daily files ``MB<YYYY><DDD>*chla.nc``.

- **Working Directory** (workDir):

  Temporary space cleared and reused each run.

- **Output Location**:

  Final monthly composite sent to ``/MB/chla/``.

Usage Example
-------------
Generate a January 2025 composite (DOY 001-031):

::
 
   python CompMBChlamday.py /data/modisgf/1day/ /tmp/mw_work/ 2025 031 001

This will average daily Chla for DOY 001…031 of 2025, write ``MB2025001_2025031_chla.nc`` in ``/tmp/mw_work/``, and upload to ``/MB/chla/``.
"""
from __future__ import print_function
from builtins import str
from builtins import range

if __name__ == "__main__":
    from datetime import datetime, timedelta
    import glob
    from itertools import chain
    from netCDF4 import Dataset
    import numpy as np
    import numpy.ma as ma
    import os
    import sys

    # Ensure 'roylib' is on the import path
    sys.path.append('/home/cwatch/pythonLibs')
    from roylib import *

    # Directory with 1-day MB chl files
    dataDir = sys.argv[1]

    # Temporary working directory
    workDir = sys.argv[2]

    # Composite end year
    endyearC = sys.argv[3]

    # Same year for start
    startYearC = endyearC

    # Zero-padded end DOY
    endDoyC = sys.argv[4]
    endDoyC = endDoyC.rjust(3, '0')
    endDoy = int(endDoyC)

    # Zero-padded start DOY
    startDoyC = sys.argv[5]
    startDoyC = startDoyC.rjust(3, '0')
    startDoy = int(startDoyC )

    # Number of days in the composite
    interval = endDoy - startDoy + 1

    # Convert end date to calendar dates
    myDateEnd = datetime(int(endyearC), 1, 1) + timedelta(endDoy - 1)
    myDateStart = myDateEnd + timedelta(days=-(interval - 1))

    # Prepare output directory
    outDir = '/ERDData1/modisa/data/modisgf/' + endyearC + '/mday'

    print(dataDir)
    print(workDir)
    print(endyearC)
    print(endDoyC)
    print(interval)

    ###
    # dtypeList = ['chla']
    # for dtype in dtypeList:

    # Data type for this composite run
    dtype = 'chla'

    # Clear the working directory
    os.chdir(workDir)
    os.system('rm -f *')

    # Move to data directory
    os.chdir(dataDir)

    # Preallocate sum (mean) and count arrays matching grid dims
    mean = np.zeros((4401, 8001), np.single)
    num = np.zeros((4401, 8001), dtype=np.int32)

    # Composite entirely within the same year
    if (endDoy > startDoy):
        doyRange = list(range(startDoy, endDoy + 1))
        fileList = []
        # Collect all matching files for each DOY
        for doy in doyRange:
            doyC = str(doy)
            doyC = doyC.rjust(3, '0')
            myString = 'MB' + endyearC + doyC + '*' + dtype + '.nc'
            fileList.append(glob.glob(myString))

        # Flatten and sort the list of lists
        fileList = list(chain.from_iterable(fileList))
        fileList.sort()
        filesUsed = ""
        print(fileList)

        # Loop through each file
        for fName in fileList:
            if (len(filesUsed) == 0):
                filesUsed = fName
            else:
                filesUsed = filesUsed + ', ' + fName

            chlaFile = Dataset(fName)
            chla = chlaFile.variables["MBchla"][:, :, :, :]
            chlaFile.close()
            chla = np.squeeze(chla)

            # Update running mean and count arrays
            mean, num = meanVar(mean, num, chla)
    else:
        # Composite wraps across year boundary
        dataDir1 = dataDir
        dataDir1 = dataDir1.replace(endyearC, startYearC)
        # Determine last DOY of start year based on leap year
        if(isleap):
            endday = 366
        else:
            endday = 365

        # From startDoy through end of start year  
        fileList = []
        os.chdir(dataDir1)
        doyRange = list(range(startDoy, endday + 1))
        for doy in doyRange:
            doyC = str(doy)
            doyC = doyC.rjust(3, '0')
            myString = 'MB' + startYearC + doyC + '*' + dtype + '.nc'
            fileList.append(glob.glob(myString))

        fileList = list(chain.from_iterable(fileList))
        fileList.sort()

        filesUsed = ""
        print(fileList)
        for fName in fileList:
            if (len(filesUsed) == 0):
                filesUsed = fName
            else:
                filesUsed = filesUsed + ', ' + fName

            chlaFile = Dataset(fName)
            chla = chlaFile.variables["MBchla"][:, :, :, :]
            chlaFile.close()
            chla = np.squeeze(chla)
            mean, num = meanVar(mean, num, chla)

        # From DOY 1 of end year through endDoy
        os.chdir(dataDir)
        fileList = []
        doyRange= list(range(1, endDoy + 1))
        for doy in doyRange:
            doyC = str(doy)
            doyC = doyC.rjust(3, '0')
            myString = 'MB' + endyearC + doyC + '*' + dtype + '.nc'
            fileList.append(glob.glob(myString))

        fileList = list(chain.from_iterable(fileList))
        fileList.sort()
        print(fileList)
        for fName in fileList:
            if (len(filesUsed) == 0):
                filesUsed = fName
            else:
                filesUsed = filesUsed + ', ' + fName

            chlaFile = Dataset(fName)
            chla = chlaFile.variables["MBchla"][:, :, :, :]
            chlaFile.close()
            chla = np.squeeze(chla)
            mean, num = meanVar(mean, num, chla)

    # Mask out pixels with zero observations and set fill value for missing data
    mean = ma.array(mean, mask=(num == 0), fill_value=-9999999.)

    # Switch to the working directory
    os.chdir(workDir)

    # Construct output filename
    outFile = 'MB' + startYearC + startDoyC + '_' + endyearC + endDoyC + '_' + dtype + '.nc'

    # Generate the monthly NetCDF file
    ncFile = makeNetcdfmDay(mean, num, interval, outFile, filesUsed, workDir)

    # Upload the composite to remote MB chla directory
    send_to_servers(ncFile, '/MB/chla/', 'm')

```

## SST
### makeSST1daynewMB

`makeSST1daynewMB` makes a 1-day SST composite for the MB dataset by aggregating valid Level-2 swaths from the target day (HOD > 10) and early hours of the following day (HOD ≤ 10), then interpolating onto a 0.025° × 0.025° regular grid spanning longitude 120–320° and latitude –45–65° via PyGMT. It writes out a CF-compliant NetCDF, masks land using a static grid, computes coverage statistics, and uploads the resulting file to a directory on the remote server. Find the 1-day product on ERDDAP [here](modisa_scripts.qmd#modisa-products-on-swfsc-erddap-server).

The following chart summarizes the script's workflow:
```{mermaid}
%%{init: {"flowchart":{"htmlLabels":true}}}%%
flowchart LR
  I("<b>Inputs</b><br/>• Raw L2 SST swaths (dataDir)<br/>• workDir<br/>• Year & DOY")
    --> P("<b>Processing</b><br/>• Compute dates & directories<br/>• Load land mask<br/>• Discover swaths for day & next-day<br/>• Stage & filter swaths (day-only, region overlap)<br/>• Extract, reshape & filter variables<br/>• Interpolate onto regular grid<br/>• Apply mask")
    --> O("<b>Output</b><br/>• CF-compliant 1-day composite SST NetCDF<br/>• Deployed to /MB/sstd/1day/")
```

```{python}
#| eval: false
#| code-fold: true
#| code-summary: "View makeSST1daynewMB.py"

"""
Overview
--------
Generate a one-day SST grid for the MODIS “MB” (Pacific Ocean) dataset by combining swaths
from Level-2 SST NetCDF files over a two-day interval (current day and early hours of the
next day). The output is a CF-compliant NetCDF file containing daily SST on a regular grid
(lon 120-320°, lat -45-65°).

Usage
-----
::

    python makeSST1daynewMB.py <dataDir> <workDir> <year> <doy>

Where:

- ``dataDir``

  Base directory containing raw Level-2 SST NetCDF swaths, organized as ``<dataDirBase>/<YYYY><MM>/``. Each file must match ``AQUA_MODIS.<YYYY><MM><DD>T*.L2.SST.NRT.nc``.

- ``workDir``

  Temporary working directory. Swaths are copied here, intermediate files are created, and then cleaned up.

- ``year``

  Four-digit year string (e.g., ``"2025"``).

- ``doy``

  Three-digit day-of-year string (zero-padded, e.g., ``"082"`` for March 23).

Description
-----------
1. **Date and Directory Setup**

     - Convert ``year`` + ``doy`` to a calendar date (``myDate``), then extract zero-padded month (``myMon``) and day (``myDay``).
     
     - Compute next-day date (``myDate1``) and its year/month/day-of-year (``year1``, ``myMon1``, ``doy1``).
     
     Define:

         - ``datadir = dataDirBase + year + myMon + "/"``  
         - ``datadir1 = dataDirBase + year1 + myMon1 + "/"``

     These folders must contain the SST swath files for the current day and next day, respectively.
     
     - Load a static land-mask grid from ``/u00/ref/landmasks/LM_120_320_0.025_-45_65_0.025_gridline.grd`` into ``my_mask``.

2. **Swath Discovery (Day N, HOD > 10)**  

     - Change directory to ``datadir``.
     
     - Build glob pattern: ``AQUA_MODIS.<year><myMon><myDay>*.L2.SST.NRT.nc``
     
     - Sort matching files into ``fileList``.
     
     - Change into ``workDir``, then remove any stale files matching ``AQUA_MODIS.*L2.SST*`` or ``MB20*``.

3. **Loop Over Each Swath for Day N (HOD > 10)**  

     For each ``fName`` in ``fileList``:

     a. Extract hour-of-day (HOD) from the filename.
     
     b. If ``HOD > 10``, copy the swath from ``datadir`` to ``workDir`` and open with ``netCDF4.Dataset`` (skip if unreadable).
      
     c. Read ``navigation_data`` (``latitude``, ``longitude``), convert negative longitudes to 0-360°. Compute swath extents and test geographic overlap:  

         - Longitude overlaps [120°, 320°]
         
         - Latitude overlaps [-45°, 65°]
         
         - ``day_night_flag == "Day"``
         
         If all true, append the filename to ``filesUsed``.

      d. Reshape variables using ``myReshape`` and read from ``geophysical_data``:

         - ``sst`` (sea surface temperature)
         
         - ``qual_sst`` (quality flag)
        
         Flatten quality flags, stack ``longitude``, ``latitude``, ``sst`` into ``dataOut``, and filter:
         
           - ``qual_sst < 3``
         
           - SST > -2 °C
         
           - Longitude between 120° and 320°
         
           - Latitude ≤ 65°
         
         Accumulate valid points into ``temp_data``.

   e. Close the NetCDF and remove the copied swath from ``workDir`` using ``safe_remove``.

4. **Swath Discovery (Day N+1, HOD ≤ 10)**

     - Change directory to ``datadir1``.
     
     - Build glob pattern: ``AQUA_MODIS.<year1><myMon1><myDay1>*.L2.SST.NRT.nc``
     
     - Sort into ``fileList``.
     
     - Change back into ``workDir``.

5. **Loop Over Each Swath for Day N+1 (HOD ≤ 10)**

     For each ``fName`` in ``fileList``:
     
       - Extract HOD; if ``HOD ≤ 10``, copy swath from ``datadir1`` to ``workDir`` and repeat steps 3b-3e (navigation, quality/filter, accumulate into ``temp_data``).

6. **Gridding Swath Point Cloud**

     - After processing both sets of swaths, define output grid filename: ``MB<year><doy>_<year><doy>_sstd.grd``

     - Set PyGMT region and spacing:

         - ``region = "120/320/-45/65"``

         - ``spacing = "0.025/0.025"``

     - Call:

     ::

         pygmt.xyz2grd(
             data=temp_data,
             region=region,
             spacing=spacing
         )

     to produce a gridded ``xarray.DataArray`` (``temp_data1``).

7. **Convert to NetCDF and Send to Server**

     - Invoke: ``ncFile = grd2netcdf1(temp_data1, fileOut, filesUsed, my_mask, "MB")`` from ``roylib``: 

     - Masks out land using ``my_mask`` (set land to NaN).

     - Computes coverage metrics (# observations, % coverage).

     - Uses a CDL template to generate an empty CF-compliant NetCDF via ``ncgen``.

     - Populates coordinates, SST values, metadata, and a center-time stamp.

     - Returns the final NetCDF filename (``MB<year><doy>_<year><doy>_sstd.nc``).

   - Call:

     ::

          send_to_servers(ncFile, "/MB/sstd/", "1")

     to copy the NetCDF into ``/MB/sstd/1day/``.

     Finally, remove the local NetCDF copy.

Dependencies
------------
- **Python 3.x**

- **Standard library:** ``os``, ``glob``, ``re``, ``shutil``, ``sys``, ``datetime``, ``timedelta``, ``chain``

- **Third-party**: ``netCDF4.Dataset``, ``numpy``, ``numpy.ma``, ``pygmt``  

- **Custom roylib functions:**

   - ``myReshape(array)``
  
   - ``grd2netcdf1(grd, outName, filesUsed, mask, fType)``

   - ``safe_remove(filePath)``  

   - ``send_to_servers(ncFile, destDir, interval)``

   - ``isleap(year)``

   - ``makeNetcdf(mean, nobs, interval, outFile, filesUsed, workDir)``

   - ``meanVar(mean, num, obs)``

Land Mask
---------
- A static GRD file is required at:

  ``/u00/ref/landmasks/LM_120_320_0.025_-45_65_0.025_gridline.grd``

Directory Structure
-------------------
- **Input swaths directory (Day N)**:

  ``<dataDirBase>/<YYYY><MM>/`` containing files named ``AQUA_MODIS.<YYYY><MM><DD>T*.L2.SST.NRT.nc``.

- **Input swaths directory (Day N+1)**:
  
  ``<dataDirBase>/<YYYY1><MM1>/`` containing the early-hour swaths.

- **Working directory** (workDir): 
  
  Temporary location where swaths are copied, processed, and removed.

- **Output grid** (fileOut):  
  
  A GMT “.grd” file named ``MB<year><doy>_<year><doy>_sstd.grd``.

- **Final NetCDF** (returned by ``grd2netcdf1``): 
  
  Named ``MB<year><doy>_<year><doy>_sstd.nc``, then copied to ``/MB/sstd/1day/``.

Usage Example
-------------
Assume your raw SST swaths are in ``/Users/you/modis_data/netcdf/202503/`` and your working directory is ``/Users/you/modis_work/``

::

    python makeSST1daynewMB.py /Users/you/modis_data/netcdf/ \
                              /Users/you/modis_work/ \
                              2025 082

This will:

  - Copy all swaths from March 23 with HOD > 10 into “/Users/you/modis_work/”.

  - Copy early swaths from March 24 with HOD ≤ 10 into “/Users/you/modis_work/”.

  - Build a combined point cloud from valid “Day” pixels for 120-320°, lat -45-65°).

  - Create “MB2025082_2025082_sstd.grd” via PyGMT ``xyz2grd``.

  - Convert the grid to “MB2025082_2025082_sstd.nc” using ``grd2netcdf1``.

  - Copy the NetCDF to “/MB/sstd/1day/” and remove local copies.
"""
from __future__ import print_function
from builtins import str

if __name__ == "__main__":
    from datetime import datetime, timedelta
    import glob
    from itertools import chain
    from netCDF4 import Dataset
    import numpy as np
    import numpy.ma as ma
    import pygmt
    import os
    import re
    import shutil
    import sys

    # Ensure 'roylib' is on the import path
    sys.path.append('/home/cwatch/pythonLibs')
    from roylib import *

    # Geographic bounds for MW region
    latmax = 65.
    latmin = -45.
    lonmax = 320.
    lonmin = 120.

    outFile = 'modisgfSSTtemp'

    # Set data directory
    datadirBase = sys.argv[1]

    # Set work directory
    workdir = sys.argv[2]

    # Get the year and doy from the command line
    year = sys.argv[3]
    doy = sys.argv[4]
    print(year)
    print(doy)

    # Convert year/doy to a calendar date and zero-pad month/day
    myDate = datetime(int(year), 1, 1) + timedelta(int(doy) - 1)
    myMon = str(myDate.month)
    myMon = myMon.rjust(2, '0')
    myDay = str(myDate.day).rjust(2, '0')

    # Directory for day N's raw swaths: <datadirBase>/<year>/<myMon>/
    datadir = datadirBase + year + myMon + '/'

    # Compute actual calendar date from year + day
    myDate1 = myDate + timedelta(days=1)
    year1 = str(myDate1.year)
    doy1 = myDate1.strftime("%j").zfill(3)
    myMon1 = str(myDate1.month)
    myMon1 = myMon1.rjust(2, '0')
    myDay1 = str(myDate1.day).rjust(2, '0')
    print(myDate)
    print(myDate1)
    print(year1)
    print(myMon1)

    # Directory for day N + 1's raw swaths: <datadirBase>/<year1>/<myMon1>/
    datadir1 = datadirBase + year1 + myMon1 + '/'

    # Load static land mask grid from GRD
    mask_root = Dataset('/u00/ref/landmasks/LM_120_320_0.025_-45_65_0.025_gridline.grd')
    my_mask = mask_root.variables['z'][:, :]
    mask_root.close()

    # Now move to the data directory
    os.chdir(datadir)

    # Set up the string for the file search in the data directory
    myString = 'AQUA_MODIS.' + year + myMon + myDay + '*.L2.SST.NRT.nc'
    print(myString)

    # Get list of files in the data directory that match with full path
    fileList = glob.glob(myString)
    # print(fileList)
    fileList.sort()

    # Now move to the work directory and clear old files
    os.chdir(workdir)
    os.system('rm -f AQUA_MODIS.*L2.SST*')
    os.system('rm -f MB20*')

    # Prepare variables to accumulate point-cloud data and track provenance
    temp_data = None
    filesUsed = ""

    # Loop through each swath filename for Day N
    for fName in fileList:
        fileName = fName

        # Extract Hour-Of-Day (hod) from filename via
        datetime = re.search('AQUA_MODIS.(.+?).L2.SST.NRT.nc', fileName)
        hodstr = datetime.group(1)[9:11]
        hod = int(hodstr)

        # Only process swaths acquired after hod > 10 
        if (hod > 10):
            print(hod)
            print(fileName)

            # Copy swath from datadir into workdir for processing
            shutil.copyfile(datadir + fName, workdir + fName)

            # Attempt to open the NetCDF; skip if unreadable
            try:
                rootgrp = Dataset(fileName, 'r')
            except IOError:
                print("bad file " + fileName)
                continue

            # Extract navigation-group data
            navDataGroup = rootgrp.groups['navigation_data']
            latitude = navDataGroup.variables['latitude'][:, :]
            longitude = navDataGroup.variables['longitude'][:, :]

            # Convert any negative longitudes to the 0-360° domain
            longitude[longitude < 0] = longitude[longitude < 0] + 360

            # Compute swath extents (min/max) for geographic filtering
            dataLonMin = np.nanmin(longitude[longitude >= 0])
            dataLonMax = np.nanmax(longitude[longitude <= 360])
            dataLatMin = np.nanmin(latitude[latitude >= -90])
            dataLatMax = np.nanmax(latitude[latitude <= 90])

            # Determine if swath overlaps our MB region (lon 120-320, lat -45-65)
            goodLon1 = (dataLonMin < lonmin) and (dataLonMax >= lonmin)
            goodLon2 = (dataLonMin >= lonmin) and (dataLonMin <= lonmax)
            goodLon = goodLon1 or goodLon2

            goodLat1 = (dataLatMin < latmin) and (dataLatMax >= latmin)
            goodLat2 = (dataLatMin >= latmin) and (dataLatMin <= latmax)
            goodLat = goodLat1 or goodLat2

            # Check if swath is daytime (only keep "Day" pixels)
            dayNightTest = (rootgrp.day_night_flag == 'Day')

            # Only proceed if geography and day-night tests pass
            if (goodLon and goodLat and dayNightTest):
                # Add filename to provenance list
                if (len(filesUsed) == 0):
                    filesUsed = fileName
                else:
                    filesUsed = filesUsed + ', ' + fileName

                # Extract geophysical data and reshape
                longitude = myReshape(longitude)
                latitude = myReshape(latitude)
                geoDataGroup = rootgrp.groups['geophysical_data']
                sst = geoDataGroup.variables['sst'][:, :]
                sst = myReshape(sst)
                qual_sst = geoDataGroup.variables['qual_sst'][:, :]
                qual_sst = myReshape(qual_sst)

                # Filter out low-quality pixels (qual_sst < 3)
                qualTest = qual_sst < 3
                qualTest = qual_sst < 3
                qualTest = qualTest.flatten()

                # Stack (lon, lat, sst) into a single 2D array with shape (N, 3)
                dataOut = np.hstack((longitude, latitude, sst))

                # Apply quality mask, valid SST > -2°C and geographic bounds
                dataOut = dataOut[qualTest]
                dataOut = dataOut[dataOut[:, 2] > -2]
                dataOut = dataOut[dataOut[:, 0] > -400]
                dataOut = dataOut[dataOut[:, 0] >= lonmin]
                dataOut = dataOut[dataOut[:, 0] <= lonmax]
                dataOut = dataOut[dataOut[:, 1] <= latmax]

                # Accumulate into temp_data array
                if (dataOut.shape[0] > 0):
                    if (temp_data is None):
                        temp_data = dataOut
                    else:
                        temp_data = np.concatenate((temp_data, dataOut), axis=0)

            # Close the NetCDF and remove swath from workdir
            rootgrp.close()
            safe_remove(fileName)

    # Process swaths for Day N+1 (hod ≤ 10)
    # Move back to data dir
    print(datadir1)
    os.chdir(datadir1)

    # Set up the string for file matching of doy+1
    myString = 'AQUA_MODIS.' + year1 + myMon1 + myDay1  + '*.L2.SST.NRT.nc'
    fileList = glob.glob(myString)
    fileList.sort()

    # Switch back to workdir for processing
    os.chdir(workdir)
    for fName in fileList:
        fileName = fName

        # Extract hod again
        datetime = re.search('AQUA_MODIS.(.+?).L2.SST.NRT.nc', fileName)
        hodstr = datetime.group(1)[9:11]
        hod = int(hodstr)

        # Only process early swaths (hod ≤ 10) from Day N+1
        if (hod <= 10):
            print(hod)
            print(fileName)

            # cp file from work directory to
            shutil.copyfile(datadir1 + fName, workdir + fName)
            try:
                rootgrp = Dataset(fileName, 'r')
            except IOError:
                print("bad file " + fileName)
                continue

            navDataGroup = rootgrp.groups['navigation_data']
            latitude = navDataGroup.variables['latitude'][:, :]
            longitude = navDataGroup.variables['longitude'][:, :]
            longitude[longitude < 0] = longitude[longitude < 0] + 360
            dataLonMin = np.nanmin(longitude[longitude >= 0])
            dataLonMax = np.nanmax(longitude[longitude <= 360])
            dataLatMin = np.nanmin(latitude[latitude >= -90] )
            dataLatMax = np.nanmax(latitude[latitude <= 90] )

            goodLon1 = (dataLonMin < lonmin) and ( dataLonMax >= lonmin)
            goodLon2 = (dataLonMin >= lonmin) and (dataLonMin <= lonmax)
            goodLon = goodLon1 or goodLon2
            goodLat1 = (dataLatMin < latmin) and (dataLatMax >= latmin)
            goodLat2 = (dataLatMin >= latmin) and (dataLatMin <= latmax)
            goodLat = goodLat1 or goodLat2

            dayNightTest = (rootgrp.day_night_flag == 'Day')

            if (goodLon and goodLat and dayNightTest):
                if (len(filesUsed) == 0):
                    filesUsed = fileName
                else:
                    filesUsed = filesUsed + ', ' + fileName

                longitude = myReshape(longitude)
                latitude = myReshape(latitude)
                geoDataGroup = rootgrp.groups['geophysical_data']
                sst = geoDataGroup.variables['sst'][:, :]
                sst = myReshape(sst)
                qual_sst = geoDataGroup.variables['qual_sst'][:, :]
                qual_sst = myReshape(qual_sst)
                qualTest = qual_sst < 3
                qualTest = qualTest.flatten()

                dataOut = np.hstack((longitude, latitude, sst))
                dataOut = dataOut[qualTest]
                dataOut = dataOut[dataOut[:, 2] > -2]
                dataOut = dataOut[dataOut[:, 0] > -400]
                dataOut = dataOut[dataOut[:, 0] >= lonmin]
                dataOut = dataOut[dataOut[:, 0] <= lonmax]
                dataOut = dataOut[dataOut[:, 1] >= latmin]
                dataOut = dataOut[dataOut[:, 1] <= latmax]
    
                if (dataOut.shape[0] > 0):
                    if (temp_data is None):
                        temp_data = dataOut
                    else:
                        temp_data = np.concatenate((temp_data, dataOut), axis=0)

            # Close the NetCDF and remove swath from workdir
            rootgrp.close()
            safe_remove(fileName)

    # Build GMT grid from accumulated point cloud
    # Define output grid filename for PyGMT
    fileOut = 'MB' + year + doy + '_' + year + doy + '_sstd.grd'
    range = '120/320/-45/65'
    increment = '0.025/0.025'

    # Use PyGMT xyz2grd to interpolate scattered (lon, lat, sst) into a regular grid
    temp_data1 = pygmt.xyz2grd(
        data=temp_data,
        region=range,
        spacing=increment,
    )

    # Convert GMT grid to CF-compliant NetCDF and send to server
    ncFile = grd2netcdf1(temp_data1, fileOut, filesUsed, my_mask, 'MB')

    #myCmd = "mv " + ncFile + " /home/cwatch/pygmt_test/outfiles"
    #os.system(myCmd)

    # Copy the resulting NetCDF to the "MB" server folder for 1-day products
    send_to_servers(ncFile, '/MB/sstd/', '1')

    # Remove local NetCDF copy from working directory
    os.remove(ncFile)

```

### CompMBSST

`CompMBSST.py` makes the 3, 5, 8, 14-day SST composites by averaging 1-day MB SST NetCDF files over  user-specified interval of days, accumulates running sum and count arrays on a 0.025° × 0.025° regular grid spanning longitude 120°–320° and latitude –45°–65°. It accumulates running sums and counts, masks out unobserved pixels, writes a CF-compliant NetCDF, and uploads the result to a directory on the remote server. Find the multi-day products on ERDDAP [here](modisa_scripts.qmd#modisa-products-on-swfsc-erddap-server).

The following chart summarizes the script's workflow:
```{mermaid}
%%{init: {"flowchart":{"htmlLabels":true}}}%%
flowchart LR
  I("<b>Inputs</b><br/>• 1-day SST NetCDFs (dataDir)<br/>• workDir<br/>• endYear & endDoy<br/>• interval (days)")
    --> P("<b>Processing</b><br/>• Compute start/end dates<br/>• Initialize mean/num arrays<br/>• Gather daily files<br/>• Loop: open each file, extract & squeeze variable<br/>• Update running mean & count<br/>• Mask pixels with zero observations")
    --> O("<b>Output</b><br/>• CF-compliant multi-day composite SST NetCDF<br/>• Deployed to /MB/sstd/")
```

```{python}
#| eval: false
#| code-fold: true
#| code-summary: "View CompMBSST.py"

"""
Overview
--------
Compute multi-day SST composites for the MODIS “MB” (Pacific Ocean) dataset by averaging
daily 1-day NetCDF SST files over a specified interval of days (e.g., 3, 5, 8, or 14).

Usage
-----
::
  
    python CompMBSST.py <dataDir> <workDir> <endYear> <endDoy> <interval>

Where:

- ``dataDir``

  Directory containing daily 1-day SST NetCDF files named ``MB<YYYY><DDD>*sstd.nc``.

- ``workDir``

  Temporary working directory for intermediate files.

- ``endYear``

  Four-digit year of the last day in the composite (e.g., ``2025``).

- ``endDoy``

  Three-digit day-of-year of the last day (e.g., ``082``).

- ``interval``

  Number of days to include (e.g., `3`, `5`, `8`, `14`).

Description
-----------
1. **Parse arguments & compute interval**

  - Read ``dataDir``, ``workDir``, ``endYear``, ``endDoy``, and ``interval`` from ``sys.argv``.

  - Compute ``startDoy = endDoy - interval + 1`` and zero-pad DOY strings.

2. **Compute calendar dates**

  - Convert ``endYear`` + ``endDoy`` to a ``datetime`` (``myDateEnd``).
  
  - Compute ``myDateStart = myDateEnd - (interval - 1)`` days.

3. **Initialize accumulators**

  - Clear ``workDir`` of old files.

  - Preallocate ``mean`` and ``num`` arrays of shape (4401, 8001) for running sum and counts.

4. **Gather daily files**

  - If ``startDoy ≤ endDoy``, collect files for DOYs ``startDoy…endDoy``; else handle wrap-around at year end by collecting ``startDoy…yearEnd`` and ``1…endDoy``.

5. **Accumulate SST**

  - For each NetCDF:

     - Open via ``Dataset()``.

     - Read 4-D variable ``MBsstd``, squeeze to 2-D array.

     - Update ``mean, num`` with ``meanVar(mean, num, sst)``.

6. **Mask and finalize**

  - Mask pixels with zero observations (``num == 0``), fill with ``-9999999.``.

7. **Write & deploy**

  - Change back to ``workDir``.

  - Build ``outFile = MB<startYear><startDoy>_<endYear><endDoy>_sstd.nc``.

  - Call ``makeNetcdf(mean, num, interval, outFile, filesUsed, workDir)`` to write CF-compliant NetCDF.

  - Upload via ``send_to_servers(ncFile, '/MB/sstd/', str(interval))``.

Dependencies
------------
- **Python 3.x**

- **Standard library:** ``os``, ``sys``, ``glob``, ``itertools.chain``, ``datetime``, ``timedelta``

- **Third-party:** ``numpy``, ``numpy.ma``, ``netCDF4.Dataset``

- **Custom roylib functions:**

  - ``isleap(year)``

  - ``meanVar(mean, num, array)``

  - ``makeNetcdf(mean, num, interval, outFile, filesUsed, workDir)``

  - ``send_to_servers(ncFile, remote_dir, interval_flag)``

Directory Structure
-------------------
- **Input Directory** (``dataDir``):

  Contains daily files ``MB<YYYY><DDD>*sstd.nc``.

- **Working Directory** (``workDir``):

  Temporary staging and output location.

- **Output location** (remote):

  Copied to ``/MB/sstd/`` with the interval flag.

Usage Example
-------------
Create a 5-day SST composite from DOY 001 to 005 of 2025:

::
 
   python CompMBSST.py /data/modisgf/1day/ /tmp/mw_work/ 2025 005 5

This will average files MB2025001*… through MB2025005*, write ``MB2025001_2025005_sstd.nc`` in ``/tmp/mw_work/``, and upload to ``/MB/sstd/``.
"""
from __future__ import print_function
from builtins import str
from builtins import range

if __name__ == "__main__":
    from datetime import datetime, timedelta
    import glob
    from itertools import chain
    from netCDF4 import Dataset
    import numpy as np
    import numpy.ma as ma
    import os
    import sys

    # Ensure 'roylib' is on the import path
    sys.path.append('/home/cwatch/pythonLibs')
    from roylib import *

    # Directory with 1-day SST NetCDFs
    dataDir = sys.argv[1]

    # Temporary working directory
    workDir = sys.argv[2]

    # Composite end year (YYYY)
    endyearC = sys.argv[3]

    # Composite end day-of-year (DDD)
    endDoyC = sys.argv[4]
    endDoyC = endDoyC.rjust(3, '0')

    # Integer form of end day-of-year
    endDoy = int(endDoyC)

    # Composite length as string
    intervalC = sys.argv[5]
    interval = int(intervalC)

    # Convert end Doy to calendar date
    myDateEnd = datetime(int(endyearC), 1, 1) + timedelta(int(endDoyC) - 1)

    # Start date = end date minus (interval-1) days
    myDateStart = myDateEnd + timedelta(days=-(interval - 1))

    # Zero-padded start Doy and year
    startDoyC = myDateStart.strftime("%j").zfill(3)
    startDoy = int(startDoyC)
    startYearC = str(myDateStart.year)

    # Prepare output directory
    outDir = '/ERDData1/modisa/data/modisgf/' + endyearC + '/' + intervalC + 'day'

    print(dataDir)
    print(workDir)
    print(endyearC)
    print(endDoyC)
    print(intervalC)
    print(startYearC)
    print(startDoyC)

    ###
    #cdtypeList = ['sstd']
    # for dtype in dtypeList:

    # Data type for SST composites
    dtype = 'sstd'

    # Clean working directory
    os.chdir(workDir)
    os.system('rm -f *')

    # Change to data directory
    os.chdir(dataDir)

    # Preallocate sum (mean) and count arrays matching grid dims
    mean =np.zeros((4401, 8001), np.single)
    num = np.zeros((4401, 8001), dtype=np.int32)

    # Composite within the same calendar year
    if (endDoy > startDoy):
        # Build range of Doys
        doyRange = list(range(startDoy, endDoy + 1))
        fileList = []
        # Gather filenames for each DOY
        for doy in doyRange:
            doyC = str(doy)
            doyC = doyC.rjust(3, '0')
            myString = 'MB' + endyearC + doyC + '*' + dtype + '.nc'
            fileList.append(glob.glob(myString))

        # Flatten and sort file list
        fileList = list(chain.from_iterable(fileList))
        fileList.sort()
        filesUsed = ""
        print(fileList)

        # Loop through files and accumulate SST
        for fName in fileList:
            if (len(filesUsed) == 0):
                filesUsed = fName
            else:
                filesUsed = filesUsed + ', ' + fName

            sstFile = Dataset(fName)
            sst = sstFile.variables["MBsstd"][:, :, :, :]
            sstFile.close()
            sst = np.squeeze(sst)
            mean, num = meanVar(mean, num, sst)
    else:
        # Composite spans year boundary
        # Determine directory for the start year
        dataDir1 = dataDir
        dataDir1 = dataDir1.replace(endyearC, startYearC)
        # Determine end of start year based on start year
        if (isleap):
            endday = 366
        else:
            endday = 365

        # A DOY from startDoy to end of start year
        fileList = []
        print(dataDir1)
        os.chdir(dataDir1)
        doyRange = list(range(startDoy, endday + 1))
        for doy in doyRange:
            doyC = str(doy)
            doyC = doyC.rjust(3, '0')
            myString = 'MB' + startYearC + doyC + '*' + dtype + '.nc'
            fileList.append(glob.glob(myString))

        fileList = list(chain.from_iterable(fileList))
        fileList.sort()
        filesUsed = ""
        print(fileList)
        for fName in fileList:
            if (len(filesUsed) == 0):
                filesUsed = fName
            else:
                filesUsed = filesUsed + ', ' + fName

            sstFile = Dataset(fName)
            sst = sstFile.variables["MBsstd"][:, :, :, :]
            sstFile.close()
            sst = np.squeeze(sst)
            mean, num = meanVar(mean, num, sst)

        # DOY from 1 to endDoy of end year
        os.chdir(dataDir)
        fileList = []
        doyRange = list(range(1, endDoy + 1))
        print(doyRange)
        for doy in doyRange:
            doyC = str(doy)
            doyC = doyC.rjust(3, '0')
            myString = 'MB' + endyearC + doyC + '*' + dtype + '.nc'
            fileList.append(glob.glob(myString))

        fileList = list(chain.from_iterable(fileList))
        fileList.sort()
        print(fileList)
        for fName in fileList:
            if (len(filesUsed) == 0):
                filesUsed = fName
            else:
                filesUsed = filesUsed + ', ' + fName

            sstFile = Dataset(fName)
            sst = sstFile.variables["MBsstd"][:, :, :, :]
            sstFile.close()
            sst = np.squeeze(sst)
            mean, num = meanVar(mean, num, sst)

    # Mask out any grid cells with zero observations, setting them to the fill value
    mean = ma.array(mean, mask=(num == 0), fill_value=-9999999.)

    # Switch to the working directory for output operations
    os.chdir(workDir)

    # Construct the output filename with start and end dates plus data types
    outFile = 'MB' + startYearC + startDoyC + '_' + endyearC + endDoyC + '_' + dtype + '.nc'
    print(interval)

    # Create multi-day NetCDF file using the mean and count arrays
    ncFile = makeNetcdf(mean, num, interval, outFile, filesUsed, workDir)

    # Upload composite NetCDF to remote MB SST directory, labeling it with the interval
    send_to_servers(ncFile, '/MB/sstd/', str(interval))

```

### CompMBSSTmday

`CompMBSSTmday.py` reads daily 1-day SST NetCDF files for a user-specified date range and accumulates running sum and count arrays on a 0.025° × 0.025° regular grid spanning longitude 120°–320° and latitude –45°–65°. It then masks out any grid cells with zero observations and writes the averaged monthly composite as a CF-compliant NetCDF, which is automatically uploaded to a directory on the remote server. Find the monthly product on ERDDAP [here](modisa_scripts.qmd#modisa-products-on-swfsc-erddap-server).

The following chart summarizes the script's workflow:
```{mermaid}
%%{init: { "flowchart": { "htmlLabels": true } }}%%
flowchart LR
  I("<b>Inputs</b><br/>• 1-day SST NetCDFs (dataDir)<br/>• workDir<br/>• endYear & endDoy<br/>• startDoy")
    --> P("<b>Processing</b><br/>• Parse args & compute interval<br/>• Compute start/end dates<br/>• Clear workDir<br/>• Initialize mean & count arrays<br/>• Gather files<br/>• Loop: read each file & update mean/num<br/>• Mask pixels with no observations")
    --> O("<b>Output</b><br/>• Monthly composite SST NetCDF<br/>• Deployed to /MB/sstd/")
```

```{python}
#| eval: false
#| code-fold: true
#| code-summary: "View CompMBSSTmday.py"

"""
Overview
--------
Generate **monthly** SST composites for the MODIS “MB” (Pacific Ocean) dataset by averaging
daily 1-day NetCDF files over a user-defined range of days (typically a calendar month).

Usage
-----
::

    python CompMBSSTmday.py <dataDir> <workDir> <endYear> <endDoy> <startDoy>

Where:

- ``dataDir``

  Directory containing the daily 1-day SST NetCDFs named ``MB<YYYY><DDD>*sstd.nc``.

- ``workDir``

  Temporary working directory for intermediate files

- ``endYear``

  Four-digit year of the last day in the composite (e.g., ``2025``).

- ``endDoy``

  Three-digit day-of-year of the last composite day (e.g., ``031``).

- ``startDoy``

  Three-digit day-of-year of the first composite day (e.g., ``001``).

Description
-----------
1. **Parse arguments & compute interval**

  - Read ``dataDir``, ``workDir``, ``endYear``, ``endDoy``, and ``startDoy`` from ``sys.argv``.

  - Compute ``interval = endDoy - startDoy + 1``, the number of days in the composite.

2. **Compute start/end dates**

  - Convert ``endYear`+`endDoy`` to a ``datetime`` for logging.

  - Derive ``startDoy`` date by subtracting ``interval - 1`` days.

3. **Initialize accumulators**

  - Change to ``workDir`` and clear any old files.

  - Preallocate two arrays of shape 4401x8001:

     - ``mean`` (float32) for the running sum of SST.

     - ``num``  (int32) for the count of observations.

4. **Gather daily files**

  - Build a sorted list of all ``MB<YYYY><DDD>*sstd.nc`` files from ``startDoy`` to ``endDoy``, handling wrap-around at year boundaries if needed.

5. **Accumulate daily SST**

  - For each file:

     - Open via ``Dataset()``, read the 4-D variable ``MBsstd``, squeeze to 2-D.

     - Update ``mean`` and ``num`` via ``meanVar(mean, num, sst2d)``.

6. **Mask and finalize**

  - Create a masked array: mask pixels where ``num==0`` (no observations) and set fill value ``-9999999.``.

7. **Write & deploy**

  - Change to ``workDir``.

  - Construct an output filename ``MB<YYYY><startDDD>_<YYYY><endDDD>_sstd_mday.nc``.

  - Call ``makeNetcdfmDay(mean, num, interval, outFile, filesUsed, workDir)`` to produce a CF-compliant NetCDF.
  
  - Transfer the file to ``/MB/sstd/`` on the remote server via ``send_to_servers()``.

Dependencies
------------
- **Python 3.x**

- **Standard library:** ``os``, ``sys``, ``glob``, ``itertools.chain``, ``datetime``, ``timedelta``

- **Third-party:** ``numpy``, ``numpy.ma``, ``netCDF4.Dataset``

- **Custom roylib functions:**

  - ``isleap(year)``

  - ``meanVar(mean, num, data)``

  - ``makeNetcdfmDay(mean, num, interval, outFile, filesUsed, workDir)``

  - ``send_to_servers(ncFile, remote_dir, interval_flag)``

Directory Structure
-------------------
- **Input Directory** (``dataDir``):

  ``MB<YYYY><DDD>*sstd.nc`` daily SST files for the year.

- **Working Directory**  (``workDir``):

  Temporary staging for intermediate and final files.

- **Output Location**:

  Monthly composite written to ``workDir`` then copied to ``/MB/sstd/``.

Usage Example
-------------
Create a January 2025 composite (DOY 001-031):
::
 
   python CompMBSSTmday.py /data/modisgf/1day/ /tmp/mb_work/ 2025 031 001

This averages daily SST files DOY 001…031 of 2025, writes ``MB2025001_2025031_sstd_mday.nc`` in ``/tmp/mb_work/``, and uploads to ``/MB/sstd/``.
"""
from __future__ import print_function
from builtins import str
from builtins import range

if __name__ == "__main__":
    from datetime import datetime, timedelta
    import glob
    from itertools import chain
    from netCDF4 import Dataset
    import numpy as np
    import numpy.ma as ma
    import os
    import sys

    # Ensure 'roylib' is on the import path
    sys.path.append('/home/cwatch/pythonLibs')
    from roylib import *

    # Directory of daily 1-day NetCDFs
    dataDir = sys.argv[1]

    # Temporary working directory
    workDir = sys.argv[2]

    # End date (year, day-of-year)
    endyearC = sys.argv[3]
    endDoyC = sys.argv[4]
    endDoyC = endDoyC.rjust(3, '0')

    # Start day-of-year (same year)
    startYearC = endyearC
    endDoy = int(endDoyC)
    startDoyC = sys.argv[5]
    startDoyC = startDoyC.rjust(3, '0')
    startDoy = int(startDoyC )

    # Number of days in the composite
    interval = endDoy - startDoy + 1

    # Convert end date to calendar dates
    myDateEnd = datetime(int(endyearC), 1, 1) + timedelta(endDoy - 1)
    myDateStart = myDateEnd + timedelta(days=-(interval - 1))

    # Prepare output directory
    outDir = '/ERDData1/modisa/data/modisgf/' + endyearC + '/mday'

    print(dataDir)
    print(workDir)
    print(endyearC)
    print(endDoyC)
    print(interval)

    ###
    #cdtypeList = ['sstd']
    # for dtype in dtypeList:

    # Only SST variable for MB composites
    dtype = 'sstd'

    # Clean working directory
    os.chdir(workDir)
    os.system('rm -f *')

    # Move to data directory
    os.chdir(dataDir)

    # Preallocate sum (mean) and count arrays matching grid dims
    mean = np.zeros((4401, 8001), np.single)
    num = np.zeros((4401, 8001), dtype=np.int32)

    # Build list of NetCDF files spanning startDoy..endDoy
    if (endDoy > startDoy):
        # Composite entirely within the same year
        doyRange = list(range(startDoy, endDoy + 1))
        print(doyRange)

        # Build list matching files for each DOY
        fileList = []
        for doy in doyRange:
            doyC = str(doy)
            doyC = doyC.rjust(3, '0')
            myString = 'MB' + endyearC + doyC + '*' + dtype + '.nc'
            fileList.append(glob.glob(myString))

        # Flatten and sort
        fileList = list(chain.from_iterable(fileList))
        fileList.sort()
        filesUsed = ""
        print(fileList)

        # Loop through each file
        for fName in fileList:
            # Build provenance string
            if (len(filesUsed) == 0):
                filesUsed = fName
            else:
                filesUsed = filesUsed + ', ' + fName

            # Read SST variable and accumulate
            sstFile = Dataset(fName)
            sst = sstFile.variables["MBsstd"][:, :, :, :]
            sstFile.close()
            sst = np.squeeze(sst)

            # Update running mean and count arrays
            mean, num = meanVar(mean, num, sst)
    else:
        # Composite spans year boundary
        dataDir1 = dataDir
        dataDir1 = dataDir1.replace(endyearC, startYearC)

        # Determine last day of start year
        if (isleap):
            endday = 366
        else:
            endday = 365

        # From startDoy through end of start year  
        fileList = []
        os.chdir(dataDir1)
        doyRange = list(range(startDoy, endday + 1))
        for doy in doyRange:
            doyC = str(doy)
            doyC = doyC.rjust(3, '0')
            myString = 'MB' + startYearC + doyC + '*' + dtype + '.nc'
            fileList.append(glob.glob(myString))

        fileList = list(chain.from_iterable(fileList))
        fileList.sort()
        fileUsed = ""
        print(fileList)
        for fName in fileList:
            if (len(filesUsed) == 0):
                filesUsed = fName
            else:
                filesUsed = filesUsed + ', ' + fName

            sstFile = Dataset(fName)
            sst = sstFile.variables["MBsstd"][:, :, :, :]
            sstFile.close()
            sst = np.squeeze(sst)
            mean, num = meanVar(mean, num, sst)

        # From DOY 1 of end year through endDoy
        os.chdir(dataDir)
        fileList = []
        doyRange = list(range(1, endDoy + 1))
        for doy in doyRange:
            doyC = str(doy)
            doyC = doyC.rjust(3, '0')
            myString = 'MB' + endyearC + doyC + '*' + dtype + '.nc'
            fileList.append(glob.glob(myString))

        fileList = list(chain.from_iterable(fileList))
        fileList.sort()
        print(fileList)
        for fName in fileList:
            if (len(filesUsed) == 0):
                filesUsed = fName
            else:
                filesUsed = filesUsed + ', ' + fName

            sstFile = Dataset(fName)
            sst = sstFile.variables["MBsstd"][:, :, :, :]
            sstFile.close()
            sst = np.squeeze(sst)
            mean, num = meanVar(mean,num,sst)

    # Mask out pixels with zero observations and set fill value for missing data
    mean = ma.array(mean, mask=(num == 0), fill_value=-9999999.)

    # Switch to the working directory
    os.chdir(workDir)

    # Construct output filename
    outFile = 'MB' + startYearC + startDoyC + '_' + endyearC + endDoyC + '_' + dtype + '.nc'

    # Generate the multi-day NetCDF file
    ncFile = makeNetcdfmDay(mean, num, interval, outFile, filesUsed, workDir)

    # Send the NetCDF to the remote server directory
    send_to_servers(ncFile, '/MB/sstd/' , 'm')

```
