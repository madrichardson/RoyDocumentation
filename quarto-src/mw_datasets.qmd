---
title: "MW Datasets"
format: html
---

Here you’ll find the MW (West Coast) processing scripts for both chlorophyll-a and SST. 

They ingest raw Level-2 swath files, apply regional spatial and quality filters to isolate the West Coast, and grid valid observations into daily NetCDF products. Those daily files are then averaged over rolling multi-day windows—with composites automatically rebuilt whenever new data arrive.

Below are the individual scripts that form the MW workflow. Use the expandable code blocks to explore each stage of the pipeline.

## Chlorophyll
### makeChla1daynewMW

`makeChla1daynewMW.py` makes a 1-day compiste by reading Level-2 ocean-color swath files for a given day, extracts and filters “Day” pixels for chlorophyll-a, Kd490, PAR(0) and fluorescence line height (cflh) within longitude 205°–255° and latitude 22°–51°, then concatenates their longitude, latitude and value arrays. It uses PyGMT to interpolate each dataset onto a 0.0125° × 0.0125° regular grid, converts the results to CF-compliant NetCDFs, and uploads them to a directory on a remote server. Find the 1-day product on ERDDAP [here](modisa_scripts.qmd#modisa-products-on-swfsc-erddap-server).

The following chart summarizes the script's workflow:
```{mermaid}
%%{init: {"flowchart":{"htmlLabels":true}}}%%
flowchart LR
  I("<b>Inputs</b><br/>• Raw L2 Chla NetCDF swaths (dataDir)<br/>• workDir<br/>• Year & DOY")
    --> P("<b>Processing</b><br/>• Compute calendar date & directories<br/>• Load static land mask<br/>• Discover swaths for Day N (HOD>10) & N+1 (HOD≤10)<br/>• Stage & filter swaths (Day-only, region overlap)<br/>• Extract, reshape & filter (Chla)<br/>• Interpolate onto 0.0125° grid<br/>• Apply land mask")
    --> O("<b>Output</b><br/>• CF-compliant 1-day composite NetCDF<br/>• Deployed to /MW/chla/1day/")
```

```{python}
#| eval: false
#| code-fold: true
#| code-summary: "View makeChla1daynewMW.py"

"""
Overview
--------
Generate one-day chlorophyll-a (Chla) and related ocean-color products for the MW (West Coast) region by combining MODIS Level-2 swath files and gridding them with PyGMT. This script produces CF-compliant NetCDF files for Chla, Kd490, PAR(0), and fluorescence line height (cflh), then copies them into the appropriate server directories.

Usage
-----
::
 
     python makeChla1daynewMW.py <dataDir> <workDir> <year> <doy>

Where:

- ``dataDir``

  Root folder containing raw Level-2 ocean-color NetCDF swath files, organized as ``<dataDirBase>/<YYYY><MM>/``. Each swath must match: ``AQUA_MODIS.<YYYY><MM><DD>*L2.OC.NRT.nc``.

- ``workDir``

  Temporary working directory for staging swath copies and intermediate files.

- ``year``

  Four-digit year string (e.g., ``"2025"``).

- ``doy``

  Three-digit day-of-year string (zero-padded, e.g., ``"082"`` for March 23).

Description
-----------
1. **Parse command-line arguments**

     - Read ``dataDir``, ``workDir``, ``year``, and ``doy`` from ``sys.argv``.

     - Print ``year`` and ``doy`` for logging.

2. **Convert ``year`` + ``doy`` to calendar date**

     - Compute a ``datetime`` object for the specified day of year.

     - Zero-pad month and day to form ``MM`` and ``DD``.

     - Construct ``datadir = dataDir + year + MM + "/"``, which must contain all raw L2 OC swath files for that date.

3. **Load static land mask**

     - Open GRD mask file at: ``/u00/ref/landmasks/LM_205_255_0.0125_22_51_0.0125_gridline.grd``

     - Read the 2D mask array ``my_mask`` (1 = ocean, other = land).

     - Close the mask dataset.

4. **List all swath granules for the given date**

     - Change into ``datadir``.

     - Build a glob pattern: ``"AQUA_MODIS.<year><MM><DD>*.L2.OC.NRT.nc"``.

     - Retrieve and sort all matching filenames.

5. **Prepare working directory**

     - Change into `workDir`.

     - Remove any stale files matching:

         - ``AQUA_MODIS.*L2.OC*``

         - ``MW20*``

6. **Initialize data accumulators**

     - ``filesUsed``: comma-separated string for provenance of processed swaths.

     - ``temp_data_Chla``, ``temp_data_k490``, ``temp_data_par0``, ``temp_data_flh``: accumulate (lon, lat, value) rows for each parameter.

7. **Loop over each OC swath granule**

     For each ``fName`` in the sorted list:

     a. **Copy swath to work directory & open NetCDF**

         - Copy from ``datadir`` to ``workDir``.

         - Attempt ``Dataset(fileName, 'r')``; skip if IOError.

     b. **Extract navigation data**

         - Read ``latitude`` and ``longitude`` from ``rootgrp.groups['navigation_data']``.

         - Convert negative longitudes (< 0) to 0-360°.

         - Compute ``dataLonMin``, ``dataLonMax``, ``dataLatMin``, ``dataLatMax`` for geographic filtering.

     c. **Determine if swath overlaps the MW region**

         - Region bounds: ``lon ∈ [205, 255]`` and ``lat ∈ [22, 51]``.

         - ``goodLon = True`` if any longitudes fall within [205, 255].

         - ``goodLat = True`` if any latitudes fall within [22, 51].

         - Proceed only if ``goodLon and goodLat``.

     d. **Record filename for provenance**

         - Append ``fileName`` to ``filesUsed`` (comma-separated).

     e. **Reshape navigation arrays**

         - Call ``myReshape(latitude)`` → column vector (Nx1).

         - Call ``myReshape(longitude)``.

     f. **Extract and filter each parameter**

      For each variable in ``geophysical_data``:

        1. **Chlorophyll-a (chlor_a)**

             - Read and reshape: ``chlor_a = myReshape(rootgrp.groups['geophysical_data'].variables['chlor_a'][:, :])``

             - Stack: ``dataOut = np.hstack((longitude, latitude, chlor_a))``

             - Filter:

                 - ``dataOut[:, 0] > -400``

                 - ``lonmin ≤ dataOut[:, 0] ≤ lonmax``

                 - ``latmin ≤ dataOut[:, 1] ≤ latmax``

                 - ``chlor_a > 0``

             - Accumulate into ``temp_data_Chla``.

         2. **Kd490 (Kd_490)**

             - Read, optionally scale, reshape, and stack with (lon, lat).

             - Filter:

                 - valid lon/lat as above,

                 - ``0 < Kd490 < 6.3``

             - Accumulate into ``temp_data_k490``.

         3. **PAR(0) (par)**
    
             - Read, optionally scale, reshape, and stack.

             - Filter:

                 - valid lon/lat,

                 - ``PAR > 0``

             - Accumulate into ``temp_data_par0``.

         4. **Fluorescence Line Height (cflh)**

             - Read, optionally scale, reshape, and stack.

             - Filter:

                 - valid lon/lat,

                 - ``cflh > 0``

            - Accumulate into ``temp_data_flh``.

     g. **Cleanup**

         - ``rootgrp.close()``

         - ``os.remove(fileName)`` from ``workDir``.

8. **Grid each parameter's point cloud with PyGMT**

     For each of ``temp_data_Chla``, ``temp_data_k490``, ``temp_data_par0``, ``temp_data_flh``:

     - Set:
         - ``region = "205/255/22/51"``

         - ``spacing = "0.0125/0.0125"``

         - ``search_radius = "2k"``

         - ``sectors = "1"``

     - Call:

     ::

         temp_data1 = pygmt.nearneighbor(
             data=temp_data_<param>,
             region=region,
             spacing=spacing,
             search_radius=search_radius,
             sectors="1"
         )

     - This returns a PyGMT grid (xarray.DataArray) covering the specified region.

9. **Convert grid(s) to CF-compliant NetCDF & send to server**

     - Call:

     ::

         ncFile = grd2netcdf1(temp_data1, fileOut, filesUsed, my_mask, "MW")

     - Masks land, computes coverage, builds CF NetCDF via CDL, and returns the NetCDF path.

     - Call:

     ::

         send_to_servers(ncFile, "/MW/<param>/", "1")

         - E.g. `send_to_servers(ncFile, "/MW/chla/", "1")`

     - Delete local NetCDF:

     ::

         os.remove(ncFile)

Dependencies
------------
- **Python 3.x**

- **Standard library:** ``sys``, ``os``, ``glob``, ``shutil``, ``re``, ``itertools.chain``, ``datetime``, ``timedelta``

- **Third-party packages:** ``netCDF4.Dataset``, ``numpy``, ``pygmt``

- **Custom roylib functions:**

  - ``myReshape(array)``
  
  - ``grd2netcdf1(grd, outName, filesUsed, mask, fType)``
  
  - ``send_to_servers(ncFile, destDir, interval)``
  
  - ``isleap(year)``
  
  - ``makeNetcdf(mean, nobs, interval, outFile, filesUsed, workDir)``
  
  - ``meanVar(mean, num, obs)``

Land Mask
---------
- A static GRD file is required at:

 ``/u00/ref/landmasks/LM_205_255_0.0125_22_51_0.0125_gridline.grd``
 
- This mask is applied to each gridded output to set land pixels to NaN.

Directory Structure
-------------------
- **Input swaths directory** (datadir):

  Directory for raw Level-2 OC NetCDF swath files for the given date, organized as ``<dataDirBase>/<YYYY><MM>/``. Each file must be named: ``AQUA_MODIS.<YYYY><MM><DD>*L2.OC.NRT.nc``

- **Working directory** (workDir):

  Temporary staging area where swaths are copied for processing and then deleted.

- **Output grid** (fileOut):

  GMT “.grd” file created by PyGMT nearneighbor, named: ``MW<YYYY><DDD>_<YYYY><DDD>_<param>.grd`` (e.g. ``MW2025082_2025082_chla.grd``).

- **Final NetCDF** (returned by ``grd2netcdf1``):

  CF-compliant NetCDF file named: ``MW<YYYY><DDD>_<YYYY><DDD>_<param>.nc`` Copied into the MW 1-day product directories, for example: ``/path/to/modis_data/modiswc/chla/1day/MW2025082_2025082_chla.nc``

Usage Example
-------------
Assume your raw OC swaths live in:

::

     /Users/you/modis_data/netcdf/202503/

and your working directory is:

::
 
     /Users/you/modis_work/

Then to produce March 23, 2025 products:

::

    python makeChla1daynewMW.py /Users/you/modis_data/netcdf/ \
                                /Users/you/modis_work/ \
                                2025 082

This will:

  - Copy all swaths matching ``AQUA_MODIS.20250323*.L2.OC.NRT.nc`` into ``/Users/you/modis_work/``.

  - Build combined point clouds for Chla, Kd490, PAR(0), and cflh.

  - Create PyGMT grids over lon 205-255°, lat 22-51° via ``nearneighbor``.

  - Convert each grid to a CF-compliant NetCDF using ``grd2netcdf1``, masked by the static GRD.

  - Copy the final NetCDFs to the appropriate MW 1-day product folders.
"""
from __future__ import print_function
from builtins import str

if __name__ == "__main__":
    from datetime import datetime, timedelta
    import glob
    from itertools import chain
    from netCDF4 import Dataset
    import numpy as np
    import numpy.ma as ma
    import pygmt
    import os
    import re
    import shutil
    import sys

    # Ensure 'roylib' is on the import path
    sys.path.append('/home/cwatch/pythonLibs')
    from roylib import *

    # Geographic bounds for MW region
    latmax = 51.
    latmin = 22.
    lonmax = 255.
    lonmin = 205.

    # Set data directory
    datadirBase = sys.argv[1]

    # Set work directory
    workdir = sys.argv[2]

    # Get the year and doy from the command line
    year = sys.argv[3]
    doy = sys.argv[4]
    print(year)
    print(doy)

    # Convert year/doy to a calendar date and zero-pad month/day
    myDate = datetime(int(year), 1, 1) + timedelta(int(doy) - 1)
    myMon = str(myDate.month)
    myMon = myMon.rjust(2, '0')
    myDay = str(myDate.day).rjust(2, '0')

    # Construct the directory path where raw OC swaths are stored for this date
    datadir = datadirBase + year + myMon + '/'

    # Load static land mask from GRD
    mask_root = Dataset('/u00/ref/landmasks/LM_205_255_0.0125_22_51_0.0125_gridline.grd')
    my_mask = mask_root.variables['z'][:, :]
    mask_root.close()

    # Now move to the data directory
    os.chdir(datadir)

    # Set up the string for the file search in the data directory
    myString = 'AQUA_MODIS.' + year + myMon + myDay  + '*.L2.OC.NRT.nc'
    print(myString)

    # Get list of files in the data directory that match with full path
    fileList = glob.glob(myString)
    # print(fileList)
    fileList.sort()

    # Now move to the work directory and clear old files
    os.chdir(workdir)
    os.system('rm -f AQUA_MODIS.*L2.OC*')
    os.system('rm -f MW20*')

    # Do the whole thing for chla
    outFileChla = 'modiswcChlatemp'
    outFilek490 = 'modiswck490temp'
    outFilepar0 = 'modiswcpar0temp'
    outFilecflh = 'modiswccflhtemp'
    
    # Initialize variables to accumulate data and track provenance
    filesUsed = ""
    temp_data_Chla = None
    temp_data_k490 = None
    temp_data_par0 = None
    temp_data_flh = None

    # Loop over each OC swath granule for the given day
    for fName in fileList:
        fileName = fName
        # find the datatime group in the filename
        # check on what to search for
        #datetime = re.search('AQUA_MODIS(.+?).L2.NRT.OC', fileName)
        # will want elements 8,9 of datatime.group(1)
        print(fileName)

        # Copy the swath from datadir into the workdir
        shutil.copyfile(datadir + fName, workdir + fName)

        # Try to open the NetCDF; skip if the file is unreadable
        try:
            rootgrp = Dataset(fileName, 'r')
        except IOError:
            print("bad file " + fileName)
            continue

        # Extract navigation-group data
        navDataGroup = rootgrp.groups['navigation_data']
        latitude = navDataGroup.variables['latitude'][:, :]
        longitude = navDataGroup.variables['longitude'][:, :]

        # Convert any negative longitudes to the 0-360° domain
        longitude[longitude < 0] = longitude[longitude < 0] + 360

        # Compute swath extents (min/max) for geographic filtering
        dataLonMin = np.nanmin(longitude[longitude >= 0])
        dataLonMax = np.nanmax(longitude[longitude <= 360])
        dataLatMin = np.nanmin(latitude[latitude >= -90])
        dataLatMax = np.nanmax(latitude[latitude <= 90])

        # Determine if swath overlaps our global MW region
        goodLon1 = (dataLonMin < lonmin) and (dataLonMax >= lonmin)
        goodLon2 = (dataLonMin >= lonmin) and (dataLonMin <= lonmax)
        goodLon = goodLon1 or goodLon2

        goodLat1 = (dataLatMin < latmin) and (dataLatMax >= latmin)
        goodLat2 = (dataLatMin >= latmin) and (dataLatMin <= latmax)
        goodLat = goodLat1 or goodLat2

        # Check if swath is daytime (only keep "Day" pixels)
        dayNightTest = (rootgrp.day_night_flag == 'Day')

        # Only proceed if geography and day-night tests pass
        if (goodLon and goodLat):
            # Add filename to provenance list
            if (len(filesUsed) == 0):
                filesUsed = fileName
            else:
                filesUsed = filesUsed + ', ' + fileName

            # Reshape latitude & longitude arrays into column vectors
            latitude = myReshape(latitude)
            longitude = myReshape(longitude)

            # Access geophysical data group
            geoDataGroup = rootgrp.groups['geophysical_data']

            # Extract chlor_a
            chlor_a = geoDataGroup.variables['chlor_a'][:, :]
            chlor_a = myReshape(chlor_a)

            # Stack (lon, lat, chlor_a) into a single 2D array with shape (N, 3)
            dataOut = np.hstack((longitude, latitude, chlor_a))

            # Filter out-of-range lon/lat and non-positive chlor_a
            dataOut = dataOut[dataOut[:, 0] > -400]
            dataOut = dataOut[dataOut[:, 0] >= lonmin]
            dataOut = dataOut[dataOut[:, 0] <= lonmax]
            dataOut = dataOut[dataOut[:, 1] >= latmin]
            dataOut = dataOut[dataOut[:, 1] <= latmax]
            dataOut = dataOut[dataOut[:, 2] > 0]

            # Accumulate into temp_data_Chla
            if (dataOut.shape[0] > 0):
                if (temp_data_Chla is None):
                    temp_data_Chla = dataOut
                else:
                    temp_data_Chla = np.concatenate((temp_data_Chla, dataOut), axis=0)

            # Extract Kd490
            k490 = geoDataGroup.variables['Kd_490'][:, :]
            # k490 = k490 * 2.0E-4
            k490 = myReshape(k490)

            dataOut = np.hstack((longitude, latitude, k490))
            dataOut = dataOut[dataOut[:, 0] > -400]
            dataOut = dataOut[dataOut[:, 0] >= lonmin]
            dataOut = dataOut[dataOut[:, 0] <= lonmax]
            dataOut = dataOut[dataOut[:, 1] >= latmin]
            dataOut = dataOut[dataOut[:, 1] <= latmax]
            dataOut = dataOut[dataOut[:, 2] < 6.3]
            dataOut = dataOut[dataOut[:, 2] > 0]

            if (dataOut.shape[0] > 0):
                if (temp_data_k490 is None):
                    temp_data_k490 = dataOut
                else:
                    temp_data_k490 = np.concatenate((temp_data_k490, dataOut), axis=0)

            # Extract PAR0
            par0 = geoDataGroup.variables['par'][:, :]
            # par0 = (0.002 * par0) + 65.5
            par0 = myReshape(par0)

            dataOut = np.hstack((longitude, latitude, par0))
            dataOut = dataOut[dataOut[:, 0] > -400]
            dataOut = dataOut[dataOut[:, 0] >= lonmin]
            dataOut = dataOut[dataOut[:, 0] <= lonmax]
            dataOut = dataOut[dataOut[:, 1] >= latmin]
            dataOut = dataOut[dataOut[:, 1] <= latmax]
            dataOut = dataOut[dataOut[:, 2] > 0]

            if (dataOut.shape[0] > 0):
                if (temp_data_par0 is None):
                    temp_data_par0 = dataOut
                else:
                    temp_data_par0 = np.concatenate((temp_data_par0, dataOut), axis=0)

            # Extract Fluorescence Line Height (cflh)
            cflh = geoDataGroup.variables['nflh'][:, :]
            # cflh = 1.0E-5 * cflh
            cflh = myReshape(cflh)

            dataOut = np.hstack((longitude, latitude, cflh))
            dataOut = dataOut[dataOut[:, 2] > 0]
            dataOut = dataOut[dataOut[:, 0] > -400]
            dataOut = dataOut[dataOut[:, 0] >= lonmin]
            dataOut = dataOut[dataOut[:, 0] <= lonmax]
            dataOut = dataOut[dataOut[:, 1] >= latmin]
            dataOut = dataOut[dataOut[:, 1] <= latmax]
            dataOut = dataOut[dataOut[:, 2] > 0]

            if (dataOut.shape[0] > 0):
                if (temp_data_flh is None):
                    temp_data_flh = dataOut
                else:
                    temp_data_flh = np.concatenate((temp_data_flh, dataOut), axis=0)

        # Close the NetCDF and remove the swath file from workdir
        rootgrp.close()
        os.remove(fileName)

    # Grid and write each parameter's point cloud via PyGMT,
    #  then convert to NetCDF and send to server

    # chlor_a composite
    fileOut = 'MW' + year + doy + '_' + year + doy + '_chla.grd'
    range = '205/255/22/51'
    increment = '0.0125/0.0125'
    smooth = '2k'

    # Create a gridded dataset from the chlor_a point cloud
    temp_data1 = pygmt.nearneighbor(
        data=temp_data_Chla,
        region=range,
        spacing=increment,
        search_radius=smooth,
        sectors='1'
    )

    # Convert the GMT grid to CF-compliant NetCDF, masking land
    ncFile = grd2netcdf1(temp_data1, fileOut, filesUsed, my_mask, 'MW')

    #myCmd = "mv " + ncFile + " /home/cwatch/pygmt_test/outfiles"
    #os.system(myCmd)

    # Copy to the MW server folder for chla (1-day product)
    send_to_servers(ncFile, '/MW/chla/' , '1')
    os.remove(ncFile)

    # Kd490 composite
    #fileIn = outFilek490
    fileOut = 'MW' + year + doy + '_' + year + doy + '_k490.grd'
    range = '205/255/22/51'
    increment = '0.0125/0.0125'
    smooth = '2k'

    # Create a gridded dataset from the Kd490 point cloud
    temp_data1 = pygmt.nearneighbor(
        data=temp_data_k490,
        region=range,
        spacing=increment,
        search_radius=smooth,
        sectors='1'
    )

    # Convert the GMT grid to CF-compliant NetCDF, masking land
    ncFile = grd2netcdf1(temp_data1, fileOut, filesUsed, my_mask, 'MW')

    #myCmd = "mv " + ncFile + " /home/cwatch/pygmt_test/outfiles"
    #os.system(myCmd)

    # Copy to the MW server folder for k490 (1-day product)
    send_to_servers(ncFile, '/MW/k490/' , '1')
    os.remove(ncFile)

    # PAR(0) composite
    fileOut = 'MW' + year + doy + '_' + year + doy + '_par0.grd'
    range = '205/255/22/51'
    increment = '0.0125/0.0125'
    smooth = '2k'

    # Create a gridded dataset from the PAR(0) point cloud
    temp_data1 = pygmt.nearneighbor(
        data=temp_data_par0,
        region=range,
        spacing=increment,
        search_radius=smooth,
        sectors='1'
    )

    # Convert the GMT grid to CF-compliant NetCDF, masking land
    ncFile = grd2netcdf1(temp_data1, fileOut, filesUsed, my_mask, 'MW')

    #myCmd = "mv " + ncFile + " /home/cwatch/pygmt_test/outfiles"
    #os.system(myCmd)

    # Copy to the MW server folder for PAR(0) (1-day product)
    send_to_servers(ncFile, '/MW/k490/' , '1')  # Note: likely intended for '/MW/par0/'
    os.remove(ncFile)

    # cflh composite
    #fileIn = outFilecflh
    fileOut = 'MW' + year + doy + '_' + year + doy + '_cflh.grd'
    range = '205/255/22/51'
    increment = '0.0125/0.0125'
    smooth = '2k'

    # Create a gridded dataset from the cflh point cloud
    temp_data1 = pygmt.nearneighbor(
        data=temp_data_flh,
        region=range,
        spacing=increment,
        search_radius=smooth,
        sectors='1'
    )

    # Convert the GMT grid to CF-compliant NetCDF, masking land
    ncFile = grd2netcdf1(temp_data1, fileOut, filesUsed, my_mask, 'MW')

    #myCmd = "mv " + ncFile + " /home/cwatch/pygmt_test/outfiles"
    #os.system(myCmd)

    # Copy to the MW server folder for cflh (1-day product)
    send_to_servers(ncFile, '/MW/k490/' , '1')  # Note: likely intended for '/MW/cflh/'
    os.remove(ncFile)

```

### CompMWChla

`CompMWChla.py` reads 1-day composite MW NetCDF files for chlorophyll-a, Kd490, PAR(0), CFLH over the specified interval and accumulates running sums and counts on a 0.0125° × 0.0125° regular grid spanning longitude 205°–255° and latitude 22°–51°. It then masks out any grid cells with zero observations, writes each multi-day (3-, 8-, or 14-day) composite as a CF-compliant NetCDF, and uploads them to their respective directories on a remote server. Find the multi-day products on ERDDAP [here](modisa_scripts.qmd#modisa-products-on-swfsc-erddap-server).

The following chart summarizes the script's workflow:
```{mermaid}
%%{init: {"flowchart":{"htmlLabels":true}}}%%
flowchart LR
  I("<b>Inputs</b><br/>• 1-day composite NetCDFs (dataDir)<br/>• workDir<br/>• endYear & endDoy<br/>• interval (days)")
    --> P("<b>Processing</b><br/>• Parse args & compute start/end DOY<br/>• Clear workDir & initialize mean/num arrays<br/>• Gather files over date range<br/>• Loop: open each file & update mean/num<br/>• Mask zero-observation pixels")
    --> O("<b>Output</b><br/>• CF-compliant multi-day (3-, 8-, 14-) composite NetCDF<br/>• Deployed to /MW/chla/interval day/")
```

```{python}
#| eval: false
#| code-fold: true
#| code-summary: "View CompMWChla.py"

"""
Overview
--------
Generate multi-day composites for MODIS MW (West Coast) products:
chlorophyll-a (Chla), Kd490, PAR0, and fluorescence line height (CFLH).

Usage
-----
::

    python CompMWChla.py <dataDir> <workDir> <endYear> <endDoy> <interval>

Where:

- ``dataDir``

  Directory containing daily 1-day NetCDFs named ``MW<YYYY><DDD>_<param>.nc``.

- ``workDir``

  Temporary working directory for intermediate files.

- ``endYear`` 

  Four-digit year of the last day in the composite (e.g., ``2025``).

- ``endDoy``

  Three-digit day-of-year of the last day (e.g., ``082``).

- ``interval``

  Composite length in days (`3`, `5`, `8`, or `14`)

Description
-----------
1. **Parse arguments & compute date range** 

  - Read ``dataDir``, ``workDir``, ``endYear``, ``endDoy``, ``interval``.

  - Compute start day = end day minus (``interval``-1), zero-pad ``startDoy``.

2. **Loop over each variable** (``chla``, ``k490``, ``par0``, ``cflh``):  

  - Clear ``workDir``.

  - Initialize ``mean`` and ``num`` arrays for sum and count.

  - Gather daily files over the date range (handles year wrap).

  - For each file, read 4D ``MW<dtype>``, squeeze to 2D, update ``mean``, ``num``.

  - Mask out pixels with zero count (fill = -9999999).

  - Write composite via ``makeNetcdf(...)`` and send via ``send_to_servers(...)``.

Dependencies
------------
- **Python 3.x**

- **Standard library:** ``os``, ``sys``, ``glob``, ``itertools.chain``, ``datetime``

- **Third-party:** ``numpy``, ``numpy.ma``, ``netCDF4``

- **Custom roylib functions:**

  - ``isleap(year)``

  - ``meanVar(mean, num, obs)``

  - ``makeNetcdf(mean, nobs, interval, outFile, filesUsed, workDir)``

  - ``send_to_servers(ncFile, destDir, interval)``

Directory Structure
-------------------
- **Input Directory** (``dataDir``):

  ``MW<YYYY><DDD>*<dtype>.nc``

- **Working Directory** (``workDir``):

  Temporary staging for intermediate files

- **Output Location** (remote):

  ``/MW/<dtype>/`` on the server

Usage Example
-------------
5-day composite (DOY 100-104 of 2025):

::
  
   python CompMWChla.py /data/MW/1day /tmp/mw_work 2025 104 5

This command will:

  - Read all daily files ``MW2025100*...MW2025104*`` from ``/data/MW/1day``.

  - Compute the 5-day composite for Chla, Kd490, PAR0, and CFLH.

  - Write the output files to ``/tmp/mw_work/`` and upload them to ``/MW/<dtype>/`` on the server.
"""
from __future__ import print_function
from builtins import str
from builtins import range

if __name__ == "__main__":
    from datetime import datetime, timedelta
    import glob
    from itertools import chain
    from netCDF4 import Dataset
    import numpy as np
    import numpy.ma as ma
    import os
    import sys

    # Ensure 'roylib' is on the import path
    sys.path.append('/home/cwatch/pythonLibs')
    from roylib import *

    # Directory with 1-day MW NetCDFs
    dataDir = sys.argv[1]

    # Temporary working directory
    workDir = sys.argv[2]

    # Composite end year (YYYY)
    endyearC = sys.argv[3]

    # Composite end day-of-year (DDD)
    endDoyC = sys.argv[4]
    endDoyC = endDoyC.rjust(3, '0')

    # Integer form of end day-of-year
    endDoy = int(endDoyC)

    # Composite length
    intervalC = sys.argv[5]
    interval = int(intervalC)

    # Convert end Doy to calendar date
    myDateEnd = datetime(int(endyearC), 1, 1) + timedelta(int(endDoyC) - 1)

    # Start date = end date minus (interval-1) days
    myDateStart = myDateEnd + timedelta(days=-(interval - 1))

    # Zero-padded start Doy and year
    startDoyC = myDateStart.strftime("%j").zfill(3)
    startDoy = int(startDoyC)
    startYearC = str(myDateStart.year)

    # Prepare output directory 
    outDir = '/ERDData1/modisa/data/modsiwc/' + endyearC + '/' + intervalC + 'day'

    print(dataDir)
    print(workDir)
    print(endyearC)
    print(endDoyC)
    print(intervalC)

    # List of parameters to composite
    dtypeList = ['chla', 'k490', 'par0', 'cflh']

    # Loop over each variable type
    for dtype in dtypeList:
        # Clear working directory
        os.chdir(workDir)
        os.system('rm -f *')

        # Move to data directory
        os.chdir(dataDir)

        # Preallocate sum (mean) and count arrays matching grid dims
        mean = np.zeros((2321, 4001), np.single)
        num = np.zeros((2321, 4001), dtype=np.int32)

        # If composite does not cross year boundary
        if (endDoy > startDoy):
            doyRange = list(range(startDoy, endDoy+1))
            fileList = []
            # Gather matching files for each day in range
            for doy in doyRange:
                doyC = str(doy)
                doyC = doyC.rjust(3, '0')
                myString = 'MW' + endyearC + doyC + '*' + dtype + '.nc'
                fileList.append(glob.glob(myString))
            
            # Flatten and sort list of lists
            fileList=list(chain.from_iterable(fileList))
            fileList.sort()

            filesUsed = ""
            print(fileList)
            for fName in fileList:
                # Build comma-separated provenance string
                if (len(filesUsed) == 0):
                    filesUsed = fName
                else:
                    filesUsed = filesUsed + ', ' + fName

                # Read the variable from NetCDF and accumulate
                chlaFile = Dataset(fName)
                param = 'MW' + dtype
                chla = chlaFile.variables[param][:, :, :, :]
                chlaFile.close()
                chla = np.squeeze(chla)

                # Update running mean and count arrays
                mean, num = meanVar(mean, num, chla)

        else:
            # Composite spans year boundary: first part in startYearC
            dataDir1 = dataDir
            dataDir1 = dataDir1.replace(endyearC, startYearC)
            if (isleap):
                endday = 366
            else:
                endday = 365

            fileList = []
            os.chdir(dataDir1)
            # Days from startDoy to end of start year
            doyRange = list(range(startDoy, endday + 1))
            for doy in doyRange:
                doyC = str(doy)
                doyC = doyC.rjust(3, '0')
                myString = 'MW' + startYearC + doyC + '*' + dtype + '.nc'
                fileList.append(glob.glob(myString))

            fileList = list(chain.from_iterable(fileList))
            fileList.sort()
            filesUsed = ""
            print(fileList)
            for fName in fileList:
                if (len(filesUsed) == 0):
                    filesUsed = fName
                else:
                    filesUsed = filesUsed + ', ' + fName

                chlaFile = Dataset(fName)
                param = 'MW' + dtype
                chla = chlaFile.variables[param][:, :, :, :]
                chlaFile.close()
                chla = np.squeeze(chla)
                mean, num = meanVar(mean, num, chla)

            # Days from DOY=1 of end year to endDoy
            os.chdir(dataDir)
            fileList = []
            doyRange = list(range(1, endDoy + 1))
            for doy in doyRange:
                doyC = str(doy)
                doyC = doyC.rjust(3, '0')
                myString = 'MW' + endyearC + doyC + '*' + dtype + '.nc'
                fileList.append(glob.glob(myString))

            fileList = list(chain.from_iterable(fileList))
            fileList.sort()
            print(fileList)
            for fName in fileList:
                if (len(filesUsed) == 0):
                    filesUsed = fName
                else:
                    filesUsed = filesUsed + ', ' + fName

                chlaFile = Dataset(fName)
                param = 'MW' + dtype
                chla = chlaFile.variables[param][:, :, :, :]
                chlaFile.close()
                chla = np.squeeze(chla)
                mean, num = meanVar(mean, num, chla)

        # Mask out any grid cells with zero observations, setting them to the fill value
        mean = ma.array(mean, mask=(num == 0), fill_value=-9999999.)

        # Switch to the working directory for output operations
        os.chdir(workDir)

        # Construct the output filename with start and end dates plus data types
        outFile = 'MW' + startYearC + startDoyC + '_' + endyearC + endDoyC + '_' + dtype + '.nc'

        # Create multi-day NetCDF file using the mean and count arrays
        ncFile = makeNetcdf(mean, num, interval, outFile, filesUsed, workDir)

        # Directory on the remote server for storing the multi-day data product
        remote_dir = '/MW/' + dtype + '/'

        # Transfer the generated NetCDF file to the remote server directory
        send_to_servers(ncFile, remote_dir , str(interval))

```

### CompMWChlamday

`CompMWChlamday.py` reads 1-day composite MW NetCDFs for chlorophyll-a, Kd490, PAR(0), and CFLH and accumulates them over a user-defined interval on a regular 0.0125° x 0.0125° grid spanning longitude 205°–255° and latitude 22°–51°. It computes the monthly mean by summing and counting valid observations, masks out grid cells with no data, writes CF-compliant NetCDF composites, and upload to a directory on a remote server Find the monthly product on ERDDAP [here](modisa_scripts.qmd#modisa-products-on-swfsc-erddap-server).

The following chart summarizes the script's workflow:
```{mermaid}
%%{init: {"flowchart":{"htmlLabels":true}}}%%
flowchart LR
  I("<b>Inputs</b><br/>• 1-day composite NetCDFs (dataDir)<br/>• workDir<br/>• endYear & endDoy<br/>• startDoy")
    --> P("<b>Processing</b><br/>• Parse args & compute interval<br/>• Compute start/end dates<br/>• Clear workDir & initialize mean/num arrays<br/>• Gather files spanning startDoy…endDoy<br/>• Loop: open each file & update mean/num<br/>• Mask zero‐observation pixels")
    --> O("<b>Output</b><br/>• CF-compliant monthly composite NetCDF<br/>• Deployed to /MW/chla/mday/")
```

```{python}
#| eval: false
#| code-fold: true
#| code-summary: "View CompMWChlamday.py"

"""
Overview
--------
Generate monthly composites of chlorophyll-a, Kd_490, PAR0, and CFLH
for the MODIS MW (West Coast) region by accumulating 1-day NetCDF products.

Usage
-----
::
 
     python CompMWChlamday.py <dataDir> <workDir> <endYear> <endDoy> <startDoy>

Description
-----------
1. **Parse arguments & compute interval** 

  - Read ``dataDir``, ``workDir``, ``endYear``, ``endDoy``, and ``startDoy``.  
  
  - Compute the number of days ``interval = endDoy - startDoy + 1``.

2. **Compute calendar dates**

  - Convert ``startDoy``/``endDoy`` to ``datetime`` for logging/output paths.

3. **Loop over each parameter**

  For each in ``['chla','k490','par0','cflh']``:

     - **Initialize sum & count arrays** of shape 2321x4001.

     - **Gather all matching 1-day NetCDF files** between ``startDoy``…``endDoy``, handling both same-year and wrap-around cases.

     - **For each file**:

         - Open with ``netCDF4.Dataset``, read 4D variable ``MW<param>``, squeeze to 2D.

         - Update running totals (``mean``) and counts (``num``) via ``meanVar()``.

     - **Mask out** grid cells with zero observations (``num==0``), fill with -9999999.0.

     - **Write** composite via ``makeNetcdfmDay()``.

     - **Send** result to ``/MW/<param>/mday/`` on the server.

Dependencies
------------
- **Python 3.x**

- **Standard library:** ``os``, ``sys``, ``glob``, ``itertools.chain``, ``datetime``, ``timedelta``

- **Third-party:** ``numpy``, ``numpy.ma``, ``netCDF4``

- **Custom roylib functions:**

  - ``isleap(year)``

  - ``meanVar(sum_array, count_array, data_slice)``

  - ``makeNetcdfmDay(mean, num, interval, outFile, filesUsed, workDir)``

  - ``send_to_servers(ncFile, remote_dir, 'm')``

Directory Structure
-------------------
- **Input Directory (dataDir):**

  Contains 1-day NetCDFs named ``MW<YYYY><DDD>_<param>.nc``.

- **Working Directory (workDir):**

  Scratch space; cleared each loop.

- **Output location (remote):**

  ``/MW/chla/mday/``, ``/MW/k490/mday/``, etc.

Usage Example
-------------
Generate a composite for the month of January 2025 (DOY 001-031):

::
 
     python CompMWChlamday.py /data/mw/1day/ /tmp/mwwork/ 2025 031 001

This command will:

  - Read daily files ``MB2025001..Mb2025031``, computes the January composite.

  - Writes MB2025001_2025031_chla.nc in ``/tmp/mw_work/``.

  - Uploads it to ``/MB/chla/`` on the server.

"""
from __future__ import print_function
from builtins import str
from builtins import range

if __name__ == "__main__":
    from datetime import datetime, timedelta
    import glob
    from itertools import chain
    from netCDF4 import Dataset
    import numpy as np
    import numpy.ma as ma
    import os
    import sys

    # Ensure 'roylib' is on the import path
    sys.path.append('/home/cwatch/pythonLibs')
    from roylib import *

    # Directory with 1-day MW NetCDFs
    dataDir = sys.argv[1]

    # Temporary working directory
    workDir = sys.argv[2]

    # Composite end year
    endyearC = sys.argv[3]
    endyear = int(endyearC)

    # Start year 
    startYearC = endyearC
    startyear = int(startYearC)

    # Composite end day-of-year
    endDoyC = sys.argv[4]
    endDoyC = endDoyC.rjust(3, '0')
    endDoy = int(endDoyC)

    # Composite start day-of-year
    startDoyC = sys.argv[5]
    startDoyC = startDoyC.rjust(3, '0')
    startDoy = int(startDoyC)

    # Number of days in the composite
    interval = endDoy - startDoy + 1

    # Compute calendar dats for logging or directory creation
    myDateEnd = datetime(endyear, 1, 1) + timedelta(endDoy - 1)
    myDateStart = datetime(startyear, 1, 1) + timedelta(startDoy - 1)

    # Output directory on server
    outDir = '/ERDData1/modisa/data/modsiwc/' + endyearC + '/mday'

    print(dataDir)
    print(workDir)
    print(endyearC)
    print(endDoyC)
    print(interval)

    # List of variables to composite
    dtypeList = ['chla', 'k490', 'par0', 'cflh']
    for dtype in dtypeList:
        # Clear working directory
        os.chdir(workDir)
        os.system('rm -f *')

        # Move to input directory
        os.chdir(dataDir)

        # Initialize sum and count arrays
        mean = np.zeros((2321, 4001), np.single)
        num = np.zeros((2321, 4001), dtype=np.int32)

        # Composite within same calendar year
        if (endDoy > startDoy):
            doyRange = list(range(startDoy, endDoy + 1))
            fileList = []
            # Gather all matching NetCDF filenames for each DOY
            for doy in doyRange:
                doyC = str(doy)
                doyC = doyC.rjust(3, '0')
                myString = 'MW' + endyearC + doyC + '*' + dtype + '.nc'
                fileList.append(glob.glob(myString))

            # Flatten and sort the list
            fileList = list(chain.from_iterable(fileList))
            fileList.sort()

            filesUsed = ""
            print(fileList)

            # Loop over each file, accumulate the variable
            for fName in fileList:
                if (len(filesUsed) == 0):
                    filesUsed = fName
                else:
                    filesUsed = filesUsed + ', ' + fName

                chlaFile = Dataset(fName)
                param = 'MW' + dtype
                chla = chlaFile.variables[param][:, :, :, :]
                chlaFile.close()

                # Remove singleton dimensions -> 2D (lat x lon)
                chla = np.squeeze(chla)

                # Update running mean and count
                mean, num = meanVar(mean, num, chla)

        else:
            # Composite spans year boundary
            dataDir1 = dataDir
            dataDir1 = dataDir1.replace(endyearC, startYearC)
            # Determine days in start year
            if (isleap):
                endday = 366
            else:
                endday = 365

            # DOY startDoy -> end of startYearC
            fileList = []
            os.chdir(dataDir1)
            doyRange = list(range(startDoy, endday + 1))
            for doy in doyRange:
                doyC = str(doy)
                doyC = doyC.rjust(3, '0')
                myString = 'MW' + startYearC + doyC + '*' + dtype + '.nc'
                fileList.append(glob.glob(myString))
            fileList = list(chain.from_iterable(fileList))
            fileList.sort()

            filesUsed = ""
            print(fileList)
            for fName in fileList:
                if (len(filesUsed) == 0):
                    filesUsed = fName
                else:
                    filesUsed = filesUsed + ', ' + fName

                chlaFile = Dataset(fName)
                param = 'MW' + dtype
                chla = chlaFile.variables[param][:, :, :, :]
                chlaFile.close()
                chla = np.squeeze(chla)
                mean, num = meanVar(mean, num, chla)

            # DOY 1 -> endDoy in endYearC
            os.chdir(dataDir)
            fileList = []
            doyRange = list(range(1, endDoy + 1))
            for doy in doyRange:
                doyC = str(doy)
                doyC = doyC.rjust(3, '0')
                myString = 'MW' + endyearC + doyC + '*' + dtype + '.nc'
                fileList.append(glob.glob(myString))

            fileList = list(chain.from_iterable(fileList))
            fileList.sort()
            print(fileList)
            for fName in fileList:
                if (len(filesUsed) == 0):
                    filesUsed = fName
                else:
                    filesUsed = filesUsed + ', ' + fName

                chlaFile = Dataset(fName)
                param = 'MW' + dtype
                chla = chlaFile.variables[param][:, :, :, :]
                chlaFile.close()
                chla = np.squeeze(chla)
                mean, num = meanVar(mean, num, chla)

        # Mask out pixels with zero observations and set fill value for missing data 
        mean = ma.array(mean, mask=(num == 0), fill_value=-9999999.)

        # Return to working directory for output
        os.chdir(workDir)

        # Construct output filename
        outFile = 'MW' + startYearC + startDoyC + '_' + endyearC + endDoyC + '_' + dtype + '.nc'

        # Create multi-day NetCDF
        ncFile = makeNetcdfmDay(mean, num, interval, outFile, filesUsed, workDir)

        # Send to server folder for this parameter
        remote_dir = '/MW/' + dtype + '/'
        send_to_servers(ncFile, remote_dir , 'm')

        #intervalDay = str(interval) + 'day'
        # myCmd = 'scp ' + ncFile  + ' cwatch@192.168.31.15:/u00/satellite/MW/' + dtype + '/mday'
        #myCmd = 'rsync -tvh ' + ncFile  + ' cwatch@192.168.31.15:/u00/satellite/MW/' + dtype + '/mday/' + ncFile
        #os.system(myCmd)
        # myCmd = 'scp ' + ncFile  + ' cwatch@192.168.31.15:/u00/satelliteNAS/MW/' + dtype + '/mday'
        #myCmd = 'rsync -tvh ' + ncFile  + ' cwatch@192.168.31.15:/u00/satelliteNAS/MW/' + dtype + '/mday/' + ncFile
        #os.system(myCmd)
        # myCmd = 'scp ' + ncFile  + ' cwatch@161.55.17.28:/u00/satellite/MW/' + dtype +  '/mday'
        # myCmd = 'rsync -tvh ' + ncFile  + ' /u00/satellite/MW/' + dtype +  '/mday/' + ncFile
        #myCmd = 'rsync -tvh ' + ncFile  + ' cwatch@161.55.17.28:/u00/satellite/MW/' + dtype +  '/mday/' + ncFile
        #os.system(myCmd)

```

## SST
### makeSST1daynewMW

`makeSST1daynewMW.py` creates a 1-day composite by reading Level-2 SST swath NetCDFs for the given date, filters “Day” pixels with quality flag <2 that fall within longitude 205°–255° and latitude 22°–51°, and collates their longitude, latitude, and SST values. It then interpolates these observations onto a 0.0125° x 0.0125° regular grid using PyGMT, writes the result as a CF-compliant NetCDF, and uploads it to a directory on a remote server. Find the 1-day composite product on ERDDAP [here](modisa_scripts.qmd#modisa-products-on-swfsc-erddap-server).

The following chart summarizes the script's workflow:
```{mermaid}
%%{init: {"flowchart":{"htmlLabels":true}}}%%
flowchart LR
  I("<b>Inputs</b><br/>• Raw L2 SST NetCDF swaths (dataDir)<br/>• workDir<br/>• Year & DOY")
    --> P("<b>Processing</b><br/>• Compute calendar date & directories<br/>• Load static land mask<br/>• Discover swaths for Day N (HOD>10) & N+1 (HOD≤10)<br/>• Stage & filter swaths (Day-only, region overlap)<br/>• Extract, reshape & filter (SST + quality)<br/>• Interpolate onto 0.0125° grid<br/>• Apply land mask")
    --> O("<b>Output</b><br/>• CF-compliant 1-day composite NetCDF<br/>• Deployed to /MW/sstd/1day/")
```

```{python}
#| eval: false
#| code-fold: true
#| code-summary: "View makeSST1daynewMW.py"

"""
Overview
--------
Generate a one-day SST grid for the MODIS MW (West Coast) region by combining swaths
from Level-2 SST NetCDF files and gridding them with PyGMT. The final product is a CF-compliant
NetCDF file containing daily SST for the MW region (lon 205-255°, lat 22-51°).

Usage
-----
::

    python makeSST1daynewMW.py <dataDir> <workDir> <year> <doy>

Where:

- ``dataDir``

  Directory containing raw Level-2 SST NetCDF swaths for the target date, organized as ``<dataDirBase>/<YYYY><MM>/``. Each file must follow the pattern: ``AQUA_MODIS.<YYYY><MM><DD>T*.L2.SST.NRT.nc``.

- ``workDir`` 

  Temporary working directory. Swath files are copied here, intermediate products are created, and then cleaned up.

- ``year``

  Four-digit year string (e.g., ``"2025"``).

- ``doy``

  Three-digit day-of-year string (zero-padded, e.g., ``"082"`` for March 23).

Description
-----------
1. **Date and Directory Setup**  

   - Convert ``year`` + ``doy`` to a calendar date (``myDate``), then extract zero-padded month (``myMon``) and day (``myDay``).

   - Define ``datadir = dataDirBase + year + myMon + "/"``. This folder must contain all SST swath NetCDF files for the date.

   - Load a static land-mask grid from ``/u00/ref/landmasks/LM_205_255_0.0125_22_51_0.0125_gridline.grd`` into ``my_mask``.

2. **Swath Discovery**  

   - Change directory to ``datadir``.

   - Build a glob pattern: ``AQUA_MODIS.<year><myMon><myDay>*.L2.SST.NRT.nc``.

   - Sort the resulting list of swath filenames (``fileList``).

3. **Data Staging and Cleanup**  

   - Change directory to ``workDir``.

   - Remove any existing temporary files matching ``AQUA_MODIS.*L2.SST*`` or ``MW20*`` to start fresh.

4. **Loop Over Each Swath**  

   For each file ``fName`` in ``fileList``:

   a. **Copy and Open**  

      - Copy the swath from ``datadir`` to ``workDir``.

      - Open with ``netCDF4.Dataset``. If unreadable, skip the swath.

   b. **Extract Navigation (Swath Geometry)**  

      - Read ``latitude`` and ``longitude`` from the ``navigation_data`` group.

      - Convert any negative longitudes to the 0-360° range.

      - Compute swath extents: ``dataLonMin``, ``dataLonMax``, ``dataLatMin``, ``dataLatMax``.

      - Define tests for geographic overlap:

       - Longitude overlaps 205°-255°. 

       - Latitude overlaps 22°-51°.  

      - Check ``day_night_flag == "Day"``.

      - Only if all tests pass, append the swath to ``filesUsed``.

   c. **Extract SST and Quality**  

      - In the ``geophysical_data`` group, read:

        - ``sst`` (sea surface temperature)

        - ``qual_sst`` (quality flag)  

      - Reshape ``sst``, ``latitude``, and ``longitude`` into column vectors using ``myReshape``.

      - Flatten ``qual_sst`` into a 1D mask array (``qual_sst1``).

      - Stack ``longitude``, ``latitude``, and ``sst`` into a 2D ``dataOut`` array.

      - Apply filters to ``dataOut``:

        - Quality flag < 2.  

        - Longitude between 205° and 255°. 

        - Latitude between 22° and 51°.  

        - SST > -2 °C.  

      - Concatenate valid points into a master array ``temp_data``.

   d. **Cleanup Swath File**  

      - Close the NetCDF dataset.

      - Delete the swath file from ``workDir``.

5. **Gridding Swath Point Cloud**  

   - After processing all swaths, define output grid filename: ``MW<year><doy>_<year><doy>_sstd.grd``.

   - Set PyGMT region and spacing:

      - ``region = "205/255/22/51"``  

     - ``spacing = "0.0125/0.0125"``  

     - ``search_radius = "2k"``  

     - ``sectors = "1"``  

   - Then run the following to produce a gridded ``xarray.DataArray`` (``temp_grid``):

   ::

       pygmt.nearneighbor(
           data=temp_data,
           region=region,
           spacing=spacing,
           search_radius=search_radius,
           sectors=sectors
       )

6. **Convert to NetCDF and Send to Server**  

   Invoke:

   ``grd2netcdf1(temp_grid, fileOut, filesUsed, my_mask, "MW")``

   from ``roylib``:

   - Applies ``my_mask`` to zero out land pixels.

   - Computes coverage statistics (# observations, % coverage).

   - Generates a CF-compliant NetCDF skeleton via ``ncgen`` + a CDL template.

   - Populates coordinates, SST data, metadata, and a center-time stamp.

   - Returns the new NetCDF filename (``MW<year><doy>_<year><doy>_sstd.nc``).

   Then run:

   ::

       send_to_servers(ncFile, "/MW/sstd/", "1")

   to copy the NetCDF into ``/MW/sstd/1day/``.

   Finally, remove the local NetCDF copy.

Dependencies
------------
- **Python 3.x**

- **Standard library:**  ``os``, ``glob``, ``re``, ``shutil``, ``sys``, ``datetime``, ``timedelta``

- **Third-party packages:** ``netCDF4.Dataset``, ``numpy``, ``pygmt`` 

- **Custom roylib functions:**

   - ``myReshape(array)``
  
   - ``grd2netcdf1(grd, outName, filesUsed, mask, fType)``

   - ``safe_remove(filePath)``  

   - ``send_to_servers(ncFile, destDir, interval)``

   - ``isleap(year)``

   - ``makeNetcdf(mean, nobs, interval, outFile, filesUsed, workDir)``

   - ``meanVar(mean, num, obs)``

Land Mask
---------
- A static GRD file is required at:

  ``/u00/ref/landmasks/LM_205_255_0.0125_22_51_0.0125_gridline.grd``

- This mask is applied to the gridded SST to set land pixels to NaN.

Directory Structure
-------------------
- **Input swaths directory** (datadir):

  Must contain all Level-2 NetCDF swath files for the date, named: ``AQUA_MODIS.<YYYY><MM><DD>T*.L2.SST.NRT.nc``

- **Working directory** (workDir):  

  Temporary location where swaths are staged, processed, and removed.

- **Output grid** (fileOut):  

  A GMT “.grd” file named ``MW<year><doy>_<year><doy>_sstd.grd``.

- **Final NetCDF** (returned by grd2netcdf1):  

  Named ``MW<year><doy>_<year><doy>_sstd.nc``, then copied to ``/MW/sstd/1day/``.

Usage Example
-------------
Assume your raw SST swaths are in  ``/Users/you/modis_data/netcdf/202503/`` and your  working directory is ``/Users/you/modis_work/``:

::

    python makeSST1daynewMW.py /Users/you/modis_data/netcdf/202503/  /Users/you/modis_work/  2025 082

This will:

  - Copy all swaths matching ``AQUA_MODIS.20250323*.L2.SST.NRT.nc`` into ``/Users/you/modis_work/``.

  - Build a combined point cloud from valid “Day” pixels within the MW region.

  - Create ``MW2025082_2025082_sstd.grd`` via PyGMT nearneighbor.

  - Convert the grid to ``MW2025082_2025082_sstd.nc`` using ``grd2netcdf1``.

  - Copy the NetCDF to ``/MW/sstd/1day/`` and remove local copies.
"""
from __future__ import print_function
from builtins import str

if __name__ == "__main__":
    from datetime import datetime, timedelta
    import glob
    from itertools import chain
    from netCDF4 import Dataset
    import numpy as np
    import numpy.ma as ma
    import pygmt
    import os
    import re
    import shutil
    import sys

    # Ensure 'roylib' is on the import path
    sys.path.append('/home/cwatch/pythonLibs')
    from roylib import *

    # Geographic bounds for MW region
    latmax = 51.
    latmin = 22.
    lonmax = 255.
    lonmin = 205.

    outFile = 'modiswcSSTtemp'

    # Set data directory
    datadirBase = sys.argv[1]

    # Set work directory
    workdir = sys.argv[2]

    # Get the year and doy from the command line
    year = sys.argv[3]
    doy = sys.argv[4]
    print(year)
    print(doy)

    # Convert year/doy to a calendar date and zero-pad month/day
    myDate = datetime(int(year), 1, 1) + timedelta(int(doy) - 1)
    myMon = str(myDate.month)
    myMon = myMon.rjust(2, '0')
    myDay = str(myDate.day).rjust(2, '0')

    # Construct the directory path
    datadir = datadirBase + year + myMon + '/'

    # Load static land mask from GRD
    mask_root = Dataset('/u00/ref/landmasks/LM_205_255_0.0125_22_51_0.0125_gridline.grd')
    my_mask = mask_root.variables['z'][:, :]
    mask_root.close()

    # Now move to the data directory
    os.chdir(datadir)

    # Set up the string for the file search in the data directory
    myString = 'AQUA_MODIS.' + year + myMon + myDay  + '*.L2.SST.NRT.nc'

    # Get list of files in the data directory that match with full path
    fileList = glob.glob(myString)
    # print(fileList)
    fileList.sort()

    # Now move to the work directory and clear old files
    os.chdir(workdir)
    os.system('rm -f AQUA_MODIS.*L2.SST*')
    os.system('rm -f MW20*')

    # Prepare variables to accumulate point-cloud data and track provenance
    filesUsed = ""
    temp_data = None

    # Loop through each swath filename for Day N
    for fName in fileList:
        fileName = fName
        print(fileName)
        shutil.copyfile(datadir + fName, workdir + fName)
        try:
            rootgrp = Dataset(fileName, 'r')
        except IOError:
            print("bad file " + fileName)
            continue

        # Extract navigation-group data
        navDataGroup = rootgrp.groups['navigation_data']
        latitude = navDataGroup.variables['latitude'][:, :]
        longitude = navDataGroup.variables['longitude'][:, :]

        # Convert any negative longitudes to the 0-360° domain
        longitude[longitude < 0] = longitude[longitude < 0] + 360

        # Compute swath extents (min/max) for geographic filtering
        dataLonMin = np.nanmin(longitude[longitude >= 0])
        dataLonMax = np.nanmax(longitude[longitude <= 360])
        dataLatMin = np.nanmin(latitude[latitude >= -90])
        dataLatMax = np.nanmax(latitude[latitude <= 90])

        # Determine if swath overlaps our global MW region
        goodLon1 = (dataLonMin < lonmin) and (dataLonMax >= lonmin)
        goodLon2 = (dataLonMin >= lonmin) and (dataLonMin <= lonmax)
        goodLon = goodLon1 or goodLon2

        goodLat1 = (dataLatMin < latmin) and (dataLatMax >= latmin)
        goodLat2 = (dataLatMin >= latmin) and (dataLatMin <= latmax)
        goodLat = goodLat1 or goodLat2

        # Check if swath is daytime (only keep "Day" pixels)
        dayNightTest = (rootgrp.day_night_flag == 'Day')

        # Only proceed if geography and day-night tests pass
        if (goodLon and goodLat and dayNightTest):
            # Add filename to provenance list
            if (len(filesUsed) == 0):
                filesUsed = fileName
            else:
                filesUsed = filesUsed + ', ' + fileName

            # Extract geophysical data and reshape
            geoDataGroup = rootgrp.groups['geophysical_data']
            sst = geoDataGroup.variables['sst'][:, :]
            sst = myReshape(sst)
            qual_sst = geoDataGroup.variables['qual_sst'][:, :]
            qual_sst1 = qual_sst.flatten()
            latitude = myReshape(latitude)
            longitude = myReshape(longitude)

            # Stack (lon, lat, sst) into a single 2D array with shape (N, 3)
            dataOut = np.hstack((longitude, latitude, sst))

            # Keep only quality < 2, valid SST > -2°C, and within MW region
            qualTest = qual_sst1 < 2
            dataOut = dataOut[qualTest]
            dataOut = dataOut[dataOut[:, 0] > -400]
            dataOut = dataOut[dataOut[:, 0] >= lonmin]
            dataOut = dataOut[dataOut[:, 0] <= lonmax]
            dataOut = dataOut[dataOut[:, 1] >= latmin]
            dataOut = dataOut[dataOut[:, 1] <= latmax]
            dataOut = dataOut[dataOut[:, 2] > -2]

            # Accumulate into temp_data array
            if (dataOut.shape[0] > 0):
                if (temp_data is None):
                    temp_data = dataOut
                else:
                    temp_data = np.concatenate((temp_data, dataOut), axis=0)

        # Close the NetCDF and remove swath copy from workdir
        rootgrp.close()
        os.remove(fileName)

    # Build and write the GMT grid from the accumulated point cloud
    # Define output grid filename
    fileOut = 'MW' + year + doy + '_' + year + doy + '_sstd.grd'
    range = '205/255/22/51'
    increment = '0.0125/0.0125'
    smooth = '2k'

    # Use PyGMT nearneighbor to interpolate scattered (lon, lat, sst) points onto a grid
    temp_data1 = pygmt.nearneighbor(
        data=temp_data,
        region=range,
        spacing=increment,
        search_radius=smooth,
        sectors='1'
    )

    # Convert the GMT grid to a CF-compliant NetCDF and send to server
    # Apply the land mask, create a NetCDF via a CDL template, and add metadata
    ncFile = grd2netcdf1(temp_data1, fileOut, filesUsed, my_mask, 'MW')

    #myCmd = "mv " + ncFile + " /home/cwatch/pygmt_test/outfiles"
    #os.system(myCmd)

    # Copy the final NetCDF to the "MW" server folder for 1-day products
    send_to_servers(ncFile, '/MW/sstd/', '1')

    # Remove the local NetCDF to copy from workdir
    os.remove(ncFile)

```

### CompMWSST

`CompMWSST.py` computes a multi-day (3-, 8-, & 14-) SST composite by reading daily 1-day SST NetCDFs over the specified interval and averaging them on a regular 0.0125° x 0.0125° grid covering longitude 205°–255° and latitude 22°–51°. It masks out pixels with no observations, writes the result as a CF-compliant NetCDF, and uploads it to a directory on a remote server. Find the multi-day composite products on ERDDAP [here](modisa_scripts.qmd#modisa-products-on-swfsc-erddap-server).

The following chart summarizes the script's workflow:
```{mermaid}
%%{init: {"flowchart":{"htmlLabels":true}}}%%
flowchart LR
  I("<b>Inputs</b><br/>• 1-day composite NetCDFs (dataDir)<br/>• workDir<br/>• endYear & endDoy<br/>• interval (days)")
    --> P("<b>Processing</b><br/>• Parse args & compute start/end DOY<br/>• Clear workDir & initialize mean/num arrays<br/>• Gather files over date range<br/>• Loop: open each file & update mean/num<br/>• Mask zero-observation pixels")
    --> O("<b>Output</b><br/>• CF-compliant multi-day composite NetCDF<br/>• Deployed to /MW/sstd/interval day/")
```


```{python}
#| eval: false
#| code-fold: true
#| code-summary: "View CompMWSST.py"

"""
Overview
--------
Generate multi-day SST composites for the MODIS MW (West Coast) dataset by averaging
1-day NetCDF files over a specified interval (e.g., 3, 5, 8, or 14 days).

Usage
-----
::
  
    python CompMWSST.py <dataDir> <workDir> <endYear> <endDoy> <interval>

Where:

- ``dataDir``

  Directory containing the daily MW SST NetCDF files named ``MW<YYYY><DDD>*sstd.nc``.

- ``workDir``

  Temporary working directory for intermediate files.

- ``endYear`` 

  Four-digit year of the last day in the composite (e.g., ``2025``).

- ``endDoy``

  Three-digit day-of-year of the last day (e.g., ``082``).

- ``interval``

  Number of days to include (e.g., `3`, `5`, `8`, `14`).

Description
-----------
1. **Parse arguments & compute dates**  

  - Read paths and parameters from ``sys.argv``.
  
  - Convert ``endYear``+``endDoy`` to a calendar date ``myDateEnd``.
  
  - Compute ``myDateStart = myDateEnd - (interval-1) days``.
  
  - Zero-pad start and end DOY strings.

2. **Initialize**

  - Change into ``workDir`` and clear old files.

  - Change into ``dataDir``.

  - Preallocate two 2321x4001 arrays:

      - ``mean`` (running sum of SST)

      - ``num``  (count of observations)

3. **Collect 1-day files**

  - If ``startDoy ≤ endDoy`` (same year), gather DOYs ``startDoy…endDoy``.

  - Else, handle year-boundary wrap: DOYs ``startDoy…endOfYear`` and ``1…endDoy``.

4. **Accumulate SST**

  For each matching file:

     - Read and squeeze the 4D SST variable ``"MWsstd"``.
     
     - Update ``mean, num`` via ``meanVar(mean, num, sst)``.

5. **Finalize composite**

  - Mask out cells with zero observations (``num==0``), setting fill value ``-9999999.``.

6. **Write & deploy**

  - Change back to ``workDir``.

  - Call ``makeNetcdf(mean, num, interval, outFile, filesUsed, workDir)`` to create CF-compliant NetCDF.

  - Transfer result via ``send_to_servers(ncFile, "/MW/sstd/", str(interval))``.

Dependencies
------------
- **Python 3.x**

- **Standard library:** ``os``, ``sys``, ``glob``, ``itertools.chain``, ``datetime``, ``timedelta``  

- **Third-party:** ``netCDF4.Dataset``, ``numpy``, ``numpy.ma``  

- **Custom roylib functions:**

  - ``isleap(year)``

  - ``meanVar(sum_array, count_array, data_slice)``

  - ``makeNetcdfmDay(mean, num, interval, outFile, filesUsed, workDir)``

  - ``send_to_servers(ncFile, remote_dir, 'm')``

Directory Structure
-------------------
- **Input Directory** (dataDir):

  ``MW<YYYY><DDD>*sstd.nc`` (1-day SST files)

- **Working directory** (workDir):  

  Temporary staging for intermediate files  

- **Output location** (remote):  

  ``/MW/sstd/<interval>day/`` on the server  

Usage Example
-------------
Generate a 5-day composite ending on 2025 day-of-year 082

::
 
   python CompMWSST.py /Users/you/modis_data/mw/1day/ /Users/you/modis_work/ 2025 082 5

This command will:

  - Read the five daily files MW202508?*sstd.nc for DOY 078-082 

  - Compute the 5-day running average for each grid cell  

  - Write ``MW2025078_2025082_sstd.nc`` in ``/Users/you/modis_work/`` 
   
  - Upload it to ``/MW/sstd/5day/`` on the server 
"""
from __future__ import print_function
from builtins import str
from builtins import range

if __name__ == "__main__":
    from datetime import datetime, timedelta
    import glob
    from itertools import chain
    from netCDF4 import Dataset
    import numpy as np
    import numpy.ma as ma
    import os
    import sys

    # Ensure 'roylib' is on the import path
    sys.path.append('/home/cwatch/pythonLibs')
    from roylib import *

    # Directory with 1-day MW SST NetCDFs
    dataDir = sys.argv[1]

    # Temporary working directory
    workDir = sys.argv[2]

    # Composite end year (YYYY)
    endyearC = sys.argv[3]

    # Composite end day-of-year (DDD)
    endDoyC = sys.argv[4]
    endDoyC = endDoyC.rjust(3, '0')

    # Integer form of end day-of-year
    endDoy = int(endDoyC)

    # Composite length as string
    intervalC = sys.argv[5]

    print(dataDir)
    print(intervalC)

    # Composite length as integer
    interval = int(intervalC)

    # Convert end Doy to calendar date
    myDateEnd = datetime(int(endyearC), 1, 1) + timedelta(int(endDoyC) - 1)

    # Start date = end date minus (interval-1) days
    myDateStart = myDateEnd + timedelta(days=-(interval - 1))

    # Zero-padded start Doy and year
    startDoyC = myDateStart.strftime("%j").zfill(3)
    startDoy = int(startDoyC)
    startYearC = str(myDateStart.year)

    # Prepare output directory 
    outDir = '/ERDData1/modisa/data/modiswc/' + endyearC + '/' + intervalC + 'day'

    print(dataDir)
    print(workDir)
    print(endyearC)
    print(endDoyC)
    print(intervalC)

    ###
    # dtypeList = ['sstd']
    # for dtype in dtypeList:

    # Data type for SST composites
    dtype = 'sstd'

    # Clear working directory
    os.chdir(workDir)
    os.system('rm -f *')

    # Move to data directory
    os.chdir(dataDir)

    # Preallocate sum (mean) and count arrays matching grid dims
    mean = np.zeros((2321, 4001), np.single)
    num = np.zeros((2321, 4001), dtype=np.int32)

    # Collect all NetCDF files spanning startDoy..endDoy
    if (endDoy > startDoy):
        # Same-year composite
        doyRange = list(range(startDoy, endDoy + 1))
        fileList = []
        for doy in doyRange:
            doyC = str(doy)
            doyC = doyC.rjust(3, '0')
            myString = 'MW' + endyearC + doyC + '*' + dtype + '.nc'
            fileList.append(glob.glob(myString))

        # Flatten nested lists and sort
        fileList = list(chain.from_iterable(fileList))
        fileList.sort()
        print(fileList)

        filesUsed = ""
        for fName in fileList:
            # Build comma-separated list of files used
            if (len(filesUsed) == 0):
                filesUsed = fName
            else:
                filesUsed = filesUsed + ', ' + fName

            # Open NetCDF file and extract 4D SST variable
            sstFile = Dataset(fName)
            sst = sstFile.variables["MWsstd"][:, :, :, :]
            sstFile.close()

            # Squeeze to remove singleton dimensions -> 2D array
            sst = np.squeeze(sst)

            # Update running mean and count arrays
            mean, num = meanVar(mean, num, sst)

    else:
        # Compute directory for start of year by replacing year in path
        dataDir1 = dataDir
        dataDir1 = dataDir1.replace(endyearC, startYearC)

        # Determine end-of-year DOY based on leap year
        if(isleap):
            endday = 366
        else:
            endday = 365

        # From startDoy through end of startYearC
        fileList = []
        os.chdir(dataDir1)
        doyRange = list(range(startDoy, endday + 1))
        for doy in doyRange:
            doyC = str(doy)
            doyC = doyC.rjust(3, '0')
            myString = 'MW' + startYearC + doyC + '*' + dtype + '.nc'
            fileList.append(glob.glob(myString))

        fileList = list(chain.from_iterable(fileList))
        fileList.sort()
        print(fileList)

        filesUsed = ''
        for fName in fileList:
            if (len(filesUsed) == 0):
                filesUsed = fName
            else:
                filesUsed = filesUsed + ', ' + fName

            sstFile = Dataset(fName)
            sst = sstFile.variables["MWsstd"][:, :, :, :]
            sstFile.close()
            sst = np.squeeze(sst)
            mean, num = meanVar(mean, num, sst)

        # From DOY=1 of endyearC through endDoy
        os.chdir(dataDir)
        fileList = []
        doyRange= list(range(1, endDoy + 1))
        for doy in doyRange:
            doyC = str(doy)
            doyC = doyC.rjust(3, '0')
            myString = 'MW' + endyearC + doyC + '*' + dtype + '.nc'
            fileList.append(glob.glob(myString))

        fileList = list(chain.from_iterable(fileList))
        fileList.sort()
        print(fileList)

        for fName in fileList:
            if (len(filesUsed) == 0):
                filesUsed = fName
            else:
                filesUsed = filesUsed + ', ' + fName

            sstFile = Dataset(fName)
            sst = sstFile.variables["MWsstd"][:, :, :, :]
            sstFile.close()
            sst = np.squeeze(sst)
            mean, num = meanVar(mean, num, sst)

    # Mask out any grid cells with zero observations, setting them to the fill value
    mean = ma.array(mean, mask=(num == 0), fill_value=-9999999.)
    print('COmpMWSST finished mean')

    # Switch to the working directory for output operations
    os.chdir(workDir)

    # Construct the output filename with start and end dates plus data types
    outFile = 'MW' + startYearC + startDoyC + '_' + endyearC + endDoyC + '_' + dtype + '.nc'

    # Create multi-day NetCDF file using the mean and count arrays
    ncFile = makeNetcdf(mean, num, interval, outFile, filesUsed, workDir)

    # Directory on the remote server for storing the multi-day SST product
    remote_dir = '/MW/sstd/'

    # Transfer the generated NetCDF file to the remote server directory, labeling it with the interval
    send_to_servers(ncFile, remote_dir , str(interval))

```

### CompMWSSTmday

`CompMWSSTmday.py` computes a monthly composite SST by reading 1-day composite MW SST NetCDFs over the specified date range and averaging them on a 0.0125° x 0.0125° regular grid covering longitude 205°–255° and latitude 22°–51°. It then masks out any pixels with no valid observations, writes the result as a CF-compliant NetCDF, and uploads it to a directory on a remote server. Find the monthly composite product on ERDDAP [here](modisa_scripts.qmd#modisa-products-on-swfsc-erddap-server).

The following chart summarizes the script's workflow:
```{mermaid}
%%{init: {"flowchart":{"htmlLabels":true}}}%%
flowchart LR
  I("<b>Inputs</b><br/>• 1-day composite NetCDFs (dataDir)<br/>• workDir<br/>• endYear & endDoy<br/>• startDoy")
    --> P("<b>Processing</b><br/>• Parse args & compute interval<br/>• Compute start/end dates<br/>• Clear workDir & initialize mean/num arrays<br/>• Gather files spanning startDoy…endDoy<br/>• Loop: open each file & update mean/num<br/>• Mask zero‐observation pixels")
    --> O("<b>Output</b><br/>• CF-compliant monthly composite NetCDF<br/>• Deployed to /MW/sstd/mday/")
```

```{python}
#| eval: false
#| code-fold: true
#| code-summary: "View CompMWSSTmday.py"

"""
Overview
--------
Compute monthly SST composites for the MODIS MW (West Coast) region by averaging
daily 1-day SST NetCDF products over a specified interval.

Usage
-----
::
  
    python CompMWSSTmday.py <dataDir> <workDir> <endYear> <endDoy> <startDoy>

Where:

- ``dataDir``

  Directory containing 1-day NetCDF files named ``MW<YYYY><DDD>*sstd.nc``

- ``workDir``

  Temporary working directory for intermediate files

- ``endYear``   

  Four-digit year of the last day in the composite (e.g., ``2025``)

- ``endDoy``

  Three-digit day-of-year of the last day (e.g., ``082``)

- ``startDoy``

  Three-digit day-of-year of the first day (e.g., ``075``)

Description
-----------
1. **Parse command-line arguments** and compute the composite length:

  - Read ``dataDir``, ``workDir``, ``endYear``, ``endDoy``, ``startDoy``.

  - Zero-pad ``endDoy`` and ``startDoy``, compute ``interval = endDoy - startDoy + 1``.

2. **Convert endDoy to a calendar date** (``myDateEnd``) for logging or output paths.

3. **Gather all 1-day SST files** spanning ``startDoy..endDoy``:

  - Build glob patterns ``MW<year><DDD>*sstd.nc``

  - Handles same-year and year-boundary wrap-around cases.

4. **Initialize accumulators**:

  - ``mean`` (float32) and ``num`` (int32) arrays of shape 2321x4001.

5. **Loop over each NetCDF**:

  - Open with ``Dataset()``.

  - Read the 4D variable ``MWsstd``, squeeze to 2D.

  - Update running ``mean`` and ``num`` via ``meanVar(mean, num, sst)``.

6. **Mask unobserved cells** (``num ==0``) and set fill value ``-9999999``.

7. **Write composite**:

  - Call ``makeNetcdfmDay(mean, num, interval, outFile, filesUsed, workDir)`` to produce a CF-compliant NetCDF.

8. **Transfer result**:

  - Use ``send_to_servers(ncFile, '/MW/sstd/', 'm')`` to copy the composite to the remote directory.

Dependencies
------------
- **Python 3.x**

- **Standard library:** ``os``, ``sys``, ``glob``, ``itertools.chain``, ``datetime``, ``timedelta``

- **Third-party:** ``numpy``, ``numpy.ma``, ``netCDF4.Dataset``

- **Custom roylib functions:**

  - ``isleap(year)``

  - ``meanVar(sum_array, count_array, data_slice)``

  - ``makeNetcdfmDay(mean, num, interval, outFile, filesUsed, workDir)``

  - ``send_to_servers(ncFile, remote_dir, 'm')``

Directory Structure
-------------------
- **Input directory** (dataDir):

  Contains files like ``MWYYYYDDD*sstd.nc``, one per day.

- **Working directory** (workDir):

  Cleared and used for any temporary artifacts (none persisted).

- **Output Location** (via ``makeNetcdfmDay``):

  Writes ``MW<startYYYY><startDDD>_<endYYYY><endDDD>_sstd.nc`` in ``<workDir>`` and then copies it to ``/MW/sstd/``.

Usage Example
-------------
Create a January 2025 composite (DOY 001-031):

::
 
   python CompMWSSTmday.py /path/to/MW/1day/ /path/to/tmp/ 2025 031 001

This command will:

  - Read daily SST files ``MW2025001*…MW2025031*`` from ``/path/to/MW/1day/``.

  - Compute the 31-day average for each grid cell.

  - Write ``MW2025001_2025031_sstd.nc`` in ``/path/to/tmp/``. 

  - Upload the composite to ``/MW/sstd/`` on the server.
"""
from __future__ import print_function
from builtins import str
from builtins import range

if __name__ == "__main__":
    from datetime import datetime, timedelta
    import glob
    from itertools import chain
    from netCDF4 import Dataset
    import numpy as np
    import numpy.ma as ma
    import os
    import sys

    # Ensure 'roylib' is on the import path
    sys.path.append('/home/cwatch/pythonLibs')
    from roylib import *

    # Directory containing daily MW NetCDF files
    dataDir = sys.argv[1]

    # Temporary working directory
    workDir = sys.argv[2]

    # End date (year, day-of-year)
    endyearC = sys.argv[3]
    endDoyC = sys.argv[4]
    endDoyC = endDoyC.rjust(3, '0')

    # Start day-of-year (same year)
    startYearC = endyearC
    endDoy = int(endDoyC)
    startDoyC = sys.argv[5]
    startDoyC = startDoyC.rjust(3, '0')
    startDoy = int(startDoyC)

    # Number of days in the composite
    interval = endDoy - startDoy + 1

    # Convert end date to calendar dates
    myDateEnd = datetime(int(endyearC), 1, 1) + timedelta(endDoy - 1)
    myDateStart = myDateEnd + timedelta(startDoy - 1)

    # Prepare output directory
    outDir = '/ERDData1/modisa/data/modiswc/' + endyearC + '/mday'
    print(dataDir)
    print(workDir)
    print(endyearC)
    print(endDoyC)
    print(interval)

    ###
    # dtypeList = ['sstd']
    # for dtype in dtypeList:

    # Set up for reading MB SST variable ("MWsstd") across multiple days
    dtype = 'sstd'

    # Clear working directory
    os.chdir(workDir)
    os.system('rm -f *')

    # Move to data directory
    os.chdir(dataDir)

    # Preallocate sum (mean) and count arrays matching grid dims
    mean = np.zeros((2321, 4001), np.single)
    num = np.zeros((2321, 4001), dtype=np.int32)

    # Build list of NetCDF files spanning startDoy..endDoy
    if (endDoy > startDoy):
        # Composite within the same calendar year
        doyRange = list(range(startDoy, endDoy + 1))
        fileList = []
        for doy in doyRange:
            doyC = str(doy)
            doyC = doyC.rjust(3, '0')
            myString = 'MW' + endyearC + doyC + '*' + dtype + '.nc'
            # find all files matching MWYYYYDD*<dtype>.nc
            fileList.append(glob.glob(myString))

        # Flatten list of lists and sort alphabetically
        fileList = list(chain.from_iterable(fileList))
        fileList.sort()
        print(fileList)

        # Track which files were used
        filesUsed = ""
        for fName in fileList:
            if (len(filesUsed) == 0):
                filesUsed = fName
            else:
                filesUsed = filesUsed + ', ' + fName

            # Open NetCDF, extract the 4D SST array, then squeeze to 2D
            sstFile = Dataset(fName)
            sst = sstFile.variables["MWsstd"][:, :, :, :]
            sstFile.close()
            sst = np.squeeze(sst)

            # Update running mean and count arrays
            mean, num = meanVar(mean, num, sst)

    else:
        # Compute directory for start of year by replacing year in path
        dataDir1 = dataDir
        dataDir1 = dataDir1.replace(endyearC, startYearC)

        # Determine end-of-year DOY based on leap year
        if (isleap):
            endday = 366
        else:
            endday = 365

        # From startDoy through end of start year  
        fileList = []
        os.chdir(dataDir1)
        doyRange = list(range(startDoy, endday + 1))
        for doy in doyRange:
            doyC = str(doy)
            doyC = doyC.rjust(3, '0')
            myString = 'MW' + startYearC + doyC + '*' + dtype + '.nc'
            fileList.append(glob.glob(myString))

        fileList = list(chain.from_iterable(fileList))
        fileList.sort()
        print(fileList)
        filesUsed = ''
        for fName in fileList:
            if (len(filesUsed) == 0):
                filesUsed = fName
            else:
                filesUsed = filesUsed + ', ' + fName

            sstFile = Dataset(fName)
            sst = sstFile.variables["MWsstd"][:, :, :, :]
            sstFile.close()
            sst = np.squeeze(sst)
            mean, num = meanVar(mean, num, sst)

        # From DOY 1 of end year through endDoy
        os.chdir(dataDir)
        fileList = []
        doyRange = list(range(1, endDoy + 1))
        for doy in doyRange:
            doyC = str(doy)
            doyC = doyC.rjust(3, '0')
            myString = 'MW' + endyearC + doyC + '*' + dtype + '.nc'
            fileList.append(glob.glob(myString))

        fileList = list(chain.from_iterable(fileList))
        fileList.sort()
        print(fileList)

        for fName in fileList:
            if (len(filesUsed) == 0):
                filesUsed = fName
            else:
                filesUsed = filesUsed + ', ' + fName

            sstFile = Dataset(fName)
            sst = sstFile.variables["MWsstd"][:, :, :, :]
            sstFile.close()
            sst = np.squeeze(sst)
            mean, num = meanVar(mean, num, sst)

    # Mask out pixels with zero observations and set fill value for missing data
    mean = ma.array(mean, mask=(num == 0), fill_value=-9999999.)

    # Switch to the working directory
    os.chdir(workDir)

    # Construct output filename
    outFile = 'MW' + startYearC + startDoyC + '_' + endyearC + endDoyC + '_' + dtype + '.nc'

    # Generate the multi-day NetCDF file
    ncFile = makeNetcdfmDay(mean, num, interval, outFile, filesUsed, workDir)

    # Directory on the remote server where multi-day SST files are stored
    remote_dir = '/MW/sstd/'

    # Send the NetCDF to the remote server directory
    send_to_servers(ncFile, remote_dir , 'm')

```