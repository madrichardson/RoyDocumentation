[
  {
    "objectID": "roylib.html",
    "href": "roylib.html",
    "title": "Utility Library",
    "section": "",
    "text": "The roylib.py script is a core utilities library providing array‐reshaping, authenticated file downloads, and safe file operations, plus helpers for incremental statistics (mean/count), API-driven file discovery, and CF‐compliant NetCDF generation. It also handles remote deployments via rsync and leap‐year checks. The script import its functions (e.g., myReshape, get_netcdfFile, meanVar, makeNetcdfmDay, send_to_servers) at the top of your processing scripts. roylib.py provides shared functions used by many of the Python scripts on saltydog.\nFor full details on every function in roylib.py, see the API reference generated by Sphinx here.\n\n\nView roylib.py\nfrom __future__ import print_function\nfrom __future__ import division\nfrom future import standard_library\n\nstandard_library.install_aliases()\nfrom builtins import str\nfrom past.utils import old_div\nfrom datetime import date, datetime, timedelta\nfrom netCDF4 import Dataset, num2date, date2num\nimport numpy as np\nimport numpy.ma as ma\nimport os\nimport shutil\nimport subprocess\nimport time\nimport urllib.request, urllib.parse, urllib.error\nimport urllib.request, urllib.error, urllib.parse\n\n\ndef myReshape(dataArray):\n    \"\"\"\n    Flatten an N-dimensional array into a 2D column vector of type float32.\n\n    This helper is used before gridding routines that expect a single column of\n    values (shape: `(n_pixels, 1)`). If `dataArray` is a `numpy.ma.MaskedArray`,\n    masked entries are preserved in the output.\n\n    Parameters\n    ----------\n    dataArray : numpy.ndarray or numpy.ma.MaskedArray\n        Any N-dimensional array (e.g., `(n, m)`, `(n,)`, or higher rank).\n        If a masked array is provided, masks are preserved in the result.\n\n    Returns\n    -------\n    numpy.ma.MaskedArray or numpy.ndarray\n        A 2D array of shape `(dataArray.size, 1)` with dtype `float32`.\n        - If `dataArray` was a masked array, the result is a masked array.\n        - Otherwise, the result is a regular `ndarray` of `float32`.\n    \"\"\"\n    dataArray = dataArray.reshape(dataArray.size, 1)\n    dataArray = np.asarray(dataArray, np.float32)\n    return dataArray\n\n\ndef get_netcdfFile(fileName):\n    \"\"\"\n    Download a NetCDF file from the NASA OceanColor server using wget.\n\n    This function builds and executes a wget command that uses stored URS cookies\n    for authentication. The target file is fetched from the NASA OceanColor `getfile`\n    endpoint, which requires a valid URS session. The downloaded file is saved to\n    the current working directory under its original name.\n\n    Parameters\n    ----------\n    fileName : str\n        The name of the remote NetCDF file to download (e.g.,\n        `'A2023123006000.L2_LAC_SST.nc'`). This is appended to the base URL\n        `https://oceandata.sci.gsfc.nasa.gov/ob/getfile/` to form the full download URL.\n\n    Returns\n    -------\n    None\n        This function does not return a value. On success, the file appears in the\n        current directory. Existing files with the same name are overwritten.\n\n    Raises\n    ------\n    RuntimeError\n        If the wget command returns a non-zero exit status, indicating a download failure.\n    OSError\n        If the operating system command cannot be executed or if required tools are missing.\n    \"\"\"\n\n    baseURL = '\"https://oceandata.sci.gsfc.nasa.gov/ob/getfile/'\n    wgetCommand = (\n        \"/usr/bin/wget -4 --load-cookies ~/.urs_cookies --save-cookies ~/.urs_cookies --auth-no-challenge=on --keep-session-cookies --content-disposition --no-check-certificate \"\n        + baseURL\n        + fileName\n        + '\"'\n    )\n    print(wgetCommand)\n    #    urllib.urlretrieve(baseURL+fileName,fileName)\n    os.system(wgetCommand)\n\n\ndef safe_remove(fileName):\n    \"\"\"\n    Delete a file if it exists, handling errors gracefully.\n\n    This function checks whether the specified path refers to an existing file.\n    If so, it attempts to delete it. On Windows, it waits one second after\n    deletion to ensure the file handle is released. Any error during removal\n    (permission denied, file in use, etc.) is caught, and the function returns\n    False. Non-file paths also return False without raising.\n\n    Parameters\n    ----------\n    fileName : str\n        The path to the file to be removed. Can be absolute or relative.\n\n    Returns\n    -------\n    bool\n        True if the file existed and was removed successfully; False otherwise.\n        - Returns False if the path does not point to a regular file.\n        - Returns False if an exception occurs during deletion.\n\n    Raises\n    ------\n    None\n        All exceptions during file removal are caught; the function never raises.\n    \"\"\"\n    try:\n        if os.path.isfile(fileName):\n            os.remove(fileName)\n            if os.name == \"nt\":\n                time.sleep(1)  # seconds\n            return True\n        else:\n            return False\n    except:\n        return False\n\n\ndef send_to_servers(ncFile, dataDir, interval):\n    \"\"\"\n    Transfer a NetCDF file to multiple remote servers via rsync and optionally copy locally.\n\n    This function constructs an rsync command to push the specified file (`ncFile`) from\n    the current working directory to two remote hosts under `/u00/satellite&lt;remote_path&gt;`.\n    The `&lt;remote_path&gt;` depends on `dataDir` and `interval`. If `interval` is `\"0\"`, the\n    remote path is `&lt;dataDir&gt;/&lt;ncFile&gt;`; otherwise it is `&lt;dataDir&gt;&lt;interval&gt;day/&lt;ncFile&gt;`.\n    After rsyncing to both servers, if `interval` equals `\"1\"` and `dataDir` starts with\n    either `\"MB\"` or `\"MW\"`, the function also copies the file locally into the\n    corresponding `/ERDData1/modisa/data/modisgf/1day/` or\n    `/ERDData1/modisa/data/modiswc/1day/` directory.\n\n    Parameters\n    ----------\n    ncFile : str\n        The filename (with extension) of the NetCDF file to be transferred (e.g., `\"A2023123006000.L2.nc\"`).\n    dataDir : str\n        The base directory path on the remote server where the file should be placed.\n        This string is used both to build the remote path and to determine local copy logic\n        when `interval` is `\"1\"`. For example, `\"/MB2023123\"` or `\"/MW2023123\"`.\n    interval : str\n        The day interval specifier, typically `\"0\"`, `\"1\"`, `\"3\"`, etc. If `\"0\"`, no\n        subdirectory is appended; otherwise, `&lt;interval&gt;day` is appended to `dataDir`\n        (e.g., `\"3\"` → `\"3day\"`).\n\n    Returns\n    -------\n    None\n        This function does not return a value. On success, the file is transferred to\n        both remote servers and possibly copied locally under `/ERDData1/modisa/...`.\n\n    Raises\n    ------\n    RuntimeError\n        If any rsync command returns a non-zero exit code, indicating a failure in transfer.\n    OSError\n        If the local file cannot be read, written, or copied (e.g., permission denied,\n        path does not exist, or `shutil.copyfile` fails).\n    \"\"\"\n\n    intervalDay = interval + \"day\"\n    if interval == \"0\":\n        remote_file = dataDir + \"/\" + ncFile\n    else:\n        remote_file = dataDir + intervalDay + \"/\" + ncFile\n    myCmd = (\n        \"rsync -tvh \"\n        + ncFile\n        + \" cwatch@192.168.31.15:/u00/satellite\"\n        + remote_file\n    )\n    print(myCmd)\n    os.system(myCmd)\n    # myCmd = 'rsync -tvh ' + ncFile + ' cwatch@192.168.31.27:/u00/satellite' + remote_file\n    # print(myCmd)\n    # os.system(myCmd)\n    myCmd = (\n        \"rsync -tvh \"\n        + ncFile\n        + \" cwatch@161.55.17.28:/u00/satellite\"\n        + remote_file\n    )\n    print(myCmd)\n    os.system(myCmd)\n    if (intervalDay == \"1day\") and (dataDir[1:3] == \"MB\"):\n        # mvFile = '/ERDData1/modisa/data/modisgf/1day/' + ncFile\n        mvFile = \"/ERDData1/modisa/data/modisgf/1day/\" + ncFile\n        shutil.copyfile(ncFile, mvFile)\n    if (intervalDay == \"1day\") and (dataDir[1:3] == \"MW\"):\n        # mvFile = '/ERDData1/modisa/data/modiswc/1day/' + ncFile\n        mvFile = \"/ERDData1/modisa/data/modiswc/1day/\" + ncFile\n        shutil.copyfile(ncFile, mvFile)\n\n\ndef send_ncml_to_servers(ncmlFile, ncmlDir, dataDir):\n    \"\"\"\n    Transfer a NetCDF Markup Language (NCML) file to three specified remote servers via rsync.\n\n    This function constructs and executes three rsync commands to copy the NCML file from\n    the local directory (`ncmlDir`) to each remote host under `/u00/satellite/&lt;dataDir&gt;`.\n    The target hosts are 192.168.31.15, 192.168.31.27, and 161.55.17.28.\n\n    Parameters\n    ----------\n    ncmlFile : str\n        The filename of the NCML file to transfer (e.g., \"example.ncml\"). This file is assumed\n        to reside in `ncmlDir`.\n    ncmlDir : str\n        Local directory path where `ncmlFile` is located.\n    dataDir : str\n        The target directory path (relative to `/u00/satellite/`) on each remote server.\n        The full remote path becomes `/u00/satellite/&lt;dataDir&gt;&lt;ncmlFile&gt;`.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    RuntimeError\n        If any rsync command returns a non-zero exit code, indicating a transfer failure.\n    OSError\n        If the local NCML file cannot be found or accessed, or if rsync is not available.\n    \"\"\"\n\n    send_file = ncmlDir + \"/\" + ncmlFile\n    myCmd = (\n        \"rsync -tvh \"\n        + send_file\n        + \" cwatch@192.168.31.15:/u00/satellite\"\n        + dataDir\n        + ncmlFile\n    )\n    os.system(myCmd)\n    myCmd = (\n        \"rsync -tvh \"\n        + send_file\n        + \" cwatch@192.168.31.27:/u00/satellite\"\n        + dataDir\n        + ncmlFile\n    )\n    os.system(myCmd)\n    myCmd = (\n        \"rsync -tvh \"\n        + send_file\n        + \" cwatch@161.55.17.28:/u00/satellite\"\n        + dataDir\n        + ncmlFile\n    )\n    os.system(myCmd)\n\n\ndef retrieve_new_files(dataDir, param, myYear, doy, param_update_flag, lag):\n    \"\"\"\n    Check for and download new MODIS files for a given parameter and date.\n\n    This function constructs a search query for the NASA OceanColor file_search API\n    based on the provided parameter (`\"OC\"` or other). It computes the calendar date\n    from `myYear` and `doy`, then builds a query string of the form:\n    `search=AQUA_MODIS.&lt;YYYY&gt;&lt;MM&gt;&lt;DD&gt;*L2.&lt;param&gt;.NRT.nc&dtype=L2&sensor=aqua&results_as_file=1`.\n    It calls `url_lines1` to fetch the list of matching filenames. For each filename,\n    it checks whether the file already exists in `dataDir`; if not, it sets the\n    corresponding index in `param_update_flag` to True and invokes `get_netcdfFile`\n    to download the missing file, pausing 20 seconds between downloads.\n\n    Parameters\n    ----------\n    dataDir : str\n        The directory where downloaded files should be saved (e.g., `/ERDData1/modisa/data/netcdf/`).\n    param : str\n        The MODIS data parameter to retrieve, typically `\"OC\"` (ocean color) or another dataset code.\n    myYear : str\n        The four-digit year (e.g., `\"2023\"`).\n    doy : str\n        The day of year as a zero-padded string (e.g., `\"005\"` for January 5).\n    param_update_flag : list of bool\n        A list of length at least 4, where `param_update_flag[lag + 3]` is set to True\n        if a new file for that lag index needs to be downloaded.\n    lag : int\n        The offset relative to the current date, ranging from -3 to 0. This determines\n        which index in `param_update_flag` to update (`lag + 3`).\n\n    Returns\n    -------\n    None\n        The function does not return a value. It updates `param_update_flag` in place\n        and downloads any missing NetCDF files to `dataDir`.\n\n    Raises\n    ------\n    urllib.error.URLError\n        If the HTTP request inside `url_lines1` fails (e.g., network error).\n    OSError\n        If file operations (checking existence or writing) fail, or if `get_netcdfFile`\n        cannot write the downloaded file.\n    \"\"\"\n\n    if param == \"OC\":\n        # modis_search_URL = 'search=A' + myYear + doy + '*L2_LAC_' + param + '.nc&dtype=L2&sensor=aqua&results_as_file=1'\n        myDate = datetime(int(myYear), 1, 1) + timedelta(int(doy) - 1)\n        myMonth = str(myDate.month).rjust(2, \"0\")\n        myDay = str(myDate.day).rjust(2, \"0\")\n        modis_search_URL = (\n            \"search=AQUA_MODIS.\"\n            + myYear\n            + myMonth\n            + myDay\n            + \"*L2.\"\n            + param\n            + \".NRT.nc&dtype=L2&sensor=aqua&results_as_file=1\"\n        )\n        print(doy)\n        print(lag)\n        print(modis_search_URL)\n        # fileList = url_lines(modis_search_URL)\n        fileList = url_lines1(modis_search_URL)\n        for fName in fileList:\n            fileTest = os.path.isfile(dataDir + \"/\" + fName)\n            if not (fileTest):\n                param_update_flag[lag + 3] = True\n                print(fName)\n                get_netcdfFile(fName)\n                time.sleep(20)\n    else:\n        myDate = datetime(int(myYear), 1, 1) + timedelta(int(doy) - 1)\n        myMonth = str(myDate.month).rjust(2, \"0\")\n        myDay = str(myDate.day).rjust(2, \"0\")\n        modis_search_URL = (\n            \"search=AQUA_MODIS.\"\n            + myYear\n            + myMonth\n            + myDay\n            + \"*L2.\"\n            + param\n            + \".NRT.nc&dtype=L2&sensor=aqua&results_as_file=1\"\n        )\n        print(doy)\n        print(lag)\n        print(modis_search_URL)\n        # fileList = url_lines(modis_search_URL)\n        fileList = url_lines1(modis_search_URL)\n        for fName in fileList:\n            fileTest = os.path.isfile(dataDir + \"/\" + fName)\n            if not (fileTest):\n                param_update_flag[lag + 3] = True\n                print(fName)\n                get_netcdfFile(fName)\n                time.sleep(20)\n\n\ndef retrieve_new_files1(dataDir, param, myYear, doy, param_update_flag, lag):\n    \"\"\"\n    Check for and download new MODIS files, handling OC and non-OC parameters differently.\n\n    This function constructs a search query for the NASA OceanColor file_search API based on\n    the given parameter. For `param == \"OC\"`, it uses a legacy L2_LAC endpoint; otherwise it uses\n    the NRT endpoint. It computes the calendar date from `myYear` and `doy`, builds the query\n    string, and retrieves the list of matching filenames via `url_lines1`. The filenames are sorted,\n    and for each filename not already present in `dataDir`, it sets the corresponding index in\n    `param_update_flag` to True and calls `get_netcdfFile` to download the file, pausing 20 seconds\n    between downloads.\n\n    Parameters\n    ----------\n    dataDir : str\n        The directory where downloaded files should be saved (e.g., `/ERDData1/modisa/data/netcdf/`).\n    param : str\n        The MODIS data parameter to retrieve. If `\"OC\"`, the search endpoint uses L2_LAC_OC; otherwise\n        it uses the NRT endpoint (e.g., `\"SST\"` or other parameter codes).\n    myYear : str\n        The four-digit year (e.g., `\"2023\"`). Used to construct the search URL.\n    doy : str\n        The day of year as a zero-padded string (e.g., `\"005\"` for January 5). Used to construct the search URL.\n    param_update_flag : list of bool\n        A list of length at least 4, where `param_update_flag[lag + 3]` is set to True if a new file\n        needs to be downloaded for the given lag index.\n    lag : int\n        The offset relative to the current date, ranging from -3 to 0. Determines which index in\n        `param_update_flag` to update (`lag + 3`).\n\n    Returns\n    -------\n    None\n        Updates `param_update_flag` in place and downloads any missing NetCDF files to `dataDir`.\n\n    Raises\n    ------\n    urllib.error.URLError\n        If the HTTP request inside `url_lines1` encounters a network error on all retry attempts.\n    OSError\n        If file operations fail (e.g., checking existence, writing to `dataDir`), or if `get_netcdfFile`\n        cannot write the downloaded file.\n    \"\"\"\n    # Rest of the code...\n\n    if param == \"OC\":\n        # modis_search_URL = 'search=A' + myYear + doy + '*L2_LAC_' + param + '.nc&dtype=L2&sensor=aqua&results_as_file=1'\n        myDate = datetime(int(myYear), 1, 1) + timedelta(int(doy) - 1)\n        myMonth = str(myDate.month).rjust(2, \"0\")\n        myDay = str(myDate.day).rjust(2, \"0\")\n        modis_search_URL = (\n            \"search=A\"\n            + myYear\n            + doy\n            + \"*L2_LAC_\"\n            + param\n            + \".nc&dtype=L2&sensor=aqua&results_as_file=1\"\n        )\n        print(doy)\n        print(lag)\n        print(modis_search_URL)\n        print(doy)\n        print(lag)\n        print(modis_search_URL)\n        # fileList = url_lines(modis_search_URL)\n        fileList = url_lines1(modis_search_URL)\n        fileList.sort()\n        for fName in fileList:\n            fileTest = os.path.isfile(dataDir + \"/\" + fName)\n            if not (fileTest):\n                param_update_flag[lag + 3] = True\n                print(fName)\n                get_netcdfFile(fName)\n                time.sleep(20)\n    else:\n        myDate = datetime(int(myYear), 1, 1) + timedelta(int(doy) - 1)\n        myMonth = str(myDate.month).rjust(2, \"0\")\n        myDay = str(myDate.day).rjust(2, \"0\")\n        modis_search_URL = (\n            \"search=AQUA_MODIS.\"\n            + myYear\n            + myMonth\n            + myDay\n            + \"*L2.\"\n            + param\n            + \".NRT.nc&dtype=L2&sensor=aqua&results_as_file=1\"\n        )\n        print(doy)\n        print(lag)\n        print(modis_search_URL)\n        # fileList = url_lines(modis_search_URL)\n        fileList = url_lines1(modis_search_URL)\n        fileList.sort()\n        for fName in fileList:\n            fileTest = os.path.isfile(dataDir + \"/\" + fName)\n            if not (fileTest):\n                param_update_flag[lag + 3] = True\n                print(fName)\n                get_netcdfFile(fName)\n                time.sleep(20)\n\n\ndef isleap(year):\n    \"\"\"\n    Determine whether the specified year is a leap year.\n\n    This function attempts to construct a date object for February 29 of the given year.\n    If the date is valid, the year is a leap year; otherwise, a ValueError is raised\n    and the function returns False.\n\n    Parameters\n    ----------\n    year : int\n        The year to check (e.g., 2024).\n\n    Returns\n    -------\n    bool\n        True if `year` is a leap year (i.e., February 29 exists in that year);\n        False otherwise.\n\n    Raises\n    ------\n    None\n        Any ValueError raised by `datetime.date(year, 2, 29)` is caught internally;\n        the function never propagates exceptions.\n    \"\"\"\n\n    from datetime import date, datetime, timedelta\n\n    try:\n        date(year, 2, 29)\n        return True\n    except ValueError:\n        return False\n\n\ndef meanVar(mean, num, obs):\n    \"\"\"\n    Update the running mean and count of observations with new data.\n\n    Parameters\n    ----------\n    mean : numpy.ma.MaskedArray or numpy.ndarray\n        The current running mean values for each element.\n    num : numpy.ndarray\n        The current count of valid observations for each element (integer array).\n    obs : numpy.ma.MaskedArray\n        The new observations with the same shape as `mean`. Masked entries are not used.\n\n    Returns\n    -------\n    tuple\n        A 2-tuple `(updated_mean, updated_count)` where:\n        - `updated_mean` is a masked or regular array of new mean values (dtype float32).\n        - `updated_count` is an integer array of updated counts (dtype int32).\n\n    Raises\n    ------\n    None\n        Shape mismatches or invalid operations will propagate NumPy errors.\n    \"\"\"\n\n    import numpy as np\n    import numpy.ma as ma\n\n    numShape = num.shape\n    temp = np.subtract(obs, mean, dtype=np.single)\n    numAdd = np.ones(numShape, dtype=np.int32)\n    numAdd[obs.mask] = 0\n    num = np.add(num, numAdd, dtype=np.int32)\n    tempNum = ma.array(num, mask=(num == 0), dtype=np.int32)\n    temp = np.divide(temp, tempNum.astype(\"float\"), dtype=np.single)\n    mean = np.add(mean, temp.filled(0.0), dtype=np.single)\n    return (mean, num)\n\n\ndef mean_sumsq(mean, ss, num, obs):\n    \"\"\"\n    Cumulative calculation of mean and sum of squares for masked arrays.\n\n    This function updates the running mean, sum of squares, and count of observations\n    given a new set of observations. All input arrays (`mean`, `ss`, and `obs`) must be\n    NumPy masked arrays of identical shape. If any input is not a masked array, the\n    function exits with an error message.\n\n    Parameters\n    ----------\n    mean : numpy.ma.MaskedArray\n        The current running mean array. Masked entries indicate missing data and\n        are not included in the update.\n    ss : numpy.ma.MaskedArray\n        The current running sum of squares array. Masked entries are not included\n        in the update.\n    num : numpy.ndarray\n        The current count of valid observations for each element, as an integer array\n        of the same shape as `mean`. This array is incremented for each unmasked entry\n        in `obs`.\n    obs : numpy.ma.MaskedArray\n        The new observations to incorporate. Masked entries indicate missing data and\n        are not used in updating `mean`, `ss`, or `num`.\n\n    Returns\n    -------\n    tuple\n        A 3-tuple `(updated_mean, updated_ss, updated_num)` where:\n        - `updated_mean` (numpy.ma.MaskedArray): The updated running mean array.\n        - `updated_ss` (numpy.ma.MaskedArray): The updated running sum of squares array.\n        - `updated_num` (numpy.ndarray): The updated count of observations as an integer array.\n\n    Raises\n    ------\n    SystemExit\n        If any of `mean`, `ss`, or `obs` is not a `numpy.ma.MaskedArray`, the function\n        prints an error message and exits.\n    \"\"\"\n    import numpy as np\n    import numpy.ma as ma\n    import sys\n\n    if (\n        not isinstance(mean, np.ma.MaskedArray)\n        or not isinstance(ss, np.ma.MaskedArray)\n        or not isinstance(obs, np.ma.MaskedArray)\n    ):\n\n        print(\"Input arguments mean, ss, and obs are not numpy masked arrays\")\n        print(\"Try converting mean, ss, and obs to masked arrays\")\n        print(\"before using them in the function, e.g. ma.array(arg)\")\n        print(\" \")\n        sys.exit(mean_sumsq.__doc__)\n\n    numShape = num.shape\n    temp = np.subtract(obs, mean.filled(0.0), dtype=np.single)\n    numAdd = np.ones(numShape, dtype=np.int32)\n    numAdd[obs.mask] = 0\n    num = np.add(num, numAdd, dtype=np.int32)\n    tempNum = ma.array(num, mask=(num == 0), dtype=np.int32)\n    print(\"tempNum\", tempNum.min(), tempNum.max())\n    tNfloat = tempNum.astype(\"float\")\n\n    temp = ma.divide(temp, tNfloat, dtype=np.single)\n    mean = np.add(mean.filled(0.0), temp.filled(0.0), dtype=np.single)\n    # mean = ma.masked_where(mean == 0., mean)\n\n    num1 = np.copy(num)\n    num2 = num1 - 1\n    print(\"num2\", num2.min(), num2.max())\n    if np.any(num1 &gt; 1):\n        num1 = np.divide(num1, num2, where=(num1 &gt; 1))\n\n    print(\"num1\", num1.min(), num1.max())\n    print(\"num\", num.min(), num.max())\n    temp1 = ma.subtract(obs, mean, dtype=np.single)\n    temp2 = np.multiply(temp1, temp1)\n    print(\n        \"ss parts\",\n        ss.filled(0.0).max(),\n        ma.multiply(num1, temp2).filled(0.0).max(),\n    )\n    ss = ss.filled(0.0) + ma.multiply(num1, temp2).filled(0.0)\n\n    mean = ma.masked_where(num == 0, mean)\n    ss = ma.masked_where(num == 0, ss)\n    vr = np.divide(ss, num)\n    sdev = np.sqrt(vr)\n    print(\"stDev\", sdev.min(), sdev.max(), sdev.mean())\n    # print(stdev.min(), stdev.max(), stdev.mean())\n    return (mean, ss, num)\n\n\ndef makeNetcdf(mean, nobs, interval, outFile, filesUsed, workDir):\n    \"\"\"\n    Create a NetCDF file from aggregated data arrays and assign metadata.\n\n    Parameters\n    ----------\n    mean : numpy.ma.MaskedArray or numpy.ndarray\n        A 2D array of mean values (masked or regular). Masked entries are treated as missing.\n    nobs : int\n        The number of observations used to compute `mean`. This value is recalculated from `mean`.\n    interval : int\n        Time interval in days (e.g., 1, 3, 5, 8, 14). Determines which CDL template to use.\n    outFile : str\n        Desired output filename (e.g., '/path/to/output/MB2023123001.nc'). The function infers:\n        - `dataset`: first two characters.\n        - `param`: substring after the first underscore past index 10.\n        - `time1` and `time2`: substrings indicating start and end DOY.\n    filesUsed : list of str\n        List of source filenames that contributed to `mean`. Stored in the NetCDF’s `files` attribute.\n    workDir : str\n        Directory in which to execute `ncgen` and write the new NetCDF file.\n\n    Returns\n    -------\n    str\n        The path of the generated NetCDF file (with “.nc” extension).\n\n    Raises\n    ------\n    OSError\n        If changing directory to `workDir` fails or if `ncgen` cannot be executed.\n    RuntimeError\n        If the `ncgen` command returns a non-zero exit code.\n    \"\"\"\n\n    print(interval)\n    os.chdir(workDir)\n    now = datetime.now()\n    now1 = date(now.year, now.month, now.day)\n    nobs = ma.count(mean)\n    noMiss = ma.count_masked(mean)\n    percentCoverage = old_div(float(nobs), float(nobs + noMiss))\n    # get netcdf file name and correct cdl file\n    ncFile = outFile[:-3]\n    dataset = outFile[0:2]\n    offset = ncFile.find(\"_\", 10)\n    param = ncFile[(offset + 1) : len(ncFile)]\n    time1 = outFile[2:9]\n    time2 = outFile[10:17]\n    ncFile = ncFile + \".nc\"\n    interval1 = str(interval)\n    print(ncFile)\n    print(dataset)\n    print(param)\n    print(interval1)\n    # cdlFile = '/ERDData1/modisa/python/' + dataset + param + interval1 + 'Day.cdl'\n    cdlFile = (\n        \"/ERDData1/modisa/python/\" + dataset + param + interval1 + \"Day.cdl\"\n    )\n    print(cdlFile)\n    # os.system('/usr/bin/ncgen -o ' + ncFile + ' ' + cdlFile)\n    os.system(\"/usr/bin/ncgen -o \" + ncFile + \" \" + cdlFile)\n    # shutil.copyfile(cdlFile, ncFile)\n    ncPointer = Dataset(ncFile, \"a\")\n    mytime = ncPointer.variables[\"time\"]\n    ncPointer.files = filesUsed\n    ncPointer.date_created = str(now1)\n    ncPointer.date_issued = str(now1)\n    paramName = dataset + param\n    myparam = ncPointer.variables[paramName]\n    tempName = myparam.long_name\n    composite = \"(\" + interval1 + \" Day Composite)\"\n    tempName = tempName.replace(\"(3 Day Composite)\", composite)\n    myparam.long_name = tempName\n    myparam.numberOfObservations = nobs\n    myparam.percentCoverage = percentCoverage\n    myparam[0, 0, :, :] = mean[:, :]\n    myparam.actual_range = np.array(([mean.min(), mean.max()]))\n    startTimeYear = int(time1[0:4])\n    startTimeDoy = int(time1[4:7])\n    startDate = datetime(startTimeYear, 1, 1, 0) + timedelta(startTimeDoy - 1)\n    if interval1 == \"1\":\n        centerDate = startDate + timedelta(hours=12)\n    elif interval1 == \"3\":\n        centerDate = startDate + timedelta(hours=36)\n    elif interval1 == \"5\":\n        centerDate = startDate + timedelta(hours=60)\n    elif interval1 == \"8\":\n        centerDate = startDate + timedelta(days=4)\n    elif interval1 == \"14\":\n        centerDate = startDate + timedelta(days=7)\n\n    udtime = date2num(centerDate, units=\"seconds since 1970-01-01\")\n    mytime[0] = udtime\n    mytime.actual_range = np.array(([udtime, udtime]))\n    ncPointer.close()\n    return ncFile\n\n\ndef makeNetcdfmDay(mean, nobs, interval, outFile, filesUsed, workDir):\n    \"\"\"\n    Create a NetCDF file for multi-day composite data.\n\n    Parameters\n    ----------\n    mean : numpy.ma.MaskedArray or numpy.ndarray\n        A 2D array of mean values for the composite period. Masked entries are treated as missing.\n    nobs : int\n        The number of observations used to compute `mean`. Recomputed internally from `mean`.\n    interval : float\n        The length of the composite period (in days). Used to select the “mDay” CDL template.\n    outFile : str\n        Desired output filename (ending in “.nc”). The function derives:\n        - `dataset`: first two characters of `outFile`.\n        - `param`: substring after the first underscore past index 10.\n        - `time1` and `time2`: substrings for start/end DOY.\n    filesUsed : list of str\n        A list of source filenames that contributed to `mean`. Stored in NetCDF’s `files` attribute.\n    workDir : str\n        Directory in which to execute `ncgen` and write the new NetCDF file.\n\n    Returns\n    -------\n    str\n        The path of the generated NetCDF file (with “.nc” extension).\n\n    Raises\n    ------\n    OSError\n        If changing directory to `workDir` fails or if `ncgen` cannot be executed.\n    RuntimeError\n        If the `ncgen` command returns a non-zero exit code.\n    \"\"\"\n\n    os.chdir(workDir)\n    now = datetime.now()\n    now1 = date(now.year, now.month, now.day)\n    nobs = ma.count(mean)\n    noMiss = ma.count_masked(mean)\n    percentCoverage = old_div(float(nobs), float(nobs + noMiss))\n    # get netcdf file name and correct cdl file\n    ncFile = outFile[:-3]\n    dataset = outFile[0:2]\n    offset = ncFile.find(\"_\", 10)\n    param = ncFile[(offset + 1) : len(ncFile)]\n    time1 = outFile[2:9]\n    time2 = outFile[10:17]\n    ncFile = ncFile + \".nc\"\n    print(ncFile)\n    # cdlFile = '/ERDData1/modisa/python/' + dataset + param + 'mDay.cdl'\n    cdlFile = \"/ERDData1/modisa/python/\" + dataset + param + \"mDay.cdl\"\n    print(cdlFile)\n    os.system(\"/usr/bin/ncgen -o \" + ncFile + \" \" + cdlFile)\n    # shutil.copyfile(cdlFile, ncFile)\n    ncPointer = Dataset(ncFile, \"a\")\n    mytime = ncPointer.variables[\"time\"]\n    ncPointer.files = filesUsed\n    ncPointer.date_created = str(now1)\n    ncPointer.date_issued = str(now1)\n    paramName = dataset + param\n    myparam = ncPointer.variables[paramName]\n    myparam.numberOfObservations = nobs\n    myparam.percentCoverage = percentCoverage\n    myparam[0, 0, :, :] = mean[:, :]\n    myparam.actual_range = np.array(([mean.min(), mean.max()]))\n    startTimeYear = int(time1[0:4])\n    startTimeDoy = int(time1[4:7])\n    startDate = datetime(startTimeYear, 1, 1, 0) + timedelta(startTimeDoy - 1)\n    endTimeYear = int(time2[0:4])\n    endTimeDoy = int(time2[4:7])\n    endDate = datetime(endTimeYear, 1, 1, 0) + timedelta(endTimeDoy - 1)\n    centerDoy = old_div((startTimeDoy + endTimeDoy), 2.0)\n    centerDate = datetime(startTimeYear, 1, 1, 0) + timedelta(centerDoy - 1)\n    print(centerDate)\n    udtime = date2num(centerDate, units=\"seconds since 1970-01-01\")\n    if (endTimeDoy - startTimeDoy + 1) == 31:\n        udtime = udtime + 43200\n    print(udtime)\n    mytime[0] = udtime\n    mytime.actual_range = np.array(([udtime, udtime]))\n    ncPointer.close()\n    return ncFile\n\n\ndef grd2netcdf(grdFile, filesUsed, fType):\n    \"\"\"\n    Convert a GRD file to a NetCDF file, copying spatial data and metadata.\n\n    This function reads a GRD file containing gridded data (with variables named\n    either \"lon\"/\"lat\"/\"z\" or \"x\"/\"y\"/\"z\" depending on `fType`), computes coverage statistics,\n    generates a NetCDF file via an `ncgen` command on a corresponding CDL template, and\n    writes spatial coordinates, data values, and metadata (file list, creation date,\n    observation count, coverage, and actual range) into the new NetCDF. The time variable\n    is centered based on the date stamps in the original filename.\n\n    Parameters\n    ----------\n    grdFile : str\n        Path to the input GRD file (e.g., \"/path/to/MB2023123001000.grd\").\n        The filename must encode the dataset, parameter, and date information:\n        - The first two characters are the dataset code.\n        - Characters 2-9 represent the start date (YYYYDDD).\n        - Characters 10-17 represent the end date (YYYYDDD).\n    filesUsed : list of str\n        A list of source filenames that contributed to the GRD data. This list is stored\n        in the NetCDF file's `files` attribute.\n    fType : str\n        File type indicator:\n        - `\"MW\"` uses variables `\"lon\"`, `\"lat\"`, and `\"z\"`.\n        - Any other value uses variables `\"x\"`, `\"y\"`, and `\"z\"`.\n\n    Returns\n    -------\n    str\n        The path of the generated NetCDF file (same as `grdFile` with a “.nc” extension).\n\n    Raises\n    ------\n    OSError\n        If reading the GRD file or executing the `ncgen` command fails (e.g., file not found,\n        permission denied).\n    RuntimeError\n        If the `ncgen` command returns a non-zero exit code, indicating failure to generate\n        the NetCDF file.\n    \"\"\"\n\n    now = datetime.now()\n    now1 = date(now.year, now.month, now.day)\n    grdPointer = Dataset(grdFile)\n    if fType == \"MW\":\n        x = grdPointer.variables[\"lon\"][:]\n        y = grdPointer.variables[\"lat\"][:]\n        z = grdPointer.variables[\"z\"][:, :]\n    else:\n        x = grdPointer.variables[\"x\"][:]\n        y = grdPointer.variables[\"y\"][:]\n        z = grdPointer.variables[\"z\"][:, :]\n    grdPointer.close()\n    nobs = ma.count(z)\n    noMiss = ma.count_masked(z)\n    percentCoverage = old_div(float(nobs), float(nobs + noMiss))\n    # get netcdf file name and correct cdl file\n    ncFile = grdFile[:-4]\n    dataset = grdFile[0:2]\n    offset = ncFile.find(\"_\", 10)\n    param = ncFile[(offset + 1) : len(ncFile)]\n    time1 = grdFile[2:9]\n    time2 = grdFile[10:17]\n    interval = str(int(time2) - int(time1) + 1)\n    ncFile = ncFile + \".nc\"\n    print(ncFile)\n    # cdlFile = '/ERDData1/modisa/python/' + dataset + param + interval + 'Day.cdl'\n    cdlFile = (\n        \"/ERDData1/modisa/python/\" + dataset + param + interval + \"Day.cdl\"\n    )\n    print(cdlFile)\n    os.system(\"/usr/bin/ncgen -o \" + ncFile + \" \" + cdlFile)\n    # shutil.copyfile(cdlFile, ncFile)\n    ncPointer = Dataset(ncFile, \"a\")\n    lat = ncPointer.variables[\"lat\"]\n    lon = ncPointer.variables[\"lon\"]\n    mytime = ncPointer.variables[\"time\"]\n    ncPointer.files = filesUsed\n    ncPointer.date_created = str(now1)\n    ncPointer.date_issued = str(now1)\n    paramName = dataset + param\n    myparam = ncPointer.variables[paramName]\n    lat[:] = y[:]\n    lon[:] = x[:]\n    myparam[0, 0, :, :] = z[:, :]\n    myparam.numberOfObservations = nobs\n    myparam.percentCoverage = percentCoverage\n    myparam.actual_range = np.array(([z.min(), z.max()]))\n    startTimeYear = int(time1[0:4])\n    startTimeDoy = int(time1[4:7])\n    startDate = datetime(startTimeYear, 1, 1, 0) + timedelta(startTimeDoy - 1)\n    if interval == \"1\":\n        centerDate = startDate + timedelta(hours=12)\n    elif interval == \"3\":\n        centerDate = startDate + timedelta(hours=36)\n    elif interval == \"5\":\n        centerDate = startDate + timedelta(hours=60)\n    elif interval == \"8\":\n        centerDate = startDate + timedelta(days=4)\n    elif interval == \"14\":\n        centerDate = startDate + timedelta(days=7)\n\n    udtime = date2num(centerDate, units=\"seconds since 1970-01-01\")\n    mytime[0] = udtime\n    mytime.actual_range = np.array(([udtime, udtime]))\n    ncPointer.close()\n    return ncFile\n\n\ndef grd2netcdf1(grdFile, fileOut, filesUsed, my_mask, fType):\n    \"\"\"\n    Convert an Xarray grid file to a NetCDF file, applying a land mask and writing metadata.\n\n    Parameters\n    ----------\n    grdFile : xarray.Dataset\n        The input grid dataset. If `fType == \"MW\"`, it must have `.lon`, `.lat`, and `.values`.\n        Otherwise, it must have `.x`, `.y`, and `.values`.\n    fileOut : str\n        The target output filename (e.g., '/path/to/output/MB2023123001.nc'). The function\n        derives dataset, parameter, start/end DOY from this name.\n    filesUsed : list of str\n        A list of source filenames that contributed to the grid; stored in the NetCDF's `files`.\n    my_mask : numpy.ndarray\n        A boolean or integer mask array of the same shape as `grdFile.values`. Points where\n        `my_mask != 1` are set to NaN and then masked.\n    fType : str\n        File type indicator:\n        - `\"MW\"` → use `grdFile.lon.values`, `grdFile.lat.values`, and `grdFile.values`.\n        - Any other string → use `grdFile.x.values`, `grdFile.y.values`, and `grdFile.values`.\n\n    Returns\n    -------\n    str\n        The path of the generated NetCDF file (same as `fileOut` with “.nc” extension).\n\n    Raises\n    ------\n    OSError\n        If the GRD file cannot be read or if the `ncgen` command fails to run.\n    RuntimeError\n        If `ncgen` returns a non-zero exit code.\n    \"\"\"\n\n    from datetime import datetime, timedelta\n    from netCDF4 import Dataset\n    import xarray as xr\n\n    now = datetime.now()\n    now1 = date(now.year, now.month, now.day)\n    if fType == \"MW\":\n        x = grdFile.lon.values\n        y = grdFile.lat.values\n        z = grdFile.values\n    else:\n        x = grdFile.x.values\n        y = grdFile.y.values\n        z = grdFile.values\n\n    # set areas in land mask not land to NaN\n    z[my_mask != 1] = np.NAN\n    # convert z to ma.array with NaNs masked\n    z = ma.array(z, mask=np.isnan(z), fill_value=-9999999.0)\n    nobs = ma.count(z)\n    noMiss = ma.count_masked(z)\n    percentCoverage = old_div(float(nobs), float(nobs + noMiss))\n    # get netcdf file name and correct cdl file\n    ncFile = fileOut[:-4]\n    dataset = fileOut[0:2]\n    offset = ncFile.find(\"_\", 10)\n    param = ncFile[(offset + 1) : len(ncFile)]\n    time1 = fileOut[2:9]\n    time2 = fileOut[10:17]\n    interval = str(int(time2) - int(time1) + 1)\n    ncFile = ncFile + \".nc\"\n    print(ncFile)\n    # cdlFile = '/ERDData1/modisa/python/' + dataset + param + interval + 'Day.cdl'\n    cdlFile = (\n        \"/ERDData1/modisa/python/\" + dataset + param + interval + \"Day.cdl\"\n    )\n    print(cdlFile)\n    os.system(\"/usr/bin/ncgen -o \" + ncFile + \" \" + cdlFile)\n    # shutil.copyfile(cdlFile, ncFile)\n    ncPointer = Dataset(ncFile, \"a\")\n    lat = ncPointer.variables[\"lat\"]\n    lon = ncPointer.variables[\"lon\"]\n    mytime = ncPointer.variables[\"time\"]\n    ncPointer.files = filesUsed\n    ncPointer.date_created = str(now1)\n    ncPointer.date_issued = str(now1)\n    paramName = dataset + param\n    myparam = ncPointer.variables[paramName]\n    lat[:] = y[:]\n    lon[:] = x[:]\n    myparam[0, 0, :, :] = z[:, :]\n    myparam.numberOfObservations = nobs\n    myparam.percentCoverage = percentCoverage\n    myparam.actual_range = np.array(([z.min(), z.max()]))\n    startTimeYear = int(time1[0:4])\n    startTimeDoy = int(time1[4:7])\n    startDate = datetime(startTimeYear, 1, 1, 0) + timedelta(startTimeDoy - 1)\n    if interval == \"1\":\n        centerDate = startDate + timedelta(hours=12)\n    elif interval == \"3\":\n        centerDate = startDate + timedelta(hours=36)\n    elif interval == \"5\":\n        centerDate = startDate + timedelta(hours=60)\n    elif interval == \"8\":\n        centerDate = startDate + timedelta(days=4)\n    elif interval == \"14\":\n        centerDate = startDate + timedelta(days=7)\n\n    udtime = date2num(centerDate, units=\"seconds since 1970-01-01\")\n    mytime[0] = udtime\n    mytime.actual_range = np.array(([udtime, udtime]))\n    ncPointer.close()\n    return ncFile\n\n\ndef update_modis_1day(now, basedataDir, param, param_update_flag):\n    \"\"\"\n    Execute daily MODIS processing scripts for SST or Chla based on update flags.\n\n    This function iterates over the three most recent lags (-3, -2, -1 days relative to `now`).\n    For each lag where `param_update_flag[lag + 3]` is True, it computes the corresponding date,\n    constructs the directory path under `basedataDir`, and invokes the appropriate daily processing\n    scripts via `os.system`. For `\"SST\"`, it runs both `makeSST1daynewMW.py` and `makeSST1daynewMB.py`;\n    for `\"Chla\"`, it runs both `makeChla1daynewMW.py` and `makeChla1daynewMB.py`. Each script is\n    passed the year and day-of-year strings as arguments.\n\n    Parameters\n    ----------\n    now : datetime.datetime\n        The reference datetime (typically the current date and time). Used to compute the target date\n        for each lag.\n    basedataDir : str\n        The base path for data directories (e.g., \"/ERDData1/modisa/data/netcdf/\"). For each lag,\n        the function will append `&lt;YYYY&gt;&lt;MM&gt;` to this path to form `dataDir`.\n    param : str\n        The parameter to process: either `\"SST\"` or `\"Chla\"`. Determines which pair of scripts to invoke.\n    param_update_flag : list of bool\n        A list of length at least 4. For each `lag` in `[-3, -2, -1]`, if `param_update_flag[lag + 3]`\n        is True, the function will trigger processing for that lag index.\n\n    Returns\n    -------\n    None\n        This function does not return a value. It updates no in-memory state beyond side effects\n        of calling external scripts.\n\n    Raises\n    ------\n    OSError\n        If the working directory or the external processing scripts cannot be invoked (e.g., path not found,\n        permission denied). Any non-zero exit code from `os.system` is not captured but may indicate failure.\n    \"\"\"\n\n    for lag in list(range(-3, 0)):\n        if param_update_flag[lag + 3]:\n            myDate1 = now + timedelta(days=lag)\n            myYear = str(myDate1.year)\n            myMon = str(myDate1.month)\n            myMon = myMon.rjust(2, \"0\")\n            doy = myDate1.strftime(\"%j\").zfill(3)\n            dataDir = basedataDir + myYear + myMon\n            if param == \"SST\":\n                # myCmd = '/home/cwatch/anaconda3/bin/python /home/cwatch/newPython/modisa/makeSST1daynewMW.py  /ERDData1/modisa/data/netcdf/ /ERDData1/modisa/work/ ' + myYear + ' ' + doy\n                myCmd = (\n                    \"/home/cwatch/mambaforge/bin/python /home/cwatch/newPython/modisa/makeSST1daynewMW.py  /ERDData1/modisa/data/netcdf/ /ERDData1/modisa/work/ \"\n                    + myYear\n                    + \" \"\n                    + doy\n                )\n                os.system(myCmd)\n                # myCmd = '/home/cwatch/anaconda3/bin/python /home/cwatch/newPython/modisa/makeSST1daynewMB.py  /ERDData1/modisa/data/netcdf/ /ERDData1/modisa/work1/ ' + myYear + ' ' + doy\n                myCmd = (\n                    \"/home/cwatch/mambaforge/bin/python /home/cwatch/newPython/modisa/makeSST1daynewMB.py  /ERDData1/modisa/data/netcdf/ /ERDData1/modisa/work1/ \"\n                    + myYear\n                    + \" \"\n                    + doy\n                )\n                os.system(myCmd)\n            else:\n                # myCmd = '/home/cwatch/anaconda3/bin/python /home/cwatch/newPython/modisa/makeChla1daynewMW.py /ERDData1/modisa/data/netcdf/ /ERDData1/modisa/work/ ' + myYear + ' ' + doy\n                myCmd = (\n                    \"/home/cwatch/mambaforge/bin/python /home/cwatch/newPython/modisa/makeChla1daynewMW.py /ERDData1/modisa/data/netcdf/ /ERDData1/modisa/work/ \"\n                    + myYear\n                    + \" \"\n                    + doy\n                )\n                os.system(myCmd)\n                # myCmd = '/home/cwatch/anaconda3/bin/python /home/cwatch/newPython/modisa/makeChla1daynewMB.py  /ERDData1/modisa/data/netcdf/ /ERDData1/modisa/work1/ ' + myYear + ' ' + doy\n                myCmd = (\n                    \"/home/cwatch/mambaforge/bin/python /home/cwatch/newPython/modisa/makeChla1daynewMB.py  /ERDData1/modisa/data/netcdf/ /ERDData1/modisa/work1/ \"\n                    + myYear\n                    + \" \"\n                    + doy\n                )\n                os.system(myCmd)\n\n\ndef update_modis_composite(\n    now, basedataDir, param, param_update_flag, composite\n):\n    \"\"\"\n    Update MODIS composite products for specified date lags and parameter.\n\n    This function iterates over the three days immediately preceding `now` (lags -3, -2, -1).\n    For each lag, it computes the corresponding date, builds the data directory path from\n    `basedataDir`, and checks if any daily updates occurred in that range by summing the\n    boolean flags in `param_update_flag` up to the current lag index. If an update is needed,\n    it invokes the appropriate composite scripts via `os.system`. When `param` is `'SST'`,\n    it runs `CompMBSST.py` and `CompMWSST.py`; when `param` is `'Chla'`, it runs\n    `CompMBChla.py` and `CompMWChla.py`. Each script is passed the year, day-of-year, and\n    the `composite` interval as command-line arguments.\n\n    Parameters\n    ----------\n    now : datetime.datetime\n        The reference datetime used to calculate the target dates for each lag.\n    basedataDir : str\n        The base path where daily composite input data resides (e.g.,\n        '/ERDData1/modisa/data/modisgf/1day/').\n    param : str\n        The parameter type to process: either `'SST'` or `'Chla'`.\n    param_update_flag : list of bool\n        A list of length at least 4. For each lag in [-3, -2, -1], if the sum of\n        `param_update_flag[0]` through `param_update_flag[lag + 3]` is greater than zero,\n        the composite update scripts will be invoked for that lag.\n    composite : str\n        The composite interval identifier (e.g., `'3'`, `'5'`, `'8'`, `'14'`). Passed to\n        external scripts to control the composite period.\n\n    Returns\n    -------\n    None\n        This function does not return a value. It performs side effects by calling\n        external composite scripts.\n\n    Raises\n    ------\n    OSError\n        If any of the external composite scripts cannot be executed (e.g., the script\n        file is missing or not executable).\n    \"\"\"\n\n    for lag in list(range(-3, 0)):\n        myDate1 = now + timedelta(days=lag)\n        myYear = str(myDate1.year)\n        myMon = str(myDate1.month)\n        myMon = myMon.rjust(2, \"0\")\n        doy = myDate1.strftime(\"%j\").zfill(3)\n        dataDir = basedataDir + myYear + myMon\n        temp = param_update_flag[0 : (lag + 4)]\n        if sum(temp) &gt; 0:\n            if param == \"SST\":\n                # myCmd = '/home/cwatch/anaconda3/bin/python /home/cwatch/newPython/modisa/CompMBSST.py /ERDData1/modisa/data/modisgf/1day/ /ERDData1/modisa/work1/ ' + myYear + ' ' + doy + ' ' + composite\n                myCmd = (\n                    \"/home/cwatch/anaconda3/bin/python /home/cwatch/newPython/modisa/CompMBSST.py /ERDData1/modisa/data/modisgf/1day/ /ERDData1/modisa/work1/ \"\n                    + myYear\n                    + \" \"\n                    + doy\n                    + \" \"\n                    + composite\n                )\n                os.system(myCmd)\n                # myCmd = '/home/cwatch/anaconda3/bin/python /home/cwatch/newPython/modisa/CompMWSST.py /ERDData1/modisa/data/modiswc/1day/ /ERDData1/modisa/work/ ' + myYear + ' ' + doy + ' ' + composite\n                myCmd = (\n                    \"/home/cwatch/anaconda3/bin/python /home/cwatch/newPython/modisa/CompMWSST.py /ERDData1/modisa/data/modiswc/1day/ /ERDData1/modisa/work/ \"\n                    + myYear\n                    + \" \"\n                    + doy\n                    + \" \"\n                    + composite\n                )\n                os.system(myCmd)\n            else:\n                # myCmd = '/home/cwatch/anaconda3/bin/python /home/cwatch/newPython/modisa/CompMBChla.py /ERDData1/modisa/data/modisgf/1day/ /ERDData1/modisa/work1/ ' + myYear + ' ' + doy + ' ' + composite\n                myCmd = (\n                    \"/home/cwatch/anaconda3/bin/python /home/cwatch/newPython/modisa/CompMBChla.py /ERDData1/modisa/data/modisgf/1day/ /ERDData1/modisa/work1/ \"\n                    + myYear\n                    + \" \"\n                    + doy\n                    + \" \"\n                    + composite\n                )\n                os.system(myCmd)\n                # myCmd = '/home/cwatch/anaconda3/bin/python /home/cwatch/newPython/modisa/CompMWChla.py /ERDData1/modisa/data/modiswc/1day/ /ERDData1/modisa/work/ ' + myYear + ' ' + doy + ' ' + composite\n                myCmd = (\n                    \"/home/cwatch/anaconda3/bin/python /home/cwatch/newPython/modisa/CompMWChla.py /ERDData1/modisa/data/modiswc/1day/ /ERDData1/modisa/work/ \"\n                    + myYear\n                    + \" \"\n                    + doy\n                    + \" \"\n                    + composite\n                )\n                os.system(myCmd)\n\n\ndef url_lines(url):\n    \"\"\"\n    Download a file from a URL via `wget` and return its lines.\n\n    This function constructs a `wget` command to fetch the content at the\n    specified URL, saves it to a temporary files, retries up to 24 times\n    if the download yields an empty file, and then returns the file's lines\n    as a list of strings (with trailing newline characters stripped).\n\n    Parameters\n    ----------\n    url: str\n        The query string or full URL to pass to the NASA OceanData\n        file_search API via `wget`. This should include any necessary\n        parameters (e.g. `?dataset=...&time_min=...`).\n\n    Returns\n    -------\n    str\n        A list of lines (`str`) read from the downloaded file. Each\n        element has had its trailing newline removed.\n\n    Raises\n    ------\n    RuntimeError\n        If the file remains empty after the maximum number of retries.\n    FileNotFoundError\n        If the downloaded file cannot be opened.\n    OSError\n        If an underlying OS call fails (e.g., `wget` not found, permission\n        issues, etc.).\n    subprocess.CalledProcessError\n        If `wget` exits with a non-zero status (when called with\n        `check=True`).\n    \"\"\"\n\n    # Initialize an empty list to store the lines read from the downloaded file\n    myList = []\n\n    print(url)\n\n    # Initialize retry counter\n    no_tries = 1\n\n    # Attempt up to 24 additional downloads (total 24 tries) until the file in non-empty\n    while no_tries &lt; 25:\n\n        # Build the wget command to fetch data from the NASA OceanData API,\n        # saving output to a temporary file\n        wgetCmd = \"/usr/bin/wget -O /home/cwatch/newPython/modisa/fileNames.txt --no-check-certificate 'https://oceandata.sci.gsfc.nasa.gov/api/file_search?\" + url + \"'\"\n        # Print the command for debugging purposes\n        print(wgetCmd)\n\n        # Execute the command in a subshell; capture the return code\n        returnCode = subprocess.call(wgetCmd, shell = True)\n        print('return code: ' + str(returnCode))\n        \n        # Check size of the downloaded file\n        file_size = os.path.getsize('/home/cwatch/newPython/modisa/fileNames.txt')\n        print('search file size: ' + str(file_size))\n\n        # If the file has content, break out of the retry loop\n        if (file_size &gt; 0):\n            break\n        else:\n            # Otherwise, increment retry counter and wait before trying\n            no_tries = no_tries + 1\n            time.sleep(5)\n\n    # Open the downloaded file for reading       \n    f = open('/home/cwatch/newPython/modisa/fileNames.txt', 'r')\n\n    # iterate over each line in the file\n    for line in f:\n        # Strip trailing newline/carriage-return and append to our list\n        myList.append(line.rstrip())\n\n    # Close the file to free up resources\n    f.close()\n\n    # Print the first two entries for a quick preview\n    print('myList: ' + str(myList[0:2]))\n\n    # Return the full list of lines\n    return myList\n\n\ndef url_lines1(url):\n    \"\"\"\n    Fetch a list of file names from the NASA OceanData API.\n\n    This function constructs a full API request URL by appending the\n    provided query string to the OceanData file_search endpoint, then\n    attempts up to 24 retries (with a 5-second delay) to fetch the\n    response. Once a non-empty response is received, it decodes each\n    line as UTF-8, strips trailing whitespace, and returns the lines\n    as a list of strings.\n\n    Parameters\n    ----------\n    url: str\n        A query string containing one or more URL-encoded parameters\n        (e.g. `\"dataset=MODISA_L2&time_min=2025-06-01&time_max=2025-06-10\"`).\n\n    Returns\n    -------\n    str\n        A list of decoded, stripped lines from the API response.\n\n    Raises\n    ------\n    urllib.error.URLError\n        If there is a network problem or the URL cannot be reached.\n    urllib.error.HTTPError\n        If the server returns an HTTP error status (e.g., 404 or 500).\n    RuntimeError\n        If no data is returned after 24 retries.\n    \"\"\"\n\n    # Initialize empty list to collect decoded lines from the response\n    myList = []\n\n    print(url)\n\n    # Construct the full API endpoint URL by prefixing the base endpoint\n    url_new = 'https://oceandata.sci.gsfc.nasa.gov/api/file_search?' +  url\n    print(url_new)\n\n    # Initialize a retry counter; we will attempt up to 24 additional tries\n    no_tries = 1\n\n    # Loop until we either get data or exceed the retry limit\n    while no_tries &lt; 25:\n        # Open the URL and read all lines from the HTTP response\n        with urllib.request.urlopen(url_new) as response:\n            flist = response.readlines()\n\n        # Log how many lines were returned in the attempt\n        print('filelist length: ' + str(len(flist)))\n\n        # If we recieved at least one line, break out of the retry loop\n        if (len(flist) &gt; 0):\n            break\n        else:\n            # Otherwise, increment retry counter and pause before retrying\n            no_tries = no_tries + 1\n            time.sleep(5)\n\n    # Iterate over each raw byte line in the response list\n    for line in flist:\n        # Decode bytes to a UTF-8 string\n        junk = line.decode('UTF-8')\n        # Strip any trailing whitespace or newline characters and append to our list\n        myList.append(junk.rstrip())\n    \n    # Return the list of cleaned, decoded lines\n    return myList",
    "crumbs": [
      "Utility Library"
    ]
  },
  {
    "objectID": "roylib.html#roylib.py-library",
    "href": "roylib.html#roylib.py-library",
    "title": "Utility Library",
    "section": "",
    "text": "The roylib.py script is a core utilities library providing array‐reshaping, authenticated file downloads, and safe file operations, plus helpers for incremental statistics (mean/count), API-driven file discovery, and CF‐compliant NetCDF generation. It also handles remote deployments via rsync and leap‐year checks. The script import its functions (e.g., myReshape, get_netcdfFile, meanVar, makeNetcdfmDay, send_to_servers) at the top of your processing scripts. roylib.py provides shared functions used by many of the Python scripts on saltydog.\nFor full details on every function in roylib.py, see the API reference generated by Sphinx here.\n\n\nView roylib.py\nfrom __future__ import print_function\nfrom __future__ import division\nfrom future import standard_library\n\nstandard_library.install_aliases()\nfrom builtins import str\nfrom past.utils import old_div\nfrom datetime import date, datetime, timedelta\nfrom netCDF4 import Dataset, num2date, date2num\nimport numpy as np\nimport numpy.ma as ma\nimport os\nimport shutil\nimport subprocess\nimport time\nimport urllib.request, urllib.parse, urllib.error\nimport urllib.request, urllib.error, urllib.parse\n\n\ndef myReshape(dataArray):\n    \"\"\"\n    Flatten an N-dimensional array into a 2D column vector of type float32.\n\n    This helper is used before gridding routines that expect a single column of\n    values (shape: `(n_pixels, 1)`). If `dataArray` is a `numpy.ma.MaskedArray`,\n    masked entries are preserved in the output.\n\n    Parameters\n    ----------\n    dataArray : numpy.ndarray or numpy.ma.MaskedArray\n        Any N-dimensional array (e.g., `(n, m)`, `(n,)`, or higher rank).\n        If a masked array is provided, masks are preserved in the result.\n\n    Returns\n    -------\n    numpy.ma.MaskedArray or numpy.ndarray\n        A 2D array of shape `(dataArray.size, 1)` with dtype `float32`.\n        - If `dataArray` was a masked array, the result is a masked array.\n        - Otherwise, the result is a regular `ndarray` of `float32`.\n    \"\"\"\n    dataArray = dataArray.reshape(dataArray.size, 1)\n    dataArray = np.asarray(dataArray, np.float32)\n    return dataArray\n\n\ndef get_netcdfFile(fileName):\n    \"\"\"\n    Download a NetCDF file from the NASA OceanColor server using wget.\n\n    This function builds and executes a wget command that uses stored URS cookies\n    for authentication. The target file is fetched from the NASA OceanColor `getfile`\n    endpoint, which requires a valid URS session. The downloaded file is saved to\n    the current working directory under its original name.\n\n    Parameters\n    ----------\n    fileName : str\n        The name of the remote NetCDF file to download (e.g.,\n        `'A2023123006000.L2_LAC_SST.nc'`). This is appended to the base URL\n        `https://oceandata.sci.gsfc.nasa.gov/ob/getfile/` to form the full download URL.\n\n    Returns\n    -------\n    None\n        This function does not return a value. On success, the file appears in the\n        current directory. Existing files with the same name are overwritten.\n\n    Raises\n    ------\n    RuntimeError\n        If the wget command returns a non-zero exit status, indicating a download failure.\n    OSError\n        If the operating system command cannot be executed or if required tools are missing.\n    \"\"\"\n\n    baseURL = '\"https://oceandata.sci.gsfc.nasa.gov/ob/getfile/'\n    wgetCommand = (\n        \"/usr/bin/wget -4 --load-cookies ~/.urs_cookies --save-cookies ~/.urs_cookies --auth-no-challenge=on --keep-session-cookies --content-disposition --no-check-certificate \"\n        + baseURL\n        + fileName\n        + '\"'\n    )\n    print(wgetCommand)\n    #    urllib.urlretrieve(baseURL+fileName,fileName)\n    os.system(wgetCommand)\n\n\ndef safe_remove(fileName):\n    \"\"\"\n    Delete a file if it exists, handling errors gracefully.\n\n    This function checks whether the specified path refers to an existing file.\n    If so, it attempts to delete it. On Windows, it waits one second after\n    deletion to ensure the file handle is released. Any error during removal\n    (permission denied, file in use, etc.) is caught, and the function returns\n    False. Non-file paths also return False without raising.\n\n    Parameters\n    ----------\n    fileName : str\n        The path to the file to be removed. Can be absolute or relative.\n\n    Returns\n    -------\n    bool\n        True if the file existed and was removed successfully; False otherwise.\n        - Returns False if the path does not point to a regular file.\n        - Returns False if an exception occurs during deletion.\n\n    Raises\n    ------\n    None\n        All exceptions during file removal are caught; the function never raises.\n    \"\"\"\n    try:\n        if os.path.isfile(fileName):\n            os.remove(fileName)\n            if os.name == \"nt\":\n                time.sleep(1)  # seconds\n            return True\n        else:\n            return False\n    except:\n        return False\n\n\ndef send_to_servers(ncFile, dataDir, interval):\n    \"\"\"\n    Transfer a NetCDF file to multiple remote servers via rsync and optionally copy locally.\n\n    This function constructs an rsync command to push the specified file (`ncFile`) from\n    the current working directory to two remote hosts under `/u00/satellite&lt;remote_path&gt;`.\n    The `&lt;remote_path&gt;` depends on `dataDir` and `interval`. If `interval` is `\"0\"`, the\n    remote path is `&lt;dataDir&gt;/&lt;ncFile&gt;`; otherwise it is `&lt;dataDir&gt;&lt;interval&gt;day/&lt;ncFile&gt;`.\n    After rsyncing to both servers, if `interval` equals `\"1\"` and `dataDir` starts with\n    either `\"MB\"` or `\"MW\"`, the function also copies the file locally into the\n    corresponding `/ERDData1/modisa/data/modisgf/1day/` or\n    `/ERDData1/modisa/data/modiswc/1day/` directory.\n\n    Parameters\n    ----------\n    ncFile : str\n        The filename (with extension) of the NetCDF file to be transferred (e.g., `\"A2023123006000.L2.nc\"`).\n    dataDir : str\n        The base directory path on the remote server where the file should be placed.\n        This string is used both to build the remote path and to determine local copy logic\n        when `interval` is `\"1\"`. For example, `\"/MB2023123\"` or `\"/MW2023123\"`.\n    interval : str\n        The day interval specifier, typically `\"0\"`, `\"1\"`, `\"3\"`, etc. If `\"0\"`, no\n        subdirectory is appended; otherwise, `&lt;interval&gt;day` is appended to `dataDir`\n        (e.g., `\"3\"` → `\"3day\"`).\n\n    Returns\n    -------\n    None\n        This function does not return a value. On success, the file is transferred to\n        both remote servers and possibly copied locally under `/ERDData1/modisa/...`.\n\n    Raises\n    ------\n    RuntimeError\n        If any rsync command returns a non-zero exit code, indicating a failure in transfer.\n    OSError\n        If the local file cannot be read, written, or copied (e.g., permission denied,\n        path does not exist, or `shutil.copyfile` fails).\n    \"\"\"\n\n    intervalDay = interval + \"day\"\n    if interval == \"0\":\n        remote_file = dataDir + \"/\" + ncFile\n    else:\n        remote_file = dataDir + intervalDay + \"/\" + ncFile\n    myCmd = (\n        \"rsync -tvh \"\n        + ncFile\n        + \" cwatch@192.168.31.15:/u00/satellite\"\n        + remote_file\n    )\n    print(myCmd)\n    os.system(myCmd)\n    # myCmd = 'rsync -tvh ' + ncFile + ' cwatch@192.168.31.27:/u00/satellite' + remote_file\n    # print(myCmd)\n    # os.system(myCmd)\n    myCmd = (\n        \"rsync -tvh \"\n        + ncFile\n        + \" cwatch@161.55.17.28:/u00/satellite\"\n        + remote_file\n    )\n    print(myCmd)\n    os.system(myCmd)\n    if (intervalDay == \"1day\") and (dataDir[1:3] == \"MB\"):\n        # mvFile = '/ERDData1/modisa/data/modisgf/1day/' + ncFile\n        mvFile = \"/ERDData1/modisa/data/modisgf/1day/\" + ncFile\n        shutil.copyfile(ncFile, mvFile)\n    if (intervalDay == \"1day\") and (dataDir[1:3] == \"MW\"):\n        # mvFile = '/ERDData1/modisa/data/modiswc/1day/' + ncFile\n        mvFile = \"/ERDData1/modisa/data/modiswc/1day/\" + ncFile\n        shutil.copyfile(ncFile, mvFile)\n\n\ndef send_ncml_to_servers(ncmlFile, ncmlDir, dataDir):\n    \"\"\"\n    Transfer a NetCDF Markup Language (NCML) file to three specified remote servers via rsync.\n\n    This function constructs and executes three rsync commands to copy the NCML file from\n    the local directory (`ncmlDir`) to each remote host under `/u00/satellite/&lt;dataDir&gt;`.\n    The target hosts are 192.168.31.15, 192.168.31.27, and 161.55.17.28.\n\n    Parameters\n    ----------\n    ncmlFile : str\n        The filename of the NCML file to transfer (e.g., \"example.ncml\"). This file is assumed\n        to reside in `ncmlDir`.\n    ncmlDir : str\n        Local directory path where `ncmlFile` is located.\n    dataDir : str\n        The target directory path (relative to `/u00/satellite/`) on each remote server.\n        The full remote path becomes `/u00/satellite/&lt;dataDir&gt;&lt;ncmlFile&gt;`.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    RuntimeError\n        If any rsync command returns a non-zero exit code, indicating a transfer failure.\n    OSError\n        If the local NCML file cannot be found or accessed, or if rsync is not available.\n    \"\"\"\n\n    send_file = ncmlDir + \"/\" + ncmlFile\n    myCmd = (\n        \"rsync -tvh \"\n        + send_file\n        + \" cwatch@192.168.31.15:/u00/satellite\"\n        + dataDir\n        + ncmlFile\n    )\n    os.system(myCmd)\n    myCmd = (\n        \"rsync -tvh \"\n        + send_file\n        + \" cwatch@192.168.31.27:/u00/satellite\"\n        + dataDir\n        + ncmlFile\n    )\n    os.system(myCmd)\n    myCmd = (\n        \"rsync -tvh \"\n        + send_file\n        + \" cwatch@161.55.17.28:/u00/satellite\"\n        + dataDir\n        + ncmlFile\n    )\n    os.system(myCmd)\n\n\ndef retrieve_new_files(dataDir, param, myYear, doy, param_update_flag, lag):\n    \"\"\"\n    Check for and download new MODIS files for a given parameter and date.\n\n    This function constructs a search query for the NASA OceanColor file_search API\n    based on the provided parameter (`\"OC\"` or other). It computes the calendar date\n    from `myYear` and `doy`, then builds a query string of the form:\n    `search=AQUA_MODIS.&lt;YYYY&gt;&lt;MM&gt;&lt;DD&gt;*L2.&lt;param&gt;.NRT.nc&dtype=L2&sensor=aqua&results_as_file=1`.\n    It calls `url_lines1` to fetch the list of matching filenames. For each filename,\n    it checks whether the file already exists in `dataDir`; if not, it sets the\n    corresponding index in `param_update_flag` to True and invokes `get_netcdfFile`\n    to download the missing file, pausing 20 seconds between downloads.\n\n    Parameters\n    ----------\n    dataDir : str\n        The directory where downloaded files should be saved (e.g., `/ERDData1/modisa/data/netcdf/`).\n    param : str\n        The MODIS data parameter to retrieve, typically `\"OC\"` (ocean color) or another dataset code.\n    myYear : str\n        The four-digit year (e.g., `\"2023\"`).\n    doy : str\n        The day of year as a zero-padded string (e.g., `\"005\"` for January 5).\n    param_update_flag : list of bool\n        A list of length at least 4, where `param_update_flag[lag + 3]` is set to True\n        if a new file for that lag index needs to be downloaded.\n    lag : int\n        The offset relative to the current date, ranging from -3 to 0. This determines\n        which index in `param_update_flag` to update (`lag + 3`).\n\n    Returns\n    -------\n    None\n        The function does not return a value. It updates `param_update_flag` in place\n        and downloads any missing NetCDF files to `dataDir`.\n\n    Raises\n    ------\n    urllib.error.URLError\n        If the HTTP request inside `url_lines1` fails (e.g., network error).\n    OSError\n        If file operations (checking existence or writing) fail, or if `get_netcdfFile`\n        cannot write the downloaded file.\n    \"\"\"\n\n    if param == \"OC\":\n        # modis_search_URL = 'search=A' + myYear + doy + '*L2_LAC_' + param + '.nc&dtype=L2&sensor=aqua&results_as_file=1'\n        myDate = datetime(int(myYear), 1, 1) + timedelta(int(doy) - 1)\n        myMonth = str(myDate.month).rjust(2, \"0\")\n        myDay = str(myDate.day).rjust(2, \"0\")\n        modis_search_URL = (\n            \"search=AQUA_MODIS.\"\n            + myYear\n            + myMonth\n            + myDay\n            + \"*L2.\"\n            + param\n            + \".NRT.nc&dtype=L2&sensor=aqua&results_as_file=1\"\n        )\n        print(doy)\n        print(lag)\n        print(modis_search_URL)\n        # fileList = url_lines(modis_search_URL)\n        fileList = url_lines1(modis_search_URL)\n        for fName in fileList:\n            fileTest = os.path.isfile(dataDir + \"/\" + fName)\n            if not (fileTest):\n                param_update_flag[lag + 3] = True\n                print(fName)\n                get_netcdfFile(fName)\n                time.sleep(20)\n    else:\n        myDate = datetime(int(myYear), 1, 1) + timedelta(int(doy) - 1)\n        myMonth = str(myDate.month).rjust(2, \"0\")\n        myDay = str(myDate.day).rjust(2, \"0\")\n        modis_search_URL = (\n            \"search=AQUA_MODIS.\"\n            + myYear\n            + myMonth\n            + myDay\n            + \"*L2.\"\n            + param\n            + \".NRT.nc&dtype=L2&sensor=aqua&results_as_file=1\"\n        )\n        print(doy)\n        print(lag)\n        print(modis_search_URL)\n        # fileList = url_lines(modis_search_URL)\n        fileList = url_lines1(modis_search_URL)\n        for fName in fileList:\n            fileTest = os.path.isfile(dataDir + \"/\" + fName)\n            if not (fileTest):\n                param_update_flag[lag + 3] = True\n                print(fName)\n                get_netcdfFile(fName)\n                time.sleep(20)\n\n\ndef retrieve_new_files1(dataDir, param, myYear, doy, param_update_flag, lag):\n    \"\"\"\n    Check for and download new MODIS files, handling OC and non-OC parameters differently.\n\n    This function constructs a search query for the NASA OceanColor file_search API based on\n    the given parameter. For `param == \"OC\"`, it uses a legacy L2_LAC endpoint; otherwise it uses\n    the NRT endpoint. It computes the calendar date from `myYear` and `doy`, builds the query\n    string, and retrieves the list of matching filenames via `url_lines1`. The filenames are sorted,\n    and for each filename not already present in `dataDir`, it sets the corresponding index in\n    `param_update_flag` to True and calls `get_netcdfFile` to download the file, pausing 20 seconds\n    between downloads.\n\n    Parameters\n    ----------\n    dataDir : str\n        The directory where downloaded files should be saved (e.g., `/ERDData1/modisa/data/netcdf/`).\n    param : str\n        The MODIS data parameter to retrieve. If `\"OC\"`, the search endpoint uses L2_LAC_OC; otherwise\n        it uses the NRT endpoint (e.g., `\"SST\"` or other parameter codes).\n    myYear : str\n        The four-digit year (e.g., `\"2023\"`). Used to construct the search URL.\n    doy : str\n        The day of year as a zero-padded string (e.g., `\"005\"` for January 5). Used to construct the search URL.\n    param_update_flag : list of bool\n        A list of length at least 4, where `param_update_flag[lag + 3]` is set to True if a new file\n        needs to be downloaded for the given lag index.\n    lag : int\n        The offset relative to the current date, ranging from -3 to 0. Determines which index in\n        `param_update_flag` to update (`lag + 3`).\n\n    Returns\n    -------\n    None\n        Updates `param_update_flag` in place and downloads any missing NetCDF files to `dataDir`.\n\n    Raises\n    ------\n    urllib.error.URLError\n        If the HTTP request inside `url_lines1` encounters a network error on all retry attempts.\n    OSError\n        If file operations fail (e.g., checking existence, writing to `dataDir`), or if `get_netcdfFile`\n        cannot write the downloaded file.\n    \"\"\"\n    # Rest of the code...\n\n    if param == \"OC\":\n        # modis_search_URL = 'search=A' + myYear + doy + '*L2_LAC_' + param + '.nc&dtype=L2&sensor=aqua&results_as_file=1'\n        myDate = datetime(int(myYear), 1, 1) + timedelta(int(doy) - 1)\n        myMonth = str(myDate.month).rjust(2, \"0\")\n        myDay = str(myDate.day).rjust(2, \"0\")\n        modis_search_URL = (\n            \"search=A\"\n            + myYear\n            + doy\n            + \"*L2_LAC_\"\n            + param\n            + \".nc&dtype=L2&sensor=aqua&results_as_file=1\"\n        )\n        print(doy)\n        print(lag)\n        print(modis_search_URL)\n        print(doy)\n        print(lag)\n        print(modis_search_URL)\n        # fileList = url_lines(modis_search_URL)\n        fileList = url_lines1(modis_search_URL)\n        fileList.sort()\n        for fName in fileList:\n            fileTest = os.path.isfile(dataDir + \"/\" + fName)\n            if not (fileTest):\n                param_update_flag[lag + 3] = True\n                print(fName)\n                get_netcdfFile(fName)\n                time.sleep(20)\n    else:\n        myDate = datetime(int(myYear), 1, 1) + timedelta(int(doy) - 1)\n        myMonth = str(myDate.month).rjust(2, \"0\")\n        myDay = str(myDate.day).rjust(2, \"0\")\n        modis_search_URL = (\n            \"search=AQUA_MODIS.\"\n            + myYear\n            + myMonth\n            + myDay\n            + \"*L2.\"\n            + param\n            + \".NRT.nc&dtype=L2&sensor=aqua&results_as_file=1\"\n        )\n        print(doy)\n        print(lag)\n        print(modis_search_URL)\n        # fileList = url_lines(modis_search_URL)\n        fileList = url_lines1(modis_search_URL)\n        fileList.sort()\n        for fName in fileList:\n            fileTest = os.path.isfile(dataDir + \"/\" + fName)\n            if not (fileTest):\n                param_update_flag[lag + 3] = True\n                print(fName)\n                get_netcdfFile(fName)\n                time.sleep(20)\n\n\ndef isleap(year):\n    \"\"\"\n    Determine whether the specified year is a leap year.\n\n    This function attempts to construct a date object for February 29 of the given year.\n    If the date is valid, the year is a leap year; otherwise, a ValueError is raised\n    and the function returns False.\n\n    Parameters\n    ----------\n    year : int\n        The year to check (e.g., 2024).\n\n    Returns\n    -------\n    bool\n        True if `year` is a leap year (i.e., February 29 exists in that year);\n        False otherwise.\n\n    Raises\n    ------\n    None\n        Any ValueError raised by `datetime.date(year, 2, 29)` is caught internally;\n        the function never propagates exceptions.\n    \"\"\"\n\n    from datetime import date, datetime, timedelta\n\n    try:\n        date(year, 2, 29)\n        return True\n    except ValueError:\n        return False\n\n\ndef meanVar(mean, num, obs):\n    \"\"\"\n    Update the running mean and count of observations with new data.\n\n    Parameters\n    ----------\n    mean : numpy.ma.MaskedArray or numpy.ndarray\n        The current running mean values for each element.\n    num : numpy.ndarray\n        The current count of valid observations for each element (integer array).\n    obs : numpy.ma.MaskedArray\n        The new observations with the same shape as `mean`. Masked entries are not used.\n\n    Returns\n    -------\n    tuple\n        A 2-tuple `(updated_mean, updated_count)` where:\n        - `updated_mean` is a masked or regular array of new mean values (dtype float32).\n        - `updated_count` is an integer array of updated counts (dtype int32).\n\n    Raises\n    ------\n    None\n        Shape mismatches or invalid operations will propagate NumPy errors.\n    \"\"\"\n\n    import numpy as np\n    import numpy.ma as ma\n\n    numShape = num.shape\n    temp = np.subtract(obs, mean, dtype=np.single)\n    numAdd = np.ones(numShape, dtype=np.int32)\n    numAdd[obs.mask] = 0\n    num = np.add(num, numAdd, dtype=np.int32)\n    tempNum = ma.array(num, mask=(num == 0), dtype=np.int32)\n    temp = np.divide(temp, tempNum.astype(\"float\"), dtype=np.single)\n    mean = np.add(mean, temp.filled(0.0), dtype=np.single)\n    return (mean, num)\n\n\ndef mean_sumsq(mean, ss, num, obs):\n    \"\"\"\n    Cumulative calculation of mean and sum of squares for masked arrays.\n\n    This function updates the running mean, sum of squares, and count of observations\n    given a new set of observations. All input arrays (`mean`, `ss`, and `obs`) must be\n    NumPy masked arrays of identical shape. If any input is not a masked array, the\n    function exits with an error message.\n\n    Parameters\n    ----------\n    mean : numpy.ma.MaskedArray\n        The current running mean array. Masked entries indicate missing data and\n        are not included in the update.\n    ss : numpy.ma.MaskedArray\n        The current running sum of squares array. Masked entries are not included\n        in the update.\n    num : numpy.ndarray\n        The current count of valid observations for each element, as an integer array\n        of the same shape as `mean`. This array is incremented for each unmasked entry\n        in `obs`.\n    obs : numpy.ma.MaskedArray\n        The new observations to incorporate. Masked entries indicate missing data and\n        are not used in updating `mean`, `ss`, or `num`.\n\n    Returns\n    -------\n    tuple\n        A 3-tuple `(updated_mean, updated_ss, updated_num)` where:\n        - `updated_mean` (numpy.ma.MaskedArray): The updated running mean array.\n        - `updated_ss` (numpy.ma.MaskedArray): The updated running sum of squares array.\n        - `updated_num` (numpy.ndarray): The updated count of observations as an integer array.\n\n    Raises\n    ------\n    SystemExit\n        If any of `mean`, `ss`, or `obs` is not a `numpy.ma.MaskedArray`, the function\n        prints an error message and exits.\n    \"\"\"\n    import numpy as np\n    import numpy.ma as ma\n    import sys\n\n    if (\n        not isinstance(mean, np.ma.MaskedArray)\n        or not isinstance(ss, np.ma.MaskedArray)\n        or not isinstance(obs, np.ma.MaskedArray)\n    ):\n\n        print(\"Input arguments mean, ss, and obs are not numpy masked arrays\")\n        print(\"Try converting mean, ss, and obs to masked arrays\")\n        print(\"before using them in the function, e.g. ma.array(arg)\")\n        print(\" \")\n        sys.exit(mean_sumsq.__doc__)\n\n    numShape = num.shape\n    temp = np.subtract(obs, mean.filled(0.0), dtype=np.single)\n    numAdd = np.ones(numShape, dtype=np.int32)\n    numAdd[obs.mask] = 0\n    num = np.add(num, numAdd, dtype=np.int32)\n    tempNum = ma.array(num, mask=(num == 0), dtype=np.int32)\n    print(\"tempNum\", tempNum.min(), tempNum.max())\n    tNfloat = tempNum.astype(\"float\")\n\n    temp = ma.divide(temp, tNfloat, dtype=np.single)\n    mean = np.add(mean.filled(0.0), temp.filled(0.0), dtype=np.single)\n    # mean = ma.masked_where(mean == 0., mean)\n\n    num1 = np.copy(num)\n    num2 = num1 - 1\n    print(\"num2\", num2.min(), num2.max())\n    if np.any(num1 &gt; 1):\n        num1 = np.divide(num1, num2, where=(num1 &gt; 1))\n\n    print(\"num1\", num1.min(), num1.max())\n    print(\"num\", num.min(), num.max())\n    temp1 = ma.subtract(obs, mean, dtype=np.single)\n    temp2 = np.multiply(temp1, temp1)\n    print(\n        \"ss parts\",\n        ss.filled(0.0).max(),\n        ma.multiply(num1, temp2).filled(0.0).max(),\n    )\n    ss = ss.filled(0.0) + ma.multiply(num1, temp2).filled(0.0)\n\n    mean = ma.masked_where(num == 0, mean)\n    ss = ma.masked_where(num == 0, ss)\n    vr = np.divide(ss, num)\n    sdev = np.sqrt(vr)\n    print(\"stDev\", sdev.min(), sdev.max(), sdev.mean())\n    # print(stdev.min(), stdev.max(), stdev.mean())\n    return (mean, ss, num)\n\n\ndef makeNetcdf(mean, nobs, interval, outFile, filesUsed, workDir):\n    \"\"\"\n    Create a NetCDF file from aggregated data arrays and assign metadata.\n\n    Parameters\n    ----------\n    mean : numpy.ma.MaskedArray or numpy.ndarray\n        A 2D array of mean values (masked or regular). Masked entries are treated as missing.\n    nobs : int\n        The number of observations used to compute `mean`. This value is recalculated from `mean`.\n    interval : int\n        Time interval in days (e.g., 1, 3, 5, 8, 14). Determines which CDL template to use.\n    outFile : str\n        Desired output filename (e.g., '/path/to/output/MB2023123001.nc'). The function infers:\n        - `dataset`: first two characters.\n        - `param`: substring after the first underscore past index 10.\n        - `time1` and `time2`: substrings indicating start and end DOY.\n    filesUsed : list of str\n        List of source filenames that contributed to `mean`. Stored in the NetCDF’s `files` attribute.\n    workDir : str\n        Directory in which to execute `ncgen` and write the new NetCDF file.\n\n    Returns\n    -------\n    str\n        The path of the generated NetCDF file (with “.nc” extension).\n\n    Raises\n    ------\n    OSError\n        If changing directory to `workDir` fails or if `ncgen` cannot be executed.\n    RuntimeError\n        If the `ncgen` command returns a non-zero exit code.\n    \"\"\"\n\n    print(interval)\n    os.chdir(workDir)\n    now = datetime.now()\n    now1 = date(now.year, now.month, now.day)\n    nobs = ma.count(mean)\n    noMiss = ma.count_masked(mean)\n    percentCoverage = old_div(float(nobs), float(nobs + noMiss))\n    # get netcdf file name and correct cdl file\n    ncFile = outFile[:-3]\n    dataset = outFile[0:2]\n    offset = ncFile.find(\"_\", 10)\n    param = ncFile[(offset + 1) : len(ncFile)]\n    time1 = outFile[2:9]\n    time2 = outFile[10:17]\n    ncFile = ncFile + \".nc\"\n    interval1 = str(interval)\n    print(ncFile)\n    print(dataset)\n    print(param)\n    print(interval1)\n    # cdlFile = '/ERDData1/modisa/python/' + dataset + param + interval1 + 'Day.cdl'\n    cdlFile = (\n        \"/ERDData1/modisa/python/\" + dataset + param + interval1 + \"Day.cdl\"\n    )\n    print(cdlFile)\n    # os.system('/usr/bin/ncgen -o ' + ncFile + ' ' + cdlFile)\n    os.system(\"/usr/bin/ncgen -o \" + ncFile + \" \" + cdlFile)\n    # shutil.copyfile(cdlFile, ncFile)\n    ncPointer = Dataset(ncFile, \"a\")\n    mytime = ncPointer.variables[\"time\"]\n    ncPointer.files = filesUsed\n    ncPointer.date_created = str(now1)\n    ncPointer.date_issued = str(now1)\n    paramName = dataset + param\n    myparam = ncPointer.variables[paramName]\n    tempName = myparam.long_name\n    composite = \"(\" + interval1 + \" Day Composite)\"\n    tempName = tempName.replace(\"(3 Day Composite)\", composite)\n    myparam.long_name = tempName\n    myparam.numberOfObservations = nobs\n    myparam.percentCoverage = percentCoverage\n    myparam[0, 0, :, :] = mean[:, :]\n    myparam.actual_range = np.array(([mean.min(), mean.max()]))\n    startTimeYear = int(time1[0:4])\n    startTimeDoy = int(time1[4:7])\n    startDate = datetime(startTimeYear, 1, 1, 0) + timedelta(startTimeDoy - 1)\n    if interval1 == \"1\":\n        centerDate = startDate + timedelta(hours=12)\n    elif interval1 == \"3\":\n        centerDate = startDate + timedelta(hours=36)\n    elif interval1 == \"5\":\n        centerDate = startDate + timedelta(hours=60)\n    elif interval1 == \"8\":\n        centerDate = startDate + timedelta(days=4)\n    elif interval1 == \"14\":\n        centerDate = startDate + timedelta(days=7)\n\n    udtime = date2num(centerDate, units=\"seconds since 1970-01-01\")\n    mytime[0] = udtime\n    mytime.actual_range = np.array(([udtime, udtime]))\n    ncPointer.close()\n    return ncFile\n\n\ndef makeNetcdfmDay(mean, nobs, interval, outFile, filesUsed, workDir):\n    \"\"\"\n    Create a NetCDF file for multi-day composite data.\n\n    Parameters\n    ----------\n    mean : numpy.ma.MaskedArray or numpy.ndarray\n        A 2D array of mean values for the composite period. Masked entries are treated as missing.\n    nobs : int\n        The number of observations used to compute `mean`. Recomputed internally from `mean`.\n    interval : float\n        The length of the composite period (in days). Used to select the “mDay” CDL template.\n    outFile : str\n        Desired output filename (ending in “.nc”). The function derives:\n        - `dataset`: first two characters of `outFile`.\n        - `param`: substring after the first underscore past index 10.\n        - `time1` and `time2`: substrings for start/end DOY.\n    filesUsed : list of str\n        A list of source filenames that contributed to `mean`. Stored in NetCDF’s `files` attribute.\n    workDir : str\n        Directory in which to execute `ncgen` and write the new NetCDF file.\n\n    Returns\n    -------\n    str\n        The path of the generated NetCDF file (with “.nc” extension).\n\n    Raises\n    ------\n    OSError\n        If changing directory to `workDir` fails or if `ncgen` cannot be executed.\n    RuntimeError\n        If the `ncgen` command returns a non-zero exit code.\n    \"\"\"\n\n    os.chdir(workDir)\n    now = datetime.now()\n    now1 = date(now.year, now.month, now.day)\n    nobs = ma.count(mean)\n    noMiss = ma.count_masked(mean)\n    percentCoverage = old_div(float(nobs), float(nobs + noMiss))\n    # get netcdf file name and correct cdl file\n    ncFile = outFile[:-3]\n    dataset = outFile[0:2]\n    offset = ncFile.find(\"_\", 10)\n    param = ncFile[(offset + 1) : len(ncFile)]\n    time1 = outFile[2:9]\n    time2 = outFile[10:17]\n    ncFile = ncFile + \".nc\"\n    print(ncFile)\n    # cdlFile = '/ERDData1/modisa/python/' + dataset + param + 'mDay.cdl'\n    cdlFile = \"/ERDData1/modisa/python/\" + dataset + param + \"mDay.cdl\"\n    print(cdlFile)\n    os.system(\"/usr/bin/ncgen -o \" + ncFile + \" \" + cdlFile)\n    # shutil.copyfile(cdlFile, ncFile)\n    ncPointer = Dataset(ncFile, \"a\")\n    mytime = ncPointer.variables[\"time\"]\n    ncPointer.files = filesUsed\n    ncPointer.date_created = str(now1)\n    ncPointer.date_issued = str(now1)\n    paramName = dataset + param\n    myparam = ncPointer.variables[paramName]\n    myparam.numberOfObservations = nobs\n    myparam.percentCoverage = percentCoverage\n    myparam[0, 0, :, :] = mean[:, :]\n    myparam.actual_range = np.array(([mean.min(), mean.max()]))\n    startTimeYear = int(time1[0:4])\n    startTimeDoy = int(time1[4:7])\n    startDate = datetime(startTimeYear, 1, 1, 0) + timedelta(startTimeDoy - 1)\n    endTimeYear = int(time2[0:4])\n    endTimeDoy = int(time2[4:7])\n    endDate = datetime(endTimeYear, 1, 1, 0) + timedelta(endTimeDoy - 1)\n    centerDoy = old_div((startTimeDoy + endTimeDoy), 2.0)\n    centerDate = datetime(startTimeYear, 1, 1, 0) + timedelta(centerDoy - 1)\n    print(centerDate)\n    udtime = date2num(centerDate, units=\"seconds since 1970-01-01\")\n    if (endTimeDoy - startTimeDoy + 1) == 31:\n        udtime = udtime + 43200\n    print(udtime)\n    mytime[0] = udtime\n    mytime.actual_range = np.array(([udtime, udtime]))\n    ncPointer.close()\n    return ncFile\n\n\ndef grd2netcdf(grdFile, filesUsed, fType):\n    \"\"\"\n    Convert a GRD file to a NetCDF file, copying spatial data and metadata.\n\n    This function reads a GRD file containing gridded data (with variables named\n    either \"lon\"/\"lat\"/\"z\" or \"x\"/\"y\"/\"z\" depending on `fType`), computes coverage statistics,\n    generates a NetCDF file via an `ncgen` command on a corresponding CDL template, and\n    writes spatial coordinates, data values, and metadata (file list, creation date,\n    observation count, coverage, and actual range) into the new NetCDF. The time variable\n    is centered based on the date stamps in the original filename.\n\n    Parameters\n    ----------\n    grdFile : str\n        Path to the input GRD file (e.g., \"/path/to/MB2023123001000.grd\").\n        The filename must encode the dataset, parameter, and date information:\n        - The first two characters are the dataset code.\n        - Characters 2-9 represent the start date (YYYYDDD).\n        - Characters 10-17 represent the end date (YYYYDDD).\n    filesUsed : list of str\n        A list of source filenames that contributed to the GRD data. This list is stored\n        in the NetCDF file's `files` attribute.\n    fType : str\n        File type indicator:\n        - `\"MW\"` uses variables `\"lon\"`, `\"lat\"`, and `\"z\"`.\n        - Any other value uses variables `\"x\"`, `\"y\"`, and `\"z\"`.\n\n    Returns\n    -------\n    str\n        The path of the generated NetCDF file (same as `grdFile` with a “.nc” extension).\n\n    Raises\n    ------\n    OSError\n        If reading the GRD file or executing the `ncgen` command fails (e.g., file not found,\n        permission denied).\n    RuntimeError\n        If the `ncgen` command returns a non-zero exit code, indicating failure to generate\n        the NetCDF file.\n    \"\"\"\n\n    now = datetime.now()\n    now1 = date(now.year, now.month, now.day)\n    grdPointer = Dataset(grdFile)\n    if fType == \"MW\":\n        x = grdPointer.variables[\"lon\"][:]\n        y = grdPointer.variables[\"lat\"][:]\n        z = grdPointer.variables[\"z\"][:, :]\n    else:\n        x = grdPointer.variables[\"x\"][:]\n        y = grdPointer.variables[\"y\"][:]\n        z = grdPointer.variables[\"z\"][:, :]\n    grdPointer.close()\n    nobs = ma.count(z)\n    noMiss = ma.count_masked(z)\n    percentCoverage = old_div(float(nobs), float(nobs + noMiss))\n    # get netcdf file name and correct cdl file\n    ncFile = grdFile[:-4]\n    dataset = grdFile[0:2]\n    offset = ncFile.find(\"_\", 10)\n    param = ncFile[(offset + 1) : len(ncFile)]\n    time1 = grdFile[2:9]\n    time2 = grdFile[10:17]\n    interval = str(int(time2) - int(time1) + 1)\n    ncFile = ncFile + \".nc\"\n    print(ncFile)\n    # cdlFile = '/ERDData1/modisa/python/' + dataset + param + interval + 'Day.cdl'\n    cdlFile = (\n        \"/ERDData1/modisa/python/\" + dataset + param + interval + \"Day.cdl\"\n    )\n    print(cdlFile)\n    os.system(\"/usr/bin/ncgen -o \" + ncFile + \" \" + cdlFile)\n    # shutil.copyfile(cdlFile, ncFile)\n    ncPointer = Dataset(ncFile, \"a\")\n    lat = ncPointer.variables[\"lat\"]\n    lon = ncPointer.variables[\"lon\"]\n    mytime = ncPointer.variables[\"time\"]\n    ncPointer.files = filesUsed\n    ncPointer.date_created = str(now1)\n    ncPointer.date_issued = str(now1)\n    paramName = dataset + param\n    myparam = ncPointer.variables[paramName]\n    lat[:] = y[:]\n    lon[:] = x[:]\n    myparam[0, 0, :, :] = z[:, :]\n    myparam.numberOfObservations = nobs\n    myparam.percentCoverage = percentCoverage\n    myparam.actual_range = np.array(([z.min(), z.max()]))\n    startTimeYear = int(time1[0:4])\n    startTimeDoy = int(time1[4:7])\n    startDate = datetime(startTimeYear, 1, 1, 0) + timedelta(startTimeDoy - 1)\n    if interval == \"1\":\n        centerDate = startDate + timedelta(hours=12)\n    elif interval == \"3\":\n        centerDate = startDate + timedelta(hours=36)\n    elif interval == \"5\":\n        centerDate = startDate + timedelta(hours=60)\n    elif interval == \"8\":\n        centerDate = startDate + timedelta(days=4)\n    elif interval == \"14\":\n        centerDate = startDate + timedelta(days=7)\n\n    udtime = date2num(centerDate, units=\"seconds since 1970-01-01\")\n    mytime[0] = udtime\n    mytime.actual_range = np.array(([udtime, udtime]))\n    ncPointer.close()\n    return ncFile\n\n\ndef grd2netcdf1(grdFile, fileOut, filesUsed, my_mask, fType):\n    \"\"\"\n    Convert an Xarray grid file to a NetCDF file, applying a land mask and writing metadata.\n\n    Parameters\n    ----------\n    grdFile : xarray.Dataset\n        The input grid dataset. If `fType == \"MW\"`, it must have `.lon`, `.lat`, and `.values`.\n        Otherwise, it must have `.x`, `.y`, and `.values`.\n    fileOut : str\n        The target output filename (e.g., '/path/to/output/MB2023123001.nc'). The function\n        derives dataset, parameter, start/end DOY from this name.\n    filesUsed : list of str\n        A list of source filenames that contributed to the grid; stored in the NetCDF's `files`.\n    my_mask : numpy.ndarray\n        A boolean or integer mask array of the same shape as `grdFile.values`. Points where\n        `my_mask != 1` are set to NaN and then masked.\n    fType : str\n        File type indicator:\n        - `\"MW\"` → use `grdFile.lon.values`, `grdFile.lat.values`, and `grdFile.values`.\n        - Any other string → use `grdFile.x.values`, `grdFile.y.values`, and `grdFile.values`.\n\n    Returns\n    -------\n    str\n        The path of the generated NetCDF file (same as `fileOut` with “.nc” extension).\n\n    Raises\n    ------\n    OSError\n        If the GRD file cannot be read or if the `ncgen` command fails to run.\n    RuntimeError\n        If `ncgen` returns a non-zero exit code.\n    \"\"\"\n\n    from datetime import datetime, timedelta\n    from netCDF4 import Dataset\n    import xarray as xr\n\n    now = datetime.now()\n    now1 = date(now.year, now.month, now.day)\n    if fType == \"MW\":\n        x = grdFile.lon.values\n        y = grdFile.lat.values\n        z = grdFile.values\n    else:\n        x = grdFile.x.values\n        y = grdFile.y.values\n        z = grdFile.values\n\n    # set areas in land mask not land to NaN\n    z[my_mask != 1] = np.NAN\n    # convert z to ma.array with NaNs masked\n    z = ma.array(z, mask=np.isnan(z), fill_value=-9999999.0)\n    nobs = ma.count(z)\n    noMiss = ma.count_masked(z)\n    percentCoverage = old_div(float(nobs), float(nobs + noMiss))\n    # get netcdf file name and correct cdl file\n    ncFile = fileOut[:-4]\n    dataset = fileOut[0:2]\n    offset = ncFile.find(\"_\", 10)\n    param = ncFile[(offset + 1) : len(ncFile)]\n    time1 = fileOut[2:9]\n    time2 = fileOut[10:17]\n    interval = str(int(time2) - int(time1) + 1)\n    ncFile = ncFile + \".nc\"\n    print(ncFile)\n    # cdlFile = '/ERDData1/modisa/python/' + dataset + param + interval + 'Day.cdl'\n    cdlFile = (\n        \"/ERDData1/modisa/python/\" + dataset + param + interval + \"Day.cdl\"\n    )\n    print(cdlFile)\n    os.system(\"/usr/bin/ncgen -o \" + ncFile + \" \" + cdlFile)\n    # shutil.copyfile(cdlFile, ncFile)\n    ncPointer = Dataset(ncFile, \"a\")\n    lat = ncPointer.variables[\"lat\"]\n    lon = ncPointer.variables[\"lon\"]\n    mytime = ncPointer.variables[\"time\"]\n    ncPointer.files = filesUsed\n    ncPointer.date_created = str(now1)\n    ncPointer.date_issued = str(now1)\n    paramName = dataset + param\n    myparam = ncPointer.variables[paramName]\n    lat[:] = y[:]\n    lon[:] = x[:]\n    myparam[0, 0, :, :] = z[:, :]\n    myparam.numberOfObservations = nobs\n    myparam.percentCoverage = percentCoverage\n    myparam.actual_range = np.array(([z.min(), z.max()]))\n    startTimeYear = int(time1[0:4])\n    startTimeDoy = int(time1[4:7])\n    startDate = datetime(startTimeYear, 1, 1, 0) + timedelta(startTimeDoy - 1)\n    if interval == \"1\":\n        centerDate = startDate + timedelta(hours=12)\n    elif interval == \"3\":\n        centerDate = startDate + timedelta(hours=36)\n    elif interval == \"5\":\n        centerDate = startDate + timedelta(hours=60)\n    elif interval == \"8\":\n        centerDate = startDate + timedelta(days=4)\n    elif interval == \"14\":\n        centerDate = startDate + timedelta(days=7)\n\n    udtime = date2num(centerDate, units=\"seconds since 1970-01-01\")\n    mytime[0] = udtime\n    mytime.actual_range = np.array(([udtime, udtime]))\n    ncPointer.close()\n    return ncFile\n\n\ndef update_modis_1day(now, basedataDir, param, param_update_flag):\n    \"\"\"\n    Execute daily MODIS processing scripts for SST or Chla based on update flags.\n\n    This function iterates over the three most recent lags (-3, -2, -1 days relative to `now`).\n    For each lag where `param_update_flag[lag + 3]` is True, it computes the corresponding date,\n    constructs the directory path under `basedataDir`, and invokes the appropriate daily processing\n    scripts via `os.system`. For `\"SST\"`, it runs both `makeSST1daynewMW.py` and `makeSST1daynewMB.py`;\n    for `\"Chla\"`, it runs both `makeChla1daynewMW.py` and `makeChla1daynewMB.py`. Each script is\n    passed the year and day-of-year strings as arguments.\n\n    Parameters\n    ----------\n    now : datetime.datetime\n        The reference datetime (typically the current date and time). Used to compute the target date\n        for each lag.\n    basedataDir : str\n        The base path for data directories (e.g., \"/ERDData1/modisa/data/netcdf/\"). For each lag,\n        the function will append `&lt;YYYY&gt;&lt;MM&gt;` to this path to form `dataDir`.\n    param : str\n        The parameter to process: either `\"SST\"` or `\"Chla\"`. Determines which pair of scripts to invoke.\n    param_update_flag : list of bool\n        A list of length at least 4. For each `lag` in `[-3, -2, -1]`, if `param_update_flag[lag + 3]`\n        is True, the function will trigger processing for that lag index.\n\n    Returns\n    -------\n    None\n        This function does not return a value. It updates no in-memory state beyond side effects\n        of calling external scripts.\n\n    Raises\n    ------\n    OSError\n        If the working directory or the external processing scripts cannot be invoked (e.g., path not found,\n        permission denied). Any non-zero exit code from `os.system` is not captured but may indicate failure.\n    \"\"\"\n\n    for lag in list(range(-3, 0)):\n        if param_update_flag[lag + 3]:\n            myDate1 = now + timedelta(days=lag)\n            myYear = str(myDate1.year)\n            myMon = str(myDate1.month)\n            myMon = myMon.rjust(2, \"0\")\n            doy = myDate1.strftime(\"%j\").zfill(3)\n            dataDir = basedataDir + myYear + myMon\n            if param == \"SST\":\n                # myCmd = '/home/cwatch/anaconda3/bin/python /home/cwatch/newPython/modisa/makeSST1daynewMW.py  /ERDData1/modisa/data/netcdf/ /ERDData1/modisa/work/ ' + myYear + ' ' + doy\n                myCmd = (\n                    \"/home/cwatch/mambaforge/bin/python /home/cwatch/newPython/modisa/makeSST1daynewMW.py  /ERDData1/modisa/data/netcdf/ /ERDData1/modisa/work/ \"\n                    + myYear\n                    + \" \"\n                    + doy\n                )\n                os.system(myCmd)\n                # myCmd = '/home/cwatch/anaconda3/bin/python /home/cwatch/newPython/modisa/makeSST1daynewMB.py  /ERDData1/modisa/data/netcdf/ /ERDData1/modisa/work1/ ' + myYear + ' ' + doy\n                myCmd = (\n                    \"/home/cwatch/mambaforge/bin/python /home/cwatch/newPython/modisa/makeSST1daynewMB.py  /ERDData1/modisa/data/netcdf/ /ERDData1/modisa/work1/ \"\n                    + myYear\n                    + \" \"\n                    + doy\n                )\n                os.system(myCmd)\n            else:\n                # myCmd = '/home/cwatch/anaconda3/bin/python /home/cwatch/newPython/modisa/makeChla1daynewMW.py /ERDData1/modisa/data/netcdf/ /ERDData1/modisa/work/ ' + myYear + ' ' + doy\n                myCmd = (\n                    \"/home/cwatch/mambaforge/bin/python /home/cwatch/newPython/modisa/makeChla1daynewMW.py /ERDData1/modisa/data/netcdf/ /ERDData1/modisa/work/ \"\n                    + myYear\n                    + \" \"\n                    + doy\n                )\n                os.system(myCmd)\n                # myCmd = '/home/cwatch/anaconda3/bin/python /home/cwatch/newPython/modisa/makeChla1daynewMB.py  /ERDData1/modisa/data/netcdf/ /ERDData1/modisa/work1/ ' + myYear + ' ' + doy\n                myCmd = (\n                    \"/home/cwatch/mambaforge/bin/python /home/cwatch/newPython/modisa/makeChla1daynewMB.py  /ERDData1/modisa/data/netcdf/ /ERDData1/modisa/work1/ \"\n                    + myYear\n                    + \" \"\n                    + doy\n                )\n                os.system(myCmd)\n\n\ndef update_modis_composite(\n    now, basedataDir, param, param_update_flag, composite\n):\n    \"\"\"\n    Update MODIS composite products for specified date lags and parameter.\n\n    This function iterates over the three days immediately preceding `now` (lags -3, -2, -1).\n    For each lag, it computes the corresponding date, builds the data directory path from\n    `basedataDir`, and checks if any daily updates occurred in that range by summing the\n    boolean flags in `param_update_flag` up to the current lag index. If an update is needed,\n    it invokes the appropriate composite scripts via `os.system`. When `param` is `'SST'`,\n    it runs `CompMBSST.py` and `CompMWSST.py`; when `param` is `'Chla'`, it runs\n    `CompMBChla.py` and `CompMWChla.py`. Each script is passed the year, day-of-year, and\n    the `composite` interval as command-line arguments.\n\n    Parameters\n    ----------\n    now : datetime.datetime\n        The reference datetime used to calculate the target dates for each lag.\n    basedataDir : str\n        The base path where daily composite input data resides (e.g.,\n        '/ERDData1/modisa/data/modisgf/1day/').\n    param : str\n        The parameter type to process: either `'SST'` or `'Chla'`.\n    param_update_flag : list of bool\n        A list of length at least 4. For each lag in [-3, -2, -1], if the sum of\n        `param_update_flag[0]` through `param_update_flag[lag + 3]` is greater than zero,\n        the composite update scripts will be invoked for that lag.\n    composite : str\n        The composite interval identifier (e.g., `'3'`, `'5'`, `'8'`, `'14'`). Passed to\n        external scripts to control the composite period.\n\n    Returns\n    -------\n    None\n        This function does not return a value. It performs side effects by calling\n        external composite scripts.\n\n    Raises\n    ------\n    OSError\n        If any of the external composite scripts cannot be executed (e.g., the script\n        file is missing or not executable).\n    \"\"\"\n\n    for lag in list(range(-3, 0)):\n        myDate1 = now + timedelta(days=lag)\n        myYear = str(myDate1.year)\n        myMon = str(myDate1.month)\n        myMon = myMon.rjust(2, \"0\")\n        doy = myDate1.strftime(\"%j\").zfill(3)\n        dataDir = basedataDir + myYear + myMon\n        temp = param_update_flag[0 : (lag + 4)]\n        if sum(temp) &gt; 0:\n            if param == \"SST\":\n                # myCmd = '/home/cwatch/anaconda3/bin/python /home/cwatch/newPython/modisa/CompMBSST.py /ERDData1/modisa/data/modisgf/1day/ /ERDData1/modisa/work1/ ' + myYear + ' ' + doy + ' ' + composite\n                myCmd = (\n                    \"/home/cwatch/anaconda3/bin/python /home/cwatch/newPython/modisa/CompMBSST.py /ERDData1/modisa/data/modisgf/1day/ /ERDData1/modisa/work1/ \"\n                    + myYear\n                    + \" \"\n                    + doy\n                    + \" \"\n                    + composite\n                )\n                os.system(myCmd)\n                # myCmd = '/home/cwatch/anaconda3/bin/python /home/cwatch/newPython/modisa/CompMWSST.py /ERDData1/modisa/data/modiswc/1day/ /ERDData1/modisa/work/ ' + myYear + ' ' + doy + ' ' + composite\n                myCmd = (\n                    \"/home/cwatch/anaconda3/bin/python /home/cwatch/newPython/modisa/CompMWSST.py /ERDData1/modisa/data/modiswc/1day/ /ERDData1/modisa/work/ \"\n                    + myYear\n                    + \" \"\n                    + doy\n                    + \" \"\n                    + composite\n                )\n                os.system(myCmd)\n            else:\n                # myCmd = '/home/cwatch/anaconda3/bin/python /home/cwatch/newPython/modisa/CompMBChla.py /ERDData1/modisa/data/modisgf/1day/ /ERDData1/modisa/work1/ ' + myYear + ' ' + doy + ' ' + composite\n                myCmd = (\n                    \"/home/cwatch/anaconda3/bin/python /home/cwatch/newPython/modisa/CompMBChla.py /ERDData1/modisa/data/modisgf/1day/ /ERDData1/modisa/work1/ \"\n                    + myYear\n                    + \" \"\n                    + doy\n                    + \" \"\n                    + composite\n                )\n                os.system(myCmd)\n                # myCmd = '/home/cwatch/anaconda3/bin/python /home/cwatch/newPython/modisa/CompMWChla.py /ERDData1/modisa/data/modiswc/1day/ /ERDData1/modisa/work/ ' + myYear + ' ' + doy + ' ' + composite\n                myCmd = (\n                    \"/home/cwatch/anaconda3/bin/python /home/cwatch/newPython/modisa/CompMWChla.py /ERDData1/modisa/data/modiswc/1day/ /ERDData1/modisa/work/ \"\n                    + myYear\n                    + \" \"\n                    + doy\n                    + \" \"\n                    + composite\n                )\n                os.system(myCmd)\n\n\ndef url_lines(url):\n    \"\"\"\n    Download a file from a URL via `wget` and return its lines.\n\n    This function constructs a `wget` command to fetch the content at the\n    specified URL, saves it to a temporary files, retries up to 24 times\n    if the download yields an empty file, and then returns the file's lines\n    as a list of strings (with trailing newline characters stripped).\n\n    Parameters\n    ----------\n    url: str\n        The query string or full URL to pass to the NASA OceanData\n        file_search API via `wget`. This should include any necessary\n        parameters (e.g. `?dataset=...&time_min=...`).\n\n    Returns\n    -------\n    str\n        A list of lines (`str`) read from the downloaded file. Each\n        element has had its trailing newline removed.\n\n    Raises\n    ------\n    RuntimeError\n        If the file remains empty after the maximum number of retries.\n    FileNotFoundError\n        If the downloaded file cannot be opened.\n    OSError\n        If an underlying OS call fails (e.g., `wget` not found, permission\n        issues, etc.).\n    subprocess.CalledProcessError\n        If `wget` exits with a non-zero status (when called with\n        `check=True`).\n    \"\"\"\n\n    # Initialize an empty list to store the lines read from the downloaded file\n    myList = []\n\n    print(url)\n\n    # Initialize retry counter\n    no_tries = 1\n\n    # Attempt up to 24 additional downloads (total 24 tries) until the file in non-empty\n    while no_tries &lt; 25:\n\n        # Build the wget command to fetch data from the NASA OceanData API,\n        # saving output to a temporary file\n        wgetCmd = \"/usr/bin/wget -O /home/cwatch/newPython/modisa/fileNames.txt --no-check-certificate 'https://oceandata.sci.gsfc.nasa.gov/api/file_search?\" + url + \"'\"\n        # Print the command for debugging purposes\n        print(wgetCmd)\n\n        # Execute the command in a subshell; capture the return code\n        returnCode = subprocess.call(wgetCmd, shell = True)\n        print('return code: ' + str(returnCode))\n        \n        # Check size of the downloaded file\n        file_size = os.path.getsize('/home/cwatch/newPython/modisa/fileNames.txt')\n        print('search file size: ' + str(file_size))\n\n        # If the file has content, break out of the retry loop\n        if (file_size &gt; 0):\n            break\n        else:\n            # Otherwise, increment retry counter and wait before trying\n            no_tries = no_tries + 1\n            time.sleep(5)\n\n    # Open the downloaded file for reading       \n    f = open('/home/cwatch/newPython/modisa/fileNames.txt', 'r')\n\n    # iterate over each line in the file\n    for line in f:\n        # Strip trailing newline/carriage-return and append to our list\n        myList.append(line.rstrip())\n\n    # Close the file to free up resources\n    f.close()\n\n    # Print the first two entries for a quick preview\n    print('myList: ' + str(myList[0:2]))\n\n    # Return the full list of lines\n    return myList\n\n\ndef url_lines1(url):\n    \"\"\"\n    Fetch a list of file names from the NASA OceanData API.\n\n    This function constructs a full API request URL by appending the\n    provided query string to the OceanData file_search endpoint, then\n    attempts up to 24 retries (with a 5-second delay) to fetch the\n    response. Once a non-empty response is received, it decodes each\n    line as UTF-8, strips trailing whitespace, and returns the lines\n    as a list of strings.\n\n    Parameters\n    ----------\n    url: str\n        A query string containing one or more URL-encoded parameters\n        (e.g. `\"dataset=MODISA_L2&time_min=2025-06-01&time_max=2025-06-10\"`).\n\n    Returns\n    -------\n    str\n        A list of decoded, stripped lines from the API response.\n\n    Raises\n    ------\n    urllib.error.URLError\n        If there is a network problem or the URL cannot be reached.\n    urllib.error.HTTPError\n        If the server returns an HTTP error status (e.g., 404 or 500).\n    RuntimeError\n        If no data is returned after 24 retries.\n    \"\"\"\n\n    # Initialize empty list to collect decoded lines from the response\n    myList = []\n\n    print(url)\n\n    # Construct the full API endpoint URL by prefixing the base endpoint\n    url_new = 'https://oceandata.sci.gsfc.nasa.gov/api/file_search?' +  url\n    print(url_new)\n\n    # Initialize a retry counter; we will attempt up to 24 additional tries\n    no_tries = 1\n\n    # Loop until we either get data or exceed the retry limit\n    while no_tries &lt; 25:\n        # Open the URL and read all lines from the HTTP response\n        with urllib.request.urlopen(url_new) as response:\n            flist = response.readlines()\n\n        # Log how many lines were returned in the attempt\n        print('filelist length: ' + str(len(flist)))\n\n        # If we recieved at least one line, break out of the retry loop\n        if (len(flist) &gt; 0):\n            break\n        else:\n            # Otherwise, increment retry counter and pause before retrying\n            no_tries = no_tries + 1\n            time.sleep(5)\n\n    # Iterate over each raw byte line in the response list\n    for line in flist:\n        # Decode bytes to a UTF-8 string\n        junk = line.decode('UTF-8')\n        # Strip any trailing whitespace or newline characters and append to our list\n        myList.append(junk.rstrip())\n    \n    # Return the list of cleaned, decoded lines\n    return myList",
    "crumbs": [
      "Utility Library"
    ]
  },
  {
    "objectID": "mw_datasets.html",
    "href": "mw_datasets.html",
    "title": "MW Datasets",
    "section": "",
    "text": "Here you’ll find the MW (West Coast) processing scripts for both chlorophyll-a and SST.\nThey ingest raw Level-2 swath files, apply regional spatial and quality filters to isolate the West Coast, and grid valid observations into daily NetCDF products. Those daily files are then averaged over rolling multi-day windows—with composites automatically rebuilt whenever new data arrive.\nBelow are the individual scripts that form the MW workflow. Use the expandable code blocks to explore each stage of the pipeline.",
    "crumbs": [
      "MODISA Scripts",
      "MW Datasets"
    ]
  },
  {
    "objectID": "mw_datasets.html#chlorophyll",
    "href": "mw_datasets.html#chlorophyll",
    "title": "MW Datasets",
    "section": "Chlorophyll",
    "text": "Chlorophyll\n\nmakeChla1daynewMW\nmakeChla1daynewMW.py makes a 1-day compiste by reading Level-2 ocean-color swath files for a given day, extracts and filters “Day” pixels for chlorophyll-a, Kd490, PAR(0) and fluorescence line height (cflh) within longitude 205°–255° and latitude 22°–51°, then concatenates their longitude, latitude and value arrays. It uses PyGMT to interpolate each dataset onto a 0.0125° × 0.0125° regular grid, converts the results to CF-compliant NetCDFs, and uploads them to a directory on a remote server. Find the 1-day product on ERDDAP here.\nThe following chart summarizes the script’s workflow:\n\n\n\n\n\n%%{init: {\"flowchart\":{\"htmlLabels\":true}}}%%\nflowchart LR\n  I(\"&lt;b&gt;Inputs&lt;/b&gt;&lt;br/&gt;• Raw L2 Chla NetCDF swaths (dataDir)&lt;br/&gt;• workDir&lt;br/&gt;• Year & DOY\")\n    --&gt; P(\"&lt;b&gt;Processing&lt;/b&gt;&lt;br/&gt;• Compute calendar date & directories&lt;br/&gt;• Load static land mask&lt;br/&gt;• Discover swaths for Day N (HOD&gt;10) & N+1 (HOD≤10)&lt;br/&gt;• Stage & filter swaths (Day-only, region overlap)&lt;br/&gt;• Extract, reshape & filter (Chla)&lt;br/&gt;• Interpolate onto 0.0125° grid&lt;br/&gt;• Apply land mask\")\n    --&gt; O(\"&lt;b&gt;Output&lt;/b&gt;&lt;br/&gt;• CF-compliant 1-day composite NetCDF&lt;br/&gt;• Deployed to /MW/chla/1day/\")\n\n\n\n\n\n\n\n\nView makeChla1daynewMW.py\n\"\"\"\nOverview\n--------\nGenerate one-day chlorophyll-a (Chla) and related ocean-color products for the MW (West Coast) region by combining MODIS Level-2 swath files and gridding them with PyGMT. This script produces CF-compliant NetCDF files for Chla, Kd490, PAR(0), and fluorescence line height (cflh), then copies them into the appropriate server directories.\n\nUsage\n-----\n::\n \n     python makeChla1daynewMW.py &lt;dataDir&gt; &lt;workDir&gt; &lt;year&gt; &lt;doy&gt;\n\nWhere:\n\n- ``dataDir``\n\n  Root folder containing raw Level-2 ocean-color NetCDF swath files, organized as ``&lt;dataDirBase&gt;/&lt;YYYY&gt;&lt;MM&gt;/``. Each swath must match: ``AQUA_MODIS.&lt;YYYY&gt;&lt;MM&gt;&lt;DD&gt;*L2.OC.NRT.nc``.\n\n- ``workDir``\n\n  Temporary working directory for staging swath copies and intermediate files.\n\n- ``year``\n\n  Four-digit year string (e.g., ``\"2025\"``).\n\n- ``doy``\n\n  Three-digit day-of-year string (zero-padded, e.g., ``\"082\"`` for March 23).\n\nDescription\n-----------\n1. **Parse command-line arguments**\n\n     - Read ``dataDir``, ``workDir``, ``year``, and ``doy`` from ``sys.argv``.\n\n     - Print ``year`` and ``doy`` for logging.\n\n2. **Convert ``year`` + ``doy`` to calendar date**\n\n     - Compute a ``datetime`` object for the specified day of year.\n\n     - Zero-pad month and day to form ``MM`` and ``DD``.\n\n     - Construct ``datadir = dataDir + year + MM + \"/\"``, which must contain all raw L2 OC swath files for that date.\n\n3. **Load static land mask**\n\n     - Open GRD mask file at: ``/u00/ref/landmasks/LM_205_255_0.0125_22_51_0.0125_gridline.grd``\n\n     - Read the 2D mask array ``my_mask`` (1 = ocean, other = land).\n\n     - Close the mask dataset.\n\n4. **List all swath granules for the given date**\n\n     - Change into ``datadir``.\n\n     - Build a glob pattern: ``\"AQUA_MODIS.&lt;year&gt;&lt;MM&gt;&lt;DD&gt;*.L2.OC.NRT.nc\"``.\n\n     - Retrieve and sort all matching filenames.\n\n5. **Prepare working directory**\n\n     - Change into `workDir`.\n\n     - Remove any stale files matching:\n\n         - ``AQUA_MODIS.*L2.OC*``\n\n         - ``MW20*``\n\n6. **Initialize data accumulators**\n\n     - ``filesUsed``: comma-separated string for provenance of processed swaths.\n\n     - ``temp_data_Chla``, ``temp_data_k490``, ``temp_data_par0``, ``temp_data_flh``: accumulate (lon, lat, value) rows for each parameter.\n\n7. **Loop over each OC swath granule**\n\n     For each ``fName`` in the sorted list:\n\n     a. **Copy swath to work directory & open NetCDF**\n\n         - Copy from ``datadir`` to ``workDir``.\n\n         - Attempt ``Dataset(fileName, 'r')``; skip if IOError.\n\n     b. **Extract navigation data**\n\n         - Read ``latitude`` and ``longitude`` from ``rootgrp.groups['navigation_data']``.\n\n         - Convert negative longitudes (&lt; 0) to 0-360°.\n\n         - Compute ``dataLonMin``, ``dataLonMax``, ``dataLatMin``, ``dataLatMax`` for geographic filtering.\n\n     c. **Determine if swath overlaps the MW region**\n\n         - Region bounds: ``lon ∈ [205, 255]`` and ``lat ∈ [22, 51]``.\n\n         - ``goodLon = True`` if any longitudes fall within [205, 255].\n\n         - ``goodLat = True`` if any latitudes fall within [22, 51].\n\n         - Proceed only if ``goodLon and goodLat``.\n\n     d. **Record filename for provenance**\n\n         - Append ``fileName`` to ``filesUsed`` (comma-separated).\n\n     e. **Reshape navigation arrays**\n\n         - Call ``myReshape(latitude)`` → column vector (Nx1).\n\n         - Call ``myReshape(longitude)``.\n\n     f. **Extract and filter each parameter**\n\n      For each variable in ``geophysical_data``:\n\n        1. **Chlorophyll-a (chlor_a)**\n\n             - Read and reshape: ``chlor_a = myReshape(rootgrp.groups['geophysical_data'].variables['chlor_a'][:, :])``\n\n             - Stack: ``dataOut = np.hstack((longitude, latitude, chlor_a))``\n\n             - Filter:\n\n                 - ``dataOut[:, 0] &gt; -400``\n\n                 - ``lonmin ≤ dataOut[:, 0] ≤ lonmax``\n\n                 - ``latmin ≤ dataOut[:, 1] ≤ latmax``\n\n                 - ``chlor_a &gt; 0``\n\n             - Accumulate into ``temp_data_Chla``.\n\n         2. **Kd490 (Kd_490)**\n\n             - Read, optionally scale, reshape, and stack with (lon, lat).\n\n             - Filter:\n\n                 - valid lon/lat as above,\n\n                 - ``0 &lt; Kd490 &lt; 6.3``\n\n             - Accumulate into ``temp_data_k490``.\n\n         3. **PAR(0) (par)**\n    \n             - Read, optionally scale, reshape, and stack.\n\n             - Filter:\n\n                 - valid lon/lat,\n\n                 - ``PAR &gt; 0``\n\n             - Accumulate into ``temp_data_par0``.\n\n         4. **Fluorescence Line Height (cflh)**\n\n             - Read, optionally scale, reshape, and stack.\n\n             - Filter:\n\n                 - valid lon/lat,\n\n                 - ``cflh &gt; 0``\n\n            - Accumulate into ``temp_data_flh``.\n\n     g. **Cleanup**\n\n         - ``rootgrp.close()``\n\n         - ``os.remove(fileName)`` from ``workDir``.\n\n8. **Grid each parameter's point cloud with PyGMT**\n\n     For each of ``temp_data_Chla``, ``temp_data_k490``, ``temp_data_par0``, ``temp_data_flh``:\n\n     - Set:\n         - ``region = \"205/255/22/51\"``\n\n         - ``spacing = \"0.0125/0.0125\"``\n\n         - ``search_radius = \"2k\"``\n\n         - ``sectors = \"1\"``\n\n     - Call:\n\n     ::\n\n         temp_data1 = pygmt.nearneighbor(\n             data=temp_data_&lt;param&gt;,\n             region=region,\n             spacing=spacing,\n             search_radius=search_radius,\n             sectors=\"1\"\n         )\n\n     - This returns a PyGMT grid (xarray.DataArray) covering the specified region.\n\n9. **Convert grid(s) to CF-compliant NetCDF & send to server**\n\n     - Call:\n\n     ::\n\n         ncFile = grd2netcdf1(temp_data1, fileOut, filesUsed, my_mask, \"MW\")\n\n     - Masks land, computes coverage, builds CF NetCDF via CDL, and returns the NetCDF path.\n\n     - Call:\n\n     ::\n\n         send_to_servers(ncFile, \"/MW/&lt;param&gt;/\", \"1\")\n\n         - E.g. `send_to_servers(ncFile, \"/MW/chla/\", \"1\")`\n\n     - Delete local NetCDF:\n\n     ::\n\n         os.remove(ncFile)\n\nDependencies\n------------\n- **Python 3.x**\n\n- **Standard library:** ``sys``, ``os``, ``glob``, ``shutil``, ``re``, ``itertools.chain``, ``datetime``, ``timedelta``\n\n- **Third-party packages:** ``netCDF4.Dataset``, ``numpy``, ``pygmt``\n\n- **Custom roylib functions:**\n\n  - ``myReshape(array)``\n  \n  - ``grd2netcdf1(grd, outName, filesUsed, mask, fType)``\n  \n  - ``send_to_servers(ncFile, destDir, interval)``\n  \n  - ``isleap(year)``\n  \n  - ``makeNetcdf(mean, nobs, interval, outFile, filesUsed, workDir)``\n  \n  - ``meanVar(mean, num, obs)``\n\nLand Mask\n---------\n- A static GRD file is required at:\n\n ``/u00/ref/landmasks/LM_205_255_0.0125_22_51_0.0125_gridline.grd``\n \n- This mask is applied to each gridded output to set land pixels to NaN.\n\nDirectory Structure\n-------------------\n- **Input swaths directory** (datadir):\n\n  Directory for raw Level-2 OC NetCDF swath files for the given date, organized as ``&lt;dataDirBase&gt;/&lt;YYYY&gt;&lt;MM&gt;/``. Each file must be named: ``AQUA_MODIS.&lt;YYYY&gt;&lt;MM&gt;&lt;DD&gt;*L2.OC.NRT.nc``\n\n- **Working directory** (workDir):\n\n  Temporary staging area where swaths are copied for processing and then deleted.\n\n- **Output grid** (fileOut):\n\n  GMT “.grd” file created by PyGMT nearneighbor, named: ``MW&lt;YYYY&gt;&lt;DDD&gt;_&lt;YYYY&gt;&lt;DDD&gt;_&lt;param&gt;.grd`` (e.g. ``MW2025082_2025082_chla.grd``).\n\n- **Final NetCDF** (returned by ``grd2netcdf1``):\n\n  CF-compliant NetCDF file named: ``MW&lt;YYYY&gt;&lt;DDD&gt;_&lt;YYYY&gt;&lt;DDD&gt;_&lt;param&gt;.nc`` Copied into the MW 1-day product directories, for example: ``/path/to/modis_data/modiswc/chla/1day/MW2025082_2025082_chla.nc``\n\nUsage Example\n-------------\nAssume your raw OC swaths live in:\n\n::\n\n     /Users/you/modis_data/netcdf/202503/\n\nand your working directory is:\n\n::\n \n     /Users/you/modis_work/\n\nThen to produce March 23, 2025 products:\n\n::\n\n    python makeChla1daynewMW.py /Users/you/modis_data/netcdf/ \\\n                                /Users/you/modis_work/ \\\n                                2025 082\n\nThis will:\n\n  - Copy all swaths matching ``AQUA_MODIS.20250323*.L2.OC.NRT.nc`` into ``/Users/you/modis_work/``.\n\n  - Build combined point clouds for Chla, Kd490, PAR(0), and cflh.\n\n  - Create PyGMT grids over lon 205-255°, lat 22-51° via ``nearneighbor``.\n\n  - Convert each grid to a CF-compliant NetCDF using ``grd2netcdf1``, masked by the static GRD.\n\n  - Copy the final NetCDFs to the appropriate MW 1-day product folders.\n\"\"\"\nfrom __future__ import print_function\nfrom builtins import str\n\nif __name__ == \"__main__\":\n    from datetime import datetime, timedelta\n    import glob\n    from itertools import chain\n    from netCDF4 import Dataset\n    import numpy as np\n    import numpy.ma as ma\n    import pygmt\n    import os\n    import re\n    import shutil\n    import sys\n\n    # Ensure 'roylib' is on the import path\n    sys.path.append('/home/cwatch/pythonLibs')\n    from roylib import *\n\n    # Geographic bounds for MW region\n    latmax = 51.\n    latmin = 22.\n    lonmax = 255.\n    lonmin = 205.\n\n    # Set data directory\n    datadirBase = sys.argv[1]\n\n    # Set work directory\n    workdir = sys.argv[2]\n\n    # Get the year and doy from the command line\n    year = sys.argv[3]\n    doy = sys.argv[4]\n    print(year)\n    print(doy)\n\n    # Convert year/doy to a calendar date and zero-pad month/day\n    myDate = datetime(int(year), 1, 1) + timedelta(int(doy) - 1)\n    myMon = str(myDate.month)\n    myMon = myMon.rjust(2, '0')\n    myDay = str(myDate.day).rjust(2, '0')\n\n    # Construct the directory path where raw OC swaths are stored for this date\n    datadir = datadirBase + year + myMon + '/'\n\n    # Load static land mask from GRD\n    mask_root = Dataset('/u00/ref/landmasks/LM_205_255_0.0125_22_51_0.0125_gridline.grd')\n    my_mask = mask_root.variables['z'][:, :]\n    mask_root.close()\n\n    # Now move to the data directory\n    os.chdir(datadir)\n\n    # Set up the string for the file search in the data directory\n    myString = 'AQUA_MODIS.' + year + myMon + myDay  + '*.L2.OC.NRT.nc'\n    print(myString)\n\n    # Get list of files in the data directory that match with full path\n    fileList = glob.glob(myString)\n    # print(fileList)\n    fileList.sort()\n\n    # Now move to the work directory and clear old files\n    os.chdir(workdir)\n    os.system('rm -f AQUA_MODIS.*L2.OC*')\n    os.system('rm -f MW20*')\n\n    # Do the whole thing for chla\n    outFileChla = 'modiswcChlatemp'\n    outFilek490 = 'modiswck490temp'\n    outFilepar0 = 'modiswcpar0temp'\n    outFilecflh = 'modiswccflhtemp'\n    \n    # Initialize variables to accumulate data and track provenance\n    filesUsed = \"\"\n    temp_data_Chla = None\n    temp_data_k490 = None\n    temp_data_par0 = None\n    temp_data_flh = None\n\n    # Loop over each OC swath granule for the given day\n    for fName in fileList:\n        fileName = fName\n        # find the datatime group in the filename\n        # check on what to search for\n        #datetime = re.search('AQUA_MODIS(.+?).L2.NRT.OC', fileName)\n        # will want elements 8,9 of datatime.group(1)\n        print(fileName)\n\n        # Copy the swath from datadir into the workdir\n        shutil.copyfile(datadir + fName, workdir + fName)\n\n        # Try to open the NetCDF; skip if the file is unreadable\n        try:\n            rootgrp = Dataset(fileName, 'r')\n        except IOError:\n            print(\"bad file \" + fileName)\n            continue\n\n        # Extract navigation-group data\n        navDataGroup = rootgrp.groups['navigation_data']\n        latitude = navDataGroup.variables['latitude'][:, :]\n        longitude = navDataGroup.variables['longitude'][:, :]\n\n        # Convert any negative longitudes to the 0-360° domain\n        longitude[longitude &lt; 0] = longitude[longitude &lt; 0] + 360\n\n        # Compute swath extents (min/max) for geographic filtering\n        dataLonMin = np.nanmin(longitude[longitude &gt;= 0])\n        dataLonMax = np.nanmax(longitude[longitude &lt;= 360])\n        dataLatMin = np.nanmin(latitude[latitude &gt;= -90])\n        dataLatMax = np.nanmax(latitude[latitude &lt;= 90])\n\n        # Determine if swath overlaps our global MW region\n        goodLon1 = (dataLonMin &lt; lonmin) and (dataLonMax &gt;= lonmin)\n        goodLon2 = (dataLonMin &gt;= lonmin) and (dataLonMin &lt;= lonmax)\n        goodLon = goodLon1 or goodLon2\n\n        goodLat1 = (dataLatMin &lt; latmin) and (dataLatMax &gt;= latmin)\n        goodLat2 = (dataLatMin &gt;= latmin) and (dataLatMin &lt;= latmax)\n        goodLat = goodLat1 or goodLat2\n\n        # Check if swath is daytime (only keep \"Day\" pixels)\n        dayNightTest = (rootgrp.day_night_flag == 'Day')\n\n        # Only proceed if geography and day-night tests pass\n        if (goodLon and goodLat):\n            # Add filename to provenance list\n            if (len(filesUsed) == 0):\n                filesUsed = fileName\n            else:\n                filesUsed = filesUsed + ', ' + fileName\n\n            # Reshape latitude & longitude arrays into column vectors\n            latitude = myReshape(latitude)\n            longitude = myReshape(longitude)\n\n            # Access geophysical data group\n            geoDataGroup = rootgrp.groups['geophysical_data']\n\n            # Extract chlor_a\n            chlor_a = geoDataGroup.variables['chlor_a'][:, :]\n            chlor_a = myReshape(chlor_a)\n\n            # Stack (lon, lat, chlor_a) into a single 2D array with shape (N, 3)\n            dataOut = np.hstack((longitude, latitude, chlor_a))\n\n            # Filter out-of-range lon/lat and non-positive chlor_a\n            dataOut = dataOut[dataOut[:, 0] &gt; -400]\n            dataOut = dataOut[dataOut[:, 0] &gt;= lonmin]\n            dataOut = dataOut[dataOut[:, 0] &lt;= lonmax]\n            dataOut = dataOut[dataOut[:, 1] &gt;= latmin]\n            dataOut = dataOut[dataOut[:, 1] &lt;= latmax]\n            dataOut = dataOut[dataOut[:, 2] &gt; 0]\n\n            # Accumulate into temp_data_Chla\n            if (dataOut.shape[0] &gt; 0):\n                if (temp_data_Chla is None):\n                    temp_data_Chla = dataOut\n                else:\n                    temp_data_Chla = np.concatenate((temp_data_Chla, dataOut), axis=0)\n\n            # Extract Kd490\n            k490 = geoDataGroup.variables['Kd_490'][:, :]\n            # k490 = k490 * 2.0E-4\n            k490 = myReshape(k490)\n\n            dataOut = np.hstack((longitude, latitude, k490))\n            dataOut = dataOut[dataOut[:, 0] &gt; -400]\n            dataOut = dataOut[dataOut[:, 0] &gt;= lonmin]\n            dataOut = dataOut[dataOut[:, 0] &lt;= lonmax]\n            dataOut = dataOut[dataOut[:, 1] &gt;= latmin]\n            dataOut = dataOut[dataOut[:, 1] &lt;= latmax]\n            dataOut = dataOut[dataOut[:, 2] &lt; 6.3]\n            dataOut = dataOut[dataOut[:, 2] &gt; 0]\n\n            if (dataOut.shape[0] &gt; 0):\n                if (temp_data_k490 is None):\n                    temp_data_k490 = dataOut\n                else:\n                    temp_data_k490 = np.concatenate((temp_data_k490, dataOut), axis=0)\n\n            # Extract PAR0\n            par0 = geoDataGroup.variables['par'][:, :]\n            # par0 = (0.002 * par0) + 65.5\n            par0 = myReshape(par0)\n\n            dataOut = np.hstack((longitude, latitude, par0))\n            dataOut = dataOut[dataOut[:, 0] &gt; -400]\n            dataOut = dataOut[dataOut[:, 0] &gt;= lonmin]\n            dataOut = dataOut[dataOut[:, 0] &lt;= lonmax]\n            dataOut = dataOut[dataOut[:, 1] &gt;= latmin]\n            dataOut = dataOut[dataOut[:, 1] &lt;= latmax]\n            dataOut = dataOut[dataOut[:, 2] &gt; 0]\n\n            if (dataOut.shape[0] &gt; 0):\n                if (temp_data_par0 is None):\n                    temp_data_par0 = dataOut\n                else:\n                    temp_data_par0 = np.concatenate((temp_data_par0, dataOut), axis=0)\n\n            # Extract Fluorescence Line Height (cflh)\n            cflh = geoDataGroup.variables['nflh'][:, :]\n            # cflh = 1.0E-5 * cflh\n            cflh = myReshape(cflh)\n\n            dataOut = np.hstack((longitude, latitude, cflh))\n            dataOut = dataOut[dataOut[:, 2] &gt; 0]\n            dataOut = dataOut[dataOut[:, 0] &gt; -400]\n            dataOut = dataOut[dataOut[:, 0] &gt;= lonmin]\n            dataOut = dataOut[dataOut[:, 0] &lt;= lonmax]\n            dataOut = dataOut[dataOut[:, 1] &gt;= latmin]\n            dataOut = dataOut[dataOut[:, 1] &lt;= latmax]\n            dataOut = dataOut[dataOut[:, 2] &gt; 0]\n\n            if (dataOut.shape[0] &gt; 0):\n                if (temp_data_flh is None):\n                    temp_data_flh = dataOut\n                else:\n                    temp_data_flh = np.concatenate((temp_data_flh, dataOut), axis=0)\n\n        # Close the NetCDF and remove the swath file from workdir\n        rootgrp.close()\n        os.remove(fileName)\n\n    # Grid and write each parameter's point cloud via PyGMT,\n    #  then convert to NetCDF and send to server\n\n    # chlor_a composite\n    fileOut = 'MW' + year + doy + '_' + year + doy + '_chla.grd'\n    range = '205/255/22/51'\n    increment = '0.0125/0.0125'\n    smooth = '2k'\n\n    # Create a gridded dataset from the chlor_a point cloud\n    temp_data1 = pygmt.nearneighbor(\n        data=temp_data_Chla,\n        region=range,\n        spacing=increment,\n        search_radius=smooth,\n        sectors='1'\n    )\n\n    # Convert the GMT grid to CF-compliant NetCDF, masking land\n    ncFile = grd2netcdf1(temp_data1, fileOut, filesUsed, my_mask, 'MW')\n\n    #myCmd = \"mv \" + ncFile + \" /home/cwatch/pygmt_test/outfiles\"\n    #os.system(myCmd)\n\n    # Copy to the MW server folder for chla (1-day product)\n    send_to_servers(ncFile, '/MW/chla/' , '1')\n    os.remove(ncFile)\n\n    # Kd490 composite\n    #fileIn = outFilek490\n    fileOut = 'MW' + year + doy + '_' + year + doy + '_k490.grd'\n    range = '205/255/22/51'\n    increment = '0.0125/0.0125'\n    smooth = '2k'\n\n    # Create a gridded dataset from the Kd490 point cloud\n    temp_data1 = pygmt.nearneighbor(\n        data=temp_data_k490,\n        region=range,\n        spacing=increment,\n        search_radius=smooth,\n        sectors='1'\n    )\n\n    # Convert the GMT grid to CF-compliant NetCDF, masking land\n    ncFile = grd2netcdf1(temp_data1, fileOut, filesUsed, my_mask, 'MW')\n\n    #myCmd = \"mv \" + ncFile + \" /home/cwatch/pygmt_test/outfiles\"\n    #os.system(myCmd)\n\n    # Copy to the MW server folder for k490 (1-day product)\n    send_to_servers(ncFile, '/MW/k490/' , '1')\n    os.remove(ncFile)\n\n    # PAR(0) composite\n    fileOut = 'MW' + year + doy + '_' + year + doy + '_par0.grd'\n    range = '205/255/22/51'\n    increment = '0.0125/0.0125'\n    smooth = '2k'\n\n    # Create a gridded dataset from the PAR(0) point cloud\n    temp_data1 = pygmt.nearneighbor(\n        data=temp_data_par0,\n        region=range,\n        spacing=increment,\n        search_radius=smooth,\n        sectors='1'\n    )\n\n    # Convert the GMT grid to CF-compliant NetCDF, masking land\n    ncFile = grd2netcdf1(temp_data1, fileOut, filesUsed, my_mask, 'MW')\n\n    #myCmd = \"mv \" + ncFile + \" /home/cwatch/pygmt_test/outfiles\"\n    #os.system(myCmd)\n\n    # Copy to the MW server folder for PAR(0) (1-day product)\n    send_to_servers(ncFile, '/MW/k490/' , '1')  # Note: likely intended for '/MW/par0/'\n    os.remove(ncFile)\n\n    # cflh composite\n    #fileIn = outFilecflh\n    fileOut = 'MW' + year + doy + '_' + year + doy + '_cflh.grd'\n    range = '205/255/22/51'\n    increment = '0.0125/0.0125'\n    smooth = '2k'\n\n    # Create a gridded dataset from the cflh point cloud\n    temp_data1 = pygmt.nearneighbor(\n        data=temp_data_flh,\n        region=range,\n        spacing=increment,\n        search_radius=smooth,\n        sectors='1'\n    )\n\n    # Convert the GMT grid to CF-compliant NetCDF, masking land\n    ncFile = grd2netcdf1(temp_data1, fileOut, filesUsed, my_mask, 'MW')\n\n    #myCmd = \"mv \" + ncFile + \" /home/cwatch/pygmt_test/outfiles\"\n    #os.system(myCmd)\n\n    # Copy to the MW server folder for cflh (1-day product)\n    send_to_servers(ncFile, '/MW/k490/' , '1')  # Note: likely intended for '/MW/cflh/'\n    os.remove(ncFile)\n\n\n\n\nCompMWChla\nCompMWChla.py reads 1-day composite MW NetCDF files for chlorophyll-a, Kd490, PAR(0), CFLH over the specified interval and accumulates running sums and counts on a 0.0125° × 0.0125° regular grid spanning longitude 205°–255° and latitude 22°–51°. It then masks out any grid cells with zero observations, writes each multi-day (3-, 8-, or 14-day) composite as a CF-compliant NetCDF, and uploads them to their respective directories on a remote server. Find the multi-day products on ERDDAP here.\nThe following chart summarizes the script’s workflow:\n\n\n\n\n\n%%{init: {\"flowchart\":{\"htmlLabels\":true}}}%%\nflowchart LR\n  I(\"&lt;b&gt;Inputs&lt;/b&gt;&lt;br/&gt;• 1-day composite NetCDFs (dataDir)&lt;br/&gt;• workDir&lt;br/&gt;• endYear & endDoy&lt;br/&gt;• interval (days)\")\n    --&gt; P(\"&lt;b&gt;Processing&lt;/b&gt;&lt;br/&gt;• Parse args & compute start/end DOY&lt;br/&gt;• Clear workDir & initialize mean/num arrays&lt;br/&gt;• Gather files over date range&lt;br/&gt;• Loop: open each file & update mean/num&lt;br/&gt;• Mask zero-observation pixels\")\n    --&gt; O(\"&lt;b&gt;Output&lt;/b&gt;&lt;br/&gt;• CF-compliant multi-day (3-, 8-, 14-) composite NetCDF&lt;br/&gt;• Deployed to /MW/chla/interval day/\")\n\n\n\n\n\n\n\n\nView CompMWChla.py\n\"\"\"\nOverview\n--------\nGenerate multi-day composites for MODIS MW (West Coast) products:\nchlorophyll-a (Chla), Kd490, PAR0, and fluorescence line height (CFLH).\n\nUsage\n-----\n::\n\n    python CompMWChla.py &lt;dataDir&gt; &lt;workDir&gt; &lt;endYear&gt; &lt;endDoy&gt; &lt;interval&gt;\n\nWhere:\n\n- ``dataDir``\n\n  Directory containing daily 1-day NetCDFs named ``MW&lt;YYYY&gt;&lt;DDD&gt;_&lt;param&gt;.nc``.\n\n- ``workDir``\n\n  Temporary working directory for intermediate files.\n\n- ``endYear`` \n\n  Four-digit year of the last day in the composite (e.g., ``2025``).\n\n- ``endDoy``\n\n  Three-digit day-of-year of the last day (e.g., ``082``).\n\n- ``interval``\n\n  Composite length in days (`3`, `5`, `8`, or `14`)\n\nDescription\n-----------\n1. **Parse arguments & compute date range** \n\n  - Read ``dataDir``, ``workDir``, ``endYear``, ``endDoy``, ``interval``.\n\n  - Compute start day = end day minus (``interval``-1), zero-pad ``startDoy``.\n\n2. **Loop over each variable** (``chla``, ``k490``, ``par0``, ``cflh``):  \n\n  - Clear ``workDir``.\n\n  - Initialize ``mean`` and ``num`` arrays for sum and count.\n\n  - Gather daily files over the date range (handles year wrap).\n\n  - For each file, read 4D ``MW&lt;dtype&gt;``, squeeze to 2D, update ``mean``, ``num``.\n\n  - Mask out pixels with zero count (fill = -9999999).\n\n  - Write composite via ``makeNetcdf(...)`` and send via ``send_to_servers(...)``.\n\nDependencies\n------------\n- **Python 3.x**\n\n- **Standard library:** ``os``, ``sys``, ``glob``, ``itertools.chain``, ``datetime``\n\n- **Third-party:** ``numpy``, ``numpy.ma``, ``netCDF4``\n\n- **Custom roylib functions:**\n\n  - ``isleap(year)``\n\n  - ``meanVar(mean, num, obs)``\n\n  - ``makeNetcdf(mean, nobs, interval, outFile, filesUsed, workDir)``\n\n  - ``send_to_servers(ncFile, destDir, interval)``\n\nDirectory Structure\n-------------------\n- **Input Directory** (``dataDir``):\n\n  ``MW&lt;YYYY&gt;&lt;DDD&gt;*&lt;dtype&gt;.nc``\n\n- **Working Directory** (``workDir``):\n\n  Temporary staging for intermediate files\n\n- **Output Location** (remote):\n\n  ``/MW/&lt;dtype&gt;/`` on the server\n\nUsage Example\n-------------\n5-day composite (DOY 100-104 of 2025):\n\n::\n  \n   python CompMWChla.py /data/MW/1day /tmp/mw_work 2025 104 5\n\nThis command will:\n\n  - Read all daily files ``MW2025100*...MW2025104*`` from ``/data/MW/1day``.\n\n  - Compute the 5-day composite for Chla, Kd490, PAR0, and CFLH.\n\n  - Write the output files to ``/tmp/mw_work/`` and upload them to ``/MW/&lt;dtype&gt;/`` on the server.\n\"\"\"\nfrom __future__ import print_function\nfrom builtins import str\nfrom builtins import range\n\nif __name__ == \"__main__\":\n    from datetime import datetime, timedelta\n    import glob\n    from itertools import chain\n    from netCDF4 import Dataset\n    import numpy as np\n    import numpy.ma as ma\n    import os\n    import sys\n\n    # Ensure 'roylib' is on the import path\n    sys.path.append('/home/cwatch/pythonLibs')\n    from roylib import *\n\n    # Directory with 1-day MW NetCDFs\n    dataDir = sys.argv[1]\n\n    # Temporary working directory\n    workDir = sys.argv[2]\n\n    # Composite end year (YYYY)\n    endyearC = sys.argv[3]\n\n    # Composite end day-of-year (DDD)\n    endDoyC = sys.argv[4]\n    endDoyC = endDoyC.rjust(3, '0')\n\n    # Integer form of end day-of-year\n    endDoy = int(endDoyC)\n\n    # Composite length\n    intervalC = sys.argv[5]\n    interval = int(intervalC)\n\n    # Convert end Doy to calendar date\n    myDateEnd = datetime(int(endyearC), 1, 1) + timedelta(int(endDoyC) - 1)\n\n    # Start date = end date minus (interval-1) days\n    myDateStart = myDateEnd + timedelta(days=-(interval - 1))\n\n    # Zero-padded start Doy and year\n    startDoyC = myDateStart.strftime(\"%j\").zfill(3)\n    startDoy = int(startDoyC)\n    startYearC = str(myDateStart.year)\n\n    # Prepare output directory \n    outDir = '/ERDData1/modisa/data/modsiwc/' + endyearC + '/' + intervalC + 'day'\n\n    print(dataDir)\n    print(workDir)\n    print(endyearC)\n    print(endDoyC)\n    print(intervalC)\n\n    # List of parameters to composite\n    dtypeList = ['chla', 'k490', 'par0', 'cflh']\n\n    # Loop over each variable type\n    for dtype in dtypeList:\n        # Clear working directory\n        os.chdir(workDir)\n        os.system('rm -f *')\n\n        # Move to data directory\n        os.chdir(dataDir)\n\n        # Preallocate sum (mean) and count arrays matching grid dims\n        mean = np.zeros((2321, 4001), np.single)\n        num = np.zeros((2321, 4001), dtype=np.int32)\n\n        # If composite does not cross year boundary\n        if (endDoy &gt; startDoy):\n            doyRange = list(range(startDoy, endDoy+1))\n            fileList = []\n            # Gather matching files for each day in range\n            for doy in doyRange:\n                doyC = str(doy)\n                doyC = doyC.rjust(3, '0')\n                myString = 'MW' + endyearC + doyC + '*' + dtype + '.nc'\n                fileList.append(glob.glob(myString))\n            \n            # Flatten and sort list of lists\n            fileList=list(chain.from_iterable(fileList))\n            fileList.sort()\n\n            filesUsed = \"\"\n            print(fileList)\n            for fName in fileList:\n                # Build comma-separated provenance string\n                if (len(filesUsed) == 0):\n                    filesUsed = fName\n                else:\n                    filesUsed = filesUsed + ', ' + fName\n\n                # Read the variable from NetCDF and accumulate\n                chlaFile = Dataset(fName)\n                param = 'MW' + dtype\n                chla = chlaFile.variables[param][:, :, :, :]\n                chlaFile.close()\n                chla = np.squeeze(chla)\n\n                # Update running mean and count arrays\n                mean, num = meanVar(mean, num, chla)\n\n        else:\n            # Composite spans year boundary: first part in startYearC\n            dataDir1 = dataDir\n            dataDir1 = dataDir1.replace(endyearC, startYearC)\n            if (isleap):\n                endday = 366\n            else:\n                endday = 365\n\n            fileList = []\n            os.chdir(dataDir1)\n            # Days from startDoy to end of start year\n            doyRange = list(range(startDoy, endday + 1))\n            for doy in doyRange:\n                doyC = str(doy)\n                doyC = doyC.rjust(3, '0')\n                myString = 'MW' + startYearC + doyC + '*' + dtype + '.nc'\n                fileList.append(glob.glob(myString))\n\n            fileList = list(chain.from_iterable(fileList))\n            fileList.sort()\n            filesUsed = \"\"\n            print(fileList)\n            for fName in fileList:\n                if (len(filesUsed) == 0):\n                    filesUsed = fName\n                else:\n                    filesUsed = filesUsed + ', ' + fName\n\n                chlaFile = Dataset(fName)\n                param = 'MW' + dtype\n                chla = chlaFile.variables[param][:, :, :, :]\n                chlaFile.close()\n                chla = np.squeeze(chla)\n                mean, num = meanVar(mean, num, chla)\n\n            # Days from DOY=1 of end year to endDoy\n            os.chdir(dataDir)\n            fileList = []\n            doyRange = list(range(1, endDoy + 1))\n            for doy in doyRange:\n                doyC = str(doy)\n                doyC = doyC.rjust(3, '0')\n                myString = 'MW' + endyearC + doyC + '*' + dtype + '.nc'\n                fileList.append(glob.glob(myString))\n\n            fileList = list(chain.from_iterable(fileList))\n            fileList.sort()\n            print(fileList)\n            for fName in fileList:\n                if (len(filesUsed) == 0):\n                    filesUsed = fName\n                else:\n                    filesUsed = filesUsed + ', ' + fName\n\n                chlaFile = Dataset(fName)\n                param = 'MW' + dtype\n                chla = chlaFile.variables[param][:, :, :, :]\n                chlaFile.close()\n                chla = np.squeeze(chla)\n                mean, num = meanVar(mean, num, chla)\n\n        # Mask out any grid cells with zero observations, setting them to the fill value\n        mean = ma.array(mean, mask=(num == 0), fill_value=-9999999.)\n\n        # Switch to the working directory for output operations\n        os.chdir(workDir)\n\n        # Construct the output filename with start and end dates plus data types\n        outFile = 'MW' + startYearC + startDoyC + '_' + endyearC + endDoyC + '_' + dtype + '.nc'\n\n        # Create multi-day NetCDF file using the mean and count arrays\n        ncFile = makeNetcdf(mean, num, interval, outFile, filesUsed, workDir)\n\n        # Directory on the remote server for storing the multi-day data product\n        remote_dir = '/MW/' + dtype + '/'\n\n        # Transfer the generated NetCDF file to the remote server directory\n        send_to_servers(ncFile, remote_dir , str(interval))\n\n\n\n\nCompMWChlamday\nCompMWChlamday.py reads 1-day composite MW NetCDFs for chlorophyll-a, Kd490, PAR(0), and CFLH and accumulates them over a user-defined interval on a regular 0.0125° x 0.0125° grid spanning longitude 205°–255° and latitude 22°–51°. It computes the monthly mean by summing and counting valid observations, masks out grid cells with no data, writes CF-compliant NetCDF composites, and upload to a directory on a remote server Find the monthly product on ERDDAP here.\nThe following chart summarizes the script’s workflow:\n\n\n\n\n\n%%{init: {\"flowchart\":{\"htmlLabels\":true}}}%%\nflowchart LR\n  I(\"&lt;b&gt;Inputs&lt;/b&gt;&lt;br/&gt;• 1-day composite NetCDFs (dataDir)&lt;br/&gt;• workDir&lt;br/&gt;• endYear & endDoy&lt;br/&gt;• startDoy\")\n    --&gt; P(\"&lt;b&gt;Processing&lt;/b&gt;&lt;br/&gt;• Parse args & compute interval&lt;br/&gt;• Compute start/end dates&lt;br/&gt;• Clear workDir & initialize mean/num arrays&lt;br/&gt;• Gather files spanning startDoy…endDoy&lt;br/&gt;• Loop: open each file & update mean/num&lt;br/&gt;• Mask zero‐observation pixels\")\n    --&gt; O(\"&lt;b&gt;Output&lt;/b&gt;&lt;br/&gt;• CF-compliant monthly composite NetCDF&lt;br/&gt;• Deployed to /MW/chla/mday/\")\n\n\n\n\n\n\n\n\nView CompMWChlamday.py\n\"\"\"\nOverview\n--------\nGenerate monthly composites of chlorophyll-a, Kd_490, PAR0, and CFLH\nfor the MODIS MW (West Coast) region by accumulating 1-day NetCDF products.\n\nUsage\n-----\n::\n \n     python CompMWChlamday.py &lt;dataDir&gt; &lt;workDir&gt; &lt;endYear&gt; &lt;endDoy&gt; &lt;startDoy&gt;\n\nDescription\n-----------\n1. **Parse arguments & compute interval** \n\n  - Read ``dataDir``, ``workDir``, ``endYear``, ``endDoy``, and ``startDoy``.  \n  \n  - Compute the number of days ``interval = endDoy - startDoy + 1``.\n\n2. **Compute calendar dates**\n\n  - Convert ``startDoy``/``endDoy`` to ``datetime`` for logging/output paths.\n\n3. **Loop over each parameter**\n\n  For each in ``['chla','k490','par0','cflh']``:\n\n     - **Initialize sum & count arrays** of shape 2321x4001.\n\n     - **Gather all matching 1-day NetCDF files** between ``startDoy``…``endDoy``, handling both same-year and wrap-around cases.\n\n     - **For each file**:\n\n         - Open with ``netCDF4.Dataset``, read 4D variable ``MW&lt;param&gt;``, squeeze to 2D.\n\n         - Update running totals (``mean``) and counts (``num``) via ``meanVar()``.\n\n     - **Mask out** grid cells with zero observations (``num==0``), fill with -9999999.0.\n\n     - **Write** composite via ``makeNetcdfmDay()``.\n\n     - **Send** result to ``/MW/&lt;param&gt;/mday/`` on the server.\n\nDependencies\n------------\n- **Python 3.x**\n\n- **Standard library:** ``os``, ``sys``, ``glob``, ``itertools.chain``, ``datetime``, ``timedelta``\n\n- **Third-party:** ``numpy``, ``numpy.ma``, ``netCDF4``\n\n- **Custom roylib functions:**\n\n  - ``isleap(year)``\n\n  - ``meanVar(sum_array, count_array, data_slice)``\n\n  - ``makeNetcdfmDay(mean, num, interval, outFile, filesUsed, workDir)``\n\n  - ``send_to_servers(ncFile, remote_dir, 'm')``\n\nDirectory Structure\n-------------------\n- **Input Directory (dataDir):**\n\n  Contains 1-day NetCDFs named ``MW&lt;YYYY&gt;&lt;DDD&gt;_&lt;param&gt;.nc``.\n\n- **Working Directory (workDir):**\n\n  Scratch space; cleared each loop.\n\n- **Output location (remote):**\n\n  ``/MW/chla/mday/``, ``/MW/k490/mday/``, etc.\n\nUsage Example\n-------------\nGenerate a composite for the month of January 2025 (DOY 001-031):\n\n::\n \n     python CompMWChlamday.py /data/mw/1day/ /tmp/mwwork/ 2025 031 001\n\nThis command will:\n\n  - Read daily files ``MB2025001..Mb2025031``, computes the January composite.\n\n  - Writes MB2025001_2025031_chla.nc in ``/tmp/mw_work/``.\n\n  - Uploads it to ``/MB/chla/`` on the server.\n\n\"\"\"\nfrom __future__ import print_function\nfrom builtins import str\nfrom builtins import range\n\nif __name__ == \"__main__\":\n    from datetime import datetime, timedelta\n    import glob\n    from itertools import chain\n    from netCDF4 import Dataset\n    import numpy as np\n    import numpy.ma as ma\n    import os\n    import sys\n\n    # Ensure 'roylib' is on the import path\n    sys.path.append('/home/cwatch/pythonLibs')\n    from roylib import *\n\n    # Directory with 1-day MW NetCDFs\n    dataDir = sys.argv[1]\n\n    # Temporary working directory\n    workDir = sys.argv[2]\n\n    # Composite end year\n    endyearC = sys.argv[3]\n    endyear = int(endyearC)\n\n    # Start year \n    startYearC = endyearC\n    startyear = int(startYearC)\n\n    # Composite end day-of-year\n    endDoyC = sys.argv[4]\n    endDoyC = endDoyC.rjust(3, '0')\n    endDoy = int(endDoyC)\n\n    # Composite start day-of-year\n    startDoyC = sys.argv[5]\n    startDoyC = startDoyC.rjust(3, '0')\n    startDoy = int(startDoyC)\n\n    # Number of days in the composite\n    interval = endDoy - startDoy + 1\n\n    # Compute calendar dats for logging or directory creation\n    myDateEnd = datetime(endyear, 1, 1) + timedelta(endDoy - 1)\n    myDateStart = datetime(startyear, 1, 1) + timedelta(startDoy - 1)\n\n    # Output directory on server\n    outDir = '/ERDData1/modisa/data/modsiwc/' + endyearC + '/mday'\n\n    print(dataDir)\n    print(workDir)\n    print(endyearC)\n    print(endDoyC)\n    print(interval)\n\n    # List of variables to composite\n    dtypeList = ['chla', 'k490', 'par0', 'cflh']\n    for dtype in dtypeList:\n        # Clear working directory\n        os.chdir(workDir)\n        os.system('rm -f *')\n\n        # Move to input directory\n        os.chdir(dataDir)\n\n        # Initialize sum and count arrays\n        mean = np.zeros((2321, 4001), np.single)\n        num = np.zeros((2321, 4001), dtype=np.int32)\n\n        # Composite within same calendar year\n        if (endDoy &gt; startDoy):\n            doyRange = list(range(startDoy, endDoy + 1))\n            fileList = []\n            # Gather all matching NetCDF filenames for each DOY\n            for doy in doyRange:\n                doyC = str(doy)\n                doyC = doyC.rjust(3, '0')\n                myString = 'MW' + endyearC + doyC + '*' + dtype + '.nc'\n                fileList.append(glob.glob(myString))\n\n            # Flatten and sort the list\n            fileList = list(chain.from_iterable(fileList))\n            fileList.sort()\n\n            filesUsed = \"\"\n            print(fileList)\n\n            # Loop over each file, accumulate the variable\n            for fName in fileList:\n                if (len(filesUsed) == 0):\n                    filesUsed = fName\n                else:\n                    filesUsed = filesUsed + ', ' + fName\n\n                chlaFile = Dataset(fName)\n                param = 'MW' + dtype\n                chla = chlaFile.variables[param][:, :, :, :]\n                chlaFile.close()\n\n                # Remove singleton dimensions -&gt; 2D (lat x lon)\n                chla = np.squeeze(chla)\n\n                # Update running mean and count\n                mean, num = meanVar(mean, num, chla)\n\n        else:\n            # Composite spans year boundary\n            dataDir1 = dataDir\n            dataDir1 = dataDir1.replace(endyearC, startYearC)\n            # Determine days in start year\n            if (isleap):\n                endday = 366\n            else:\n                endday = 365\n\n            # DOY startDoy -&gt; end of startYearC\n            fileList = []\n            os.chdir(dataDir1)\n            doyRange = list(range(startDoy, endday + 1))\n            for doy in doyRange:\n                doyC = str(doy)\n                doyC = doyC.rjust(3, '0')\n                myString = 'MW' + startYearC + doyC + '*' + dtype + '.nc'\n                fileList.append(glob.glob(myString))\n            fileList = list(chain.from_iterable(fileList))\n            fileList.sort()\n\n            filesUsed = \"\"\n            print(fileList)\n            for fName in fileList:\n                if (len(filesUsed) == 0):\n                    filesUsed = fName\n                else:\n                    filesUsed = filesUsed + ', ' + fName\n\n                chlaFile = Dataset(fName)\n                param = 'MW' + dtype\n                chla = chlaFile.variables[param][:, :, :, :]\n                chlaFile.close()\n                chla = np.squeeze(chla)\n                mean, num = meanVar(mean, num, chla)\n\n            # DOY 1 -&gt; endDoy in endYearC\n            os.chdir(dataDir)\n            fileList = []\n            doyRange = list(range(1, endDoy + 1))\n            for doy in doyRange:\n                doyC = str(doy)\n                doyC = doyC.rjust(3, '0')\n                myString = 'MW' + endyearC + doyC + '*' + dtype + '.nc'\n                fileList.append(glob.glob(myString))\n\n            fileList = list(chain.from_iterable(fileList))\n            fileList.sort()\n            print(fileList)\n            for fName in fileList:\n                if (len(filesUsed) == 0):\n                    filesUsed = fName\n                else:\n                    filesUsed = filesUsed + ', ' + fName\n\n                chlaFile = Dataset(fName)\n                param = 'MW' + dtype\n                chla = chlaFile.variables[param][:, :, :, :]\n                chlaFile.close()\n                chla = np.squeeze(chla)\n                mean, num = meanVar(mean, num, chla)\n\n        # Mask out pixels with zero observations and set fill value for missing data \n        mean = ma.array(mean, mask=(num == 0), fill_value=-9999999.)\n\n        # Return to working directory for output\n        os.chdir(workDir)\n\n        # Construct output filename\n        outFile = 'MW' + startYearC + startDoyC + '_' + endyearC + endDoyC + '_' + dtype + '.nc'\n\n        # Create multi-day NetCDF\n        ncFile = makeNetcdfmDay(mean, num, interval, outFile, filesUsed, workDir)\n\n        # Send to server folder for this parameter\n        remote_dir = '/MW/' + dtype + '/'\n        send_to_servers(ncFile, remote_dir , 'm')\n\n        #intervalDay = str(interval) + 'day'\n        # myCmd = 'scp ' + ncFile  + ' cwatch@192.168.31.15:/u00/satellite/MW/' + dtype + '/mday'\n        #myCmd = 'rsync -tvh ' + ncFile  + ' cwatch@192.168.31.15:/u00/satellite/MW/' + dtype + '/mday/' + ncFile\n        #os.system(myCmd)\n        # myCmd = 'scp ' + ncFile  + ' cwatch@192.168.31.15:/u00/satelliteNAS/MW/' + dtype + '/mday'\n        #myCmd = 'rsync -tvh ' + ncFile  + ' cwatch@192.168.31.15:/u00/satelliteNAS/MW/' + dtype + '/mday/' + ncFile\n        #os.system(myCmd)\n        # myCmd = 'scp ' + ncFile  + ' cwatch@161.55.17.28:/u00/satellite/MW/' + dtype +  '/mday'\n        # myCmd = 'rsync -tvh ' + ncFile  + ' /u00/satellite/MW/' + dtype +  '/mday/' + ncFile\n        #myCmd = 'rsync -tvh ' + ncFile  + ' cwatch@161.55.17.28:/u00/satellite/MW/' + dtype +  '/mday/' + ncFile\n        #os.system(myCmd)",
    "crumbs": [
      "MODISA Scripts",
      "MW Datasets"
    ]
  },
  {
    "objectID": "mw_datasets.html#sst",
    "href": "mw_datasets.html#sst",
    "title": "MW Datasets",
    "section": "SST",
    "text": "SST\n\nmakeSST1daynewMW\nmakeSST1daynewMW.py creates a 1-day composite by reading Level-2 SST swath NetCDFs for the given date, filters “Day” pixels with quality flag &lt;2 that fall within longitude 205°–255° and latitude 22°–51°, and collates their longitude, latitude, and SST values. It then interpolates these observations onto a 0.0125° x 0.0125° regular grid using PyGMT, writes the result as a CF-compliant NetCDF, and uploads it to a directory on a remote server. Find the 1-day composite product on ERDDAP here.\nThe following chart summarizes the script’s workflow:\n\n\n\n\n\n%%{init: {\"flowchart\":{\"htmlLabels\":true}}}%%\nflowchart LR\n  I(\"&lt;b&gt;Inputs&lt;/b&gt;&lt;br/&gt;• Raw L2 SST NetCDF swaths (dataDir)&lt;br/&gt;• workDir&lt;br/&gt;• Year & DOY\")\n    --&gt; P(\"&lt;b&gt;Processing&lt;/b&gt;&lt;br/&gt;• Compute calendar date & directories&lt;br/&gt;• Load static land mask&lt;br/&gt;• Discover swaths for Day N (HOD&gt;10) & N+1 (HOD≤10)&lt;br/&gt;• Stage & filter swaths (Day-only, region overlap)&lt;br/&gt;• Extract, reshape & filter (SST + quality)&lt;br/&gt;• Interpolate onto 0.0125° grid&lt;br/&gt;• Apply land mask\")\n    --&gt; O(\"&lt;b&gt;Output&lt;/b&gt;&lt;br/&gt;• CF-compliant 1-day composite NetCDF&lt;br/&gt;• Deployed to /MW/sstd/1day/\")\n\n\n\n\n\n\n\n\nView makeSST1daynewMW.py\n\"\"\"\nOverview\n--------\nGenerate a one-day SST grid for the MODIS MW (West Coast) region by combining swaths\nfrom Level-2 SST NetCDF files and gridding them with PyGMT. The final product is a CF-compliant\nNetCDF file containing daily SST for the MW region (lon 205-255°, lat 22-51°).\n\nUsage\n-----\n::\n\n    python makeSST1daynewMW.py &lt;dataDir&gt; &lt;workDir&gt; &lt;year&gt; &lt;doy&gt;\n\nWhere:\n\n- ``dataDir``\n\n  Directory containing raw Level-2 SST NetCDF swaths for the target date, organized as ``&lt;dataDirBase&gt;/&lt;YYYY&gt;&lt;MM&gt;/``. Each file must follow the pattern: ``AQUA_MODIS.&lt;YYYY&gt;&lt;MM&gt;&lt;DD&gt;T*.L2.SST.NRT.nc``.\n\n- ``workDir`` \n\n  Temporary working directory. Swath files are copied here, intermediate products are created, and then cleaned up.\n\n- ``year``\n\n  Four-digit year string (e.g., ``\"2025\"``).\n\n- ``doy``\n\n  Three-digit day-of-year string (zero-padded, e.g., ``\"082\"`` for March 23).\n\nDescription\n-----------\n1. **Date and Directory Setup**  \n\n   - Convert ``year`` + ``doy`` to a calendar date (``myDate``), then extract zero-padded month (``myMon``) and day (``myDay``).\n\n   - Define ``datadir = dataDirBase + year + myMon + \"/\"``. This folder must contain all SST swath NetCDF files for the date.\n\n   - Load a static land-mask grid from ``/u00/ref/landmasks/LM_205_255_0.0125_22_51_0.0125_gridline.grd`` into ``my_mask``.\n\n2. **Swath Discovery**  \n\n   - Change directory to ``datadir``.\n\n   - Build a glob pattern: ``AQUA_MODIS.&lt;year&gt;&lt;myMon&gt;&lt;myDay&gt;*.L2.SST.NRT.nc``.\n\n   - Sort the resulting list of swath filenames (``fileList``).\n\n3. **Data Staging and Cleanup**  \n\n   - Change directory to ``workDir``.\n\n   - Remove any existing temporary files matching ``AQUA_MODIS.*L2.SST*`` or ``MW20*`` to start fresh.\n\n4. **Loop Over Each Swath**  \n\n   For each file ``fName`` in ``fileList``:\n\n   a. **Copy and Open**  \n\n      - Copy the swath from ``datadir`` to ``workDir``.\n\n      - Open with ``netCDF4.Dataset``. If unreadable, skip the swath.\n\n   b. **Extract Navigation (Swath Geometry)**  \n\n      - Read ``latitude`` and ``longitude`` from the ``navigation_data`` group.\n\n      - Convert any negative longitudes to the 0-360° range.\n\n      - Compute swath extents: ``dataLonMin``, ``dataLonMax``, ``dataLatMin``, ``dataLatMax``.\n\n      - Define tests for geographic overlap:\n\n       - Longitude overlaps 205°-255°. \n\n       - Latitude overlaps 22°-51°.  \n\n      - Check ``day_night_flag == \"Day\"``.\n\n      - Only if all tests pass, append the swath to ``filesUsed``.\n\n   c. **Extract SST and Quality**  \n\n      - In the ``geophysical_data`` group, read:\n\n        - ``sst`` (sea surface temperature)\n\n        - ``qual_sst`` (quality flag)  \n\n      - Reshape ``sst``, ``latitude``, and ``longitude`` into column vectors using ``myReshape``.\n\n      - Flatten ``qual_sst`` into a 1D mask array (``qual_sst1``).\n\n      - Stack ``longitude``, ``latitude``, and ``sst`` into a 2D ``dataOut`` array.\n\n      - Apply filters to ``dataOut``:\n\n        - Quality flag &lt; 2.  \n\n        - Longitude between 205° and 255°. \n\n        - Latitude between 22° and 51°.  \n\n        - SST &gt; -2 °C.  \n\n      - Concatenate valid points into a master array ``temp_data``.\n\n   d. **Cleanup Swath File**  \n\n      - Close the NetCDF dataset.\n\n      - Delete the swath file from ``workDir``.\n\n5. **Gridding Swath Point Cloud**  \n\n   - After processing all swaths, define output grid filename: ``MW&lt;year&gt;&lt;doy&gt;_&lt;year&gt;&lt;doy&gt;_sstd.grd``.\n\n   - Set PyGMT region and spacing:\n\n      - ``region = \"205/255/22/51\"``  \n\n     - ``spacing = \"0.0125/0.0125\"``  \n\n     - ``search_radius = \"2k\"``  \n\n     - ``sectors = \"1\"``  \n\n   - Then run the following to produce a gridded ``xarray.DataArray`` (``temp_grid``):\n\n   ::\n\n       pygmt.nearneighbor(\n           data=temp_data,\n           region=region,\n           spacing=spacing,\n           search_radius=search_radius,\n           sectors=sectors\n       )\n\n6. **Convert to NetCDF and Send to Server**  \n\n   Invoke:\n\n   ``grd2netcdf1(temp_grid, fileOut, filesUsed, my_mask, \"MW\")``\n\n   from ``roylib``:\n\n   - Applies ``my_mask`` to zero out land pixels.\n\n   - Computes coverage statistics (# observations, % coverage).\n\n   - Generates a CF-compliant NetCDF skeleton via ``ncgen`` + a CDL template.\n\n   - Populates coordinates, SST data, metadata, and a center-time stamp.\n\n   - Returns the new NetCDF filename (``MW&lt;year&gt;&lt;doy&gt;_&lt;year&gt;&lt;doy&gt;_sstd.nc``).\n\n   Then run:\n\n   ::\n\n       send_to_servers(ncFile, \"/MW/sstd/\", \"1\")\n\n   to copy the NetCDF into ``/MW/sstd/1day/``.\n\n   Finally, remove the local NetCDF copy.\n\nDependencies\n------------\n- **Python 3.x**\n\n- **Standard library:**  ``os``, ``glob``, ``re``, ``shutil``, ``sys``, ``datetime``, ``timedelta``\n\n- **Third-party packages:** ``netCDF4.Dataset``, ``numpy``, ``pygmt`` \n\n- **Custom roylib functions:**\n\n   - ``myReshape(array)``\n  \n   - ``grd2netcdf1(grd, outName, filesUsed, mask, fType)``\n\n   - ``safe_remove(filePath)``  \n\n   - ``send_to_servers(ncFile, destDir, interval)``\n\n   - ``isleap(year)``\n\n   - ``makeNetcdf(mean, nobs, interval, outFile, filesUsed, workDir)``\n\n   - ``meanVar(mean, num, obs)``\n\nLand Mask\n---------\n- A static GRD file is required at:\n\n  ``/u00/ref/landmasks/LM_205_255_0.0125_22_51_0.0125_gridline.grd``\n\n- This mask is applied to the gridded SST to set land pixels to NaN.\n\nDirectory Structure\n-------------------\n- **Input swaths directory** (datadir):\n\n  Must contain all Level-2 NetCDF swath files for the date, named: ``AQUA_MODIS.&lt;YYYY&gt;&lt;MM&gt;&lt;DD&gt;T*.L2.SST.NRT.nc``\n\n- **Working directory** (workDir):  \n\n  Temporary location where swaths are staged, processed, and removed.\n\n- **Output grid** (fileOut):  \n\n  A GMT “.grd” file named ``MW&lt;year&gt;&lt;doy&gt;_&lt;year&gt;&lt;doy&gt;_sstd.grd``.\n\n- **Final NetCDF** (returned by grd2netcdf1):  \n\n  Named ``MW&lt;year&gt;&lt;doy&gt;_&lt;year&gt;&lt;doy&gt;_sstd.nc``, then copied to ``/MW/sstd/1day/``.\n\nUsage Example\n-------------\nAssume your raw SST swaths are in  ``/Users/you/modis_data/netcdf/202503/`` and your  working directory is ``/Users/you/modis_work/``:\n\n::\n\n    python makeSST1daynewMW.py /Users/you/modis_data/netcdf/202503/  /Users/you/modis_work/  2025 082\n\nThis will:\n\n  - Copy all swaths matching ``AQUA_MODIS.20250323*.L2.SST.NRT.nc`` into ``/Users/you/modis_work/``.\n\n  - Build a combined point cloud from valid “Day” pixels within the MW region.\n\n  - Create ``MW2025082_2025082_sstd.grd`` via PyGMT nearneighbor.\n\n  - Convert the grid to ``MW2025082_2025082_sstd.nc`` using ``grd2netcdf1``.\n\n  - Copy the NetCDF to ``/MW/sstd/1day/`` and remove local copies.\n\"\"\"\nfrom __future__ import print_function\nfrom builtins import str\n\nif __name__ == \"__main__\":\n    from datetime import datetime, timedelta\n    import glob\n    from itertools import chain\n    from netCDF4 import Dataset\n    import numpy as np\n    import numpy.ma as ma\n    import pygmt\n    import os\n    import re\n    import shutil\n    import sys\n\n    # Ensure 'roylib' is on the import path\n    sys.path.append('/home/cwatch/pythonLibs')\n    from roylib import *\n\n    # Geographic bounds for MW region\n    latmax = 51.\n    latmin = 22.\n    lonmax = 255.\n    lonmin = 205.\n\n    outFile = 'modiswcSSTtemp'\n\n    # Set data directory\n    datadirBase = sys.argv[1]\n\n    # Set work directory\n    workdir = sys.argv[2]\n\n    # Get the year and doy from the command line\n    year = sys.argv[3]\n    doy = sys.argv[4]\n    print(year)\n    print(doy)\n\n    # Convert year/doy to a calendar date and zero-pad month/day\n    myDate = datetime(int(year), 1, 1) + timedelta(int(doy) - 1)\n    myMon = str(myDate.month)\n    myMon = myMon.rjust(2, '0')\n    myDay = str(myDate.day).rjust(2, '0')\n\n    # Construct the directory path\n    datadir = datadirBase + year + myMon + '/'\n\n    # Load static land mask from GRD\n    mask_root = Dataset('/u00/ref/landmasks/LM_205_255_0.0125_22_51_0.0125_gridline.grd')\n    my_mask = mask_root.variables['z'][:, :]\n    mask_root.close()\n\n    # Now move to the data directory\n    os.chdir(datadir)\n\n    # Set up the string for the file search in the data directory\n    myString = 'AQUA_MODIS.' + year + myMon + myDay  + '*.L2.SST.NRT.nc'\n\n    # Get list of files in the data directory that match with full path\n    fileList = glob.glob(myString)\n    # print(fileList)\n    fileList.sort()\n\n    # Now move to the work directory and clear old files\n    os.chdir(workdir)\n    os.system('rm -f AQUA_MODIS.*L2.SST*')\n    os.system('rm -f MW20*')\n\n    # Prepare variables to accumulate point-cloud data and track provenance\n    filesUsed = \"\"\n    temp_data = None\n\n    # Loop through each swath filename for Day N\n    for fName in fileList:\n        fileName = fName\n        print(fileName)\n        shutil.copyfile(datadir + fName, workdir + fName)\n        try:\n            rootgrp = Dataset(fileName, 'r')\n        except IOError:\n            print(\"bad file \" + fileName)\n            continue\n\n        # Extract navigation-group data\n        navDataGroup = rootgrp.groups['navigation_data']\n        latitude = navDataGroup.variables['latitude'][:, :]\n        longitude = navDataGroup.variables['longitude'][:, :]\n\n        # Convert any negative longitudes to the 0-360° domain\n        longitude[longitude &lt; 0] = longitude[longitude &lt; 0] + 360\n\n        # Compute swath extents (min/max) for geographic filtering\n        dataLonMin = np.nanmin(longitude[longitude &gt;= 0])\n        dataLonMax = np.nanmax(longitude[longitude &lt;= 360])\n        dataLatMin = np.nanmin(latitude[latitude &gt;= -90])\n        dataLatMax = np.nanmax(latitude[latitude &lt;= 90])\n\n        # Determine if swath overlaps our global MW region\n        goodLon1 = (dataLonMin &lt; lonmin) and (dataLonMax &gt;= lonmin)\n        goodLon2 = (dataLonMin &gt;= lonmin) and (dataLonMin &lt;= lonmax)\n        goodLon = goodLon1 or goodLon2\n\n        goodLat1 = (dataLatMin &lt; latmin) and (dataLatMax &gt;= latmin)\n        goodLat2 = (dataLatMin &gt;= latmin) and (dataLatMin &lt;= latmax)\n        goodLat = goodLat1 or goodLat2\n\n        # Check if swath is daytime (only keep \"Day\" pixels)\n        dayNightTest = (rootgrp.day_night_flag == 'Day')\n\n        # Only proceed if geography and day-night tests pass\n        if (goodLon and goodLat and dayNightTest):\n            # Add filename to provenance list\n            if (len(filesUsed) == 0):\n                filesUsed = fileName\n            else:\n                filesUsed = filesUsed + ', ' + fileName\n\n            # Extract geophysical data and reshape\n            geoDataGroup = rootgrp.groups['geophysical_data']\n            sst = geoDataGroup.variables['sst'][:, :]\n            sst = myReshape(sst)\n            qual_sst = geoDataGroup.variables['qual_sst'][:, :]\n            qual_sst1 = qual_sst.flatten()\n            latitude = myReshape(latitude)\n            longitude = myReshape(longitude)\n\n            # Stack (lon, lat, sst) into a single 2D array with shape (N, 3)\n            dataOut = np.hstack((longitude, latitude, sst))\n\n            # Keep only quality &lt; 2, valid SST &gt; -2°C, and within MW region\n            qualTest = qual_sst1 &lt; 2\n            dataOut = dataOut[qualTest]\n            dataOut = dataOut[dataOut[:, 0] &gt; -400]\n            dataOut = dataOut[dataOut[:, 0] &gt;= lonmin]\n            dataOut = dataOut[dataOut[:, 0] &lt;= lonmax]\n            dataOut = dataOut[dataOut[:, 1] &gt;= latmin]\n            dataOut = dataOut[dataOut[:, 1] &lt;= latmax]\n            dataOut = dataOut[dataOut[:, 2] &gt; -2]\n\n            # Accumulate into temp_data array\n            if (dataOut.shape[0] &gt; 0):\n                if (temp_data is None):\n                    temp_data = dataOut\n                else:\n                    temp_data = np.concatenate((temp_data, dataOut), axis=0)\n\n        # Close the NetCDF and remove swath copy from workdir\n        rootgrp.close()\n        os.remove(fileName)\n\n    # Build and write the GMT grid from the accumulated point cloud\n    # Define output grid filename\n    fileOut = 'MW' + year + doy + '_' + year + doy + '_sstd.grd'\n    range = '205/255/22/51'\n    increment = '0.0125/0.0125'\n    smooth = '2k'\n\n    # Use PyGMT nearneighbor to interpolate scattered (lon, lat, sst) points onto a grid\n    temp_data1 = pygmt.nearneighbor(\n        data=temp_data,\n        region=range,\n        spacing=increment,\n        search_radius=smooth,\n        sectors='1'\n    )\n\n    # Convert the GMT grid to a CF-compliant NetCDF and send to server\n    # Apply the land mask, create a NetCDF via a CDL template, and add metadata\n    ncFile = grd2netcdf1(temp_data1, fileOut, filesUsed, my_mask, 'MW')\n\n    #myCmd = \"mv \" + ncFile + \" /home/cwatch/pygmt_test/outfiles\"\n    #os.system(myCmd)\n\n    # Copy the final NetCDF to the \"MW\" server folder for 1-day products\n    send_to_servers(ncFile, '/MW/sstd/', '1')\n\n    # Remove the local NetCDF to copy from workdir\n    os.remove(ncFile)\n\n\n\n\nCompMWSST\nCompMWSST.py computes a multi-day (3-, 8-, & 14-) SST composite by reading daily 1-day SST NetCDFs over the specified interval and averaging them on a regular 0.0125° x 0.0125° grid covering longitude 205°–255° and latitude 22°–51°. It masks out pixels with no observations, writes the result as a CF-compliant NetCDF, and uploads it to a directory on a remote server. Find the multi-day composite products on ERDDAP here.\nThe following chart summarizes the script’s workflow:\n\n\n\n\n\n%%{init: {\"flowchart\":{\"htmlLabels\":true}}}%%\nflowchart LR\n  I(\"&lt;b&gt;Inputs&lt;/b&gt;&lt;br/&gt;• 1-day composite NetCDFs (dataDir)&lt;br/&gt;• workDir&lt;br/&gt;• endYear & endDoy&lt;br/&gt;• interval (days)\")\n    --&gt; P(\"&lt;b&gt;Processing&lt;/b&gt;&lt;br/&gt;• Parse args & compute start/end DOY&lt;br/&gt;• Clear workDir & initialize mean/num arrays&lt;br/&gt;• Gather files over date range&lt;br/&gt;• Loop: open each file & update mean/num&lt;br/&gt;• Mask zero-observation pixels\")\n    --&gt; O(\"&lt;b&gt;Output&lt;/b&gt;&lt;br/&gt;• CF-compliant multi-day composite NetCDF&lt;br/&gt;• Deployed to /MW/sstd/interval day/\")\n\n\n\n\n\n\n\n\nView CompMWSST.py\n\"\"\"\nOverview\n--------\nGenerate multi-day SST composites for the MODIS MW (West Coast) dataset by averaging\n1-day NetCDF files over a specified interval (e.g., 3, 5, 8, or 14 days).\n\nUsage\n-----\n::\n  \n    python CompMWSST.py &lt;dataDir&gt; &lt;workDir&gt; &lt;endYear&gt; &lt;endDoy&gt; &lt;interval&gt;\n\nWhere:\n\n- ``dataDir``\n\n  Directory containing the daily MW SST NetCDF files named ``MW&lt;YYYY&gt;&lt;DDD&gt;*sstd.nc``.\n\n- ``workDir``\n\n  Temporary working directory for intermediate files.\n\n- ``endYear`` \n\n  Four-digit year of the last day in the composite (e.g., ``2025``).\n\n- ``endDoy``\n\n  Three-digit day-of-year of the last day (e.g., ``082``).\n\n- ``interval``\n\n  Number of days to include (e.g., `3`, `5`, `8`, `14`).\n\nDescription\n-----------\n1. **Parse arguments & compute dates**  \n\n  - Read paths and parameters from ``sys.argv``.\n  \n  - Convert ``endYear``+``endDoy`` to a calendar date ``myDateEnd``.\n  \n  - Compute ``myDateStart = myDateEnd - (interval-1) days``.\n  \n  - Zero-pad start and end DOY strings.\n\n2. **Initialize**\n\n  - Change into ``workDir`` and clear old files.\n\n  - Change into ``dataDir``.\n\n  - Preallocate two 2321x4001 arrays:\n\n      - ``mean`` (running sum of SST)\n\n      - ``num``  (count of observations)\n\n3. **Collect 1-day files**\n\n  - If ``startDoy ≤ endDoy`` (same year), gather DOYs ``startDoy…endDoy``.\n\n  - Else, handle year-boundary wrap: DOYs ``startDoy…endOfYear`` and ``1…endDoy``.\n\n4. **Accumulate SST**\n\n  For each matching file:\n\n     - Read and squeeze the 4D SST variable ``\"MWsstd\"``.\n     \n     - Update ``mean, num`` via ``meanVar(mean, num, sst)``.\n\n5. **Finalize composite**\n\n  - Mask out cells with zero observations (``num==0``), setting fill value ``-9999999.``.\n\n6. **Write & deploy**\n\n  - Change back to ``workDir``.\n\n  - Call ``makeNetcdf(mean, num, interval, outFile, filesUsed, workDir)`` to create CF-compliant NetCDF.\n\n  - Transfer result via ``send_to_servers(ncFile, \"/MW/sstd/\", str(interval))``.\n\nDependencies\n------------\n- **Python 3.x**\n\n- **Standard library:** ``os``, ``sys``, ``glob``, ``itertools.chain``, ``datetime``, ``timedelta``  \n\n- **Third-party:** ``netCDF4.Dataset``, ``numpy``, ``numpy.ma``  \n\n- **Custom roylib functions:**\n\n  - ``isleap(year)``\n\n  - ``meanVar(sum_array, count_array, data_slice)``\n\n  - ``makeNetcdfmDay(mean, num, interval, outFile, filesUsed, workDir)``\n\n  - ``send_to_servers(ncFile, remote_dir, 'm')``\n\nDirectory Structure\n-------------------\n- **Input Directory** (dataDir):\n\n  ``MW&lt;YYYY&gt;&lt;DDD&gt;*sstd.nc`` (1-day SST files)\n\n- **Working directory** (workDir):  \n\n  Temporary staging for intermediate files  \n\n- **Output location** (remote):  \n\n  ``/MW/sstd/&lt;interval&gt;day/`` on the server  \n\nUsage Example\n-------------\nGenerate a 5-day composite ending on 2025 day-of-year 082\n\n::\n \n   python CompMWSST.py /Users/you/modis_data/mw/1day/ /Users/you/modis_work/ 2025 082 5\n\nThis command will:\n\n  - Read the five daily files MW202508?*sstd.nc for DOY 078-082 \n\n  - Compute the 5-day running average for each grid cell  \n\n  - Write ``MW2025078_2025082_sstd.nc`` in ``/Users/you/modis_work/`` \n   \n  - Upload it to ``/MW/sstd/5day/`` on the server \n\"\"\"\nfrom __future__ import print_function\nfrom builtins import str\nfrom builtins import range\n\nif __name__ == \"__main__\":\n    from datetime import datetime, timedelta\n    import glob\n    from itertools import chain\n    from netCDF4 import Dataset\n    import numpy as np\n    import numpy.ma as ma\n    import os\n    import sys\n\n    # Ensure 'roylib' is on the import path\n    sys.path.append('/home/cwatch/pythonLibs')\n    from roylib import *\n\n    # Directory with 1-day MW SST NetCDFs\n    dataDir = sys.argv[1]\n\n    # Temporary working directory\n    workDir = sys.argv[2]\n\n    # Composite end year (YYYY)\n    endyearC = sys.argv[3]\n\n    # Composite end day-of-year (DDD)\n    endDoyC = sys.argv[4]\n    endDoyC = endDoyC.rjust(3, '0')\n\n    # Integer form of end day-of-year\n    endDoy = int(endDoyC)\n\n    # Composite length as string\n    intervalC = sys.argv[5]\n\n    print(dataDir)\n    print(intervalC)\n\n    # Composite length as integer\n    interval = int(intervalC)\n\n    # Convert end Doy to calendar date\n    myDateEnd = datetime(int(endyearC), 1, 1) + timedelta(int(endDoyC) - 1)\n\n    # Start date = end date minus (interval-1) days\n    myDateStart = myDateEnd + timedelta(days=-(interval - 1))\n\n    # Zero-padded start Doy and year\n    startDoyC = myDateStart.strftime(\"%j\").zfill(3)\n    startDoy = int(startDoyC)\n    startYearC = str(myDateStart.year)\n\n    # Prepare output directory \n    outDir = '/ERDData1/modisa/data/modiswc/' + endyearC + '/' + intervalC + 'day'\n\n    print(dataDir)\n    print(workDir)\n    print(endyearC)\n    print(endDoyC)\n    print(intervalC)\n\n    ###\n    # dtypeList = ['sstd']\n    # for dtype in dtypeList:\n\n    # Data type for SST composites\n    dtype = 'sstd'\n\n    # Clear working directory\n    os.chdir(workDir)\n    os.system('rm -f *')\n\n    # Move to data directory\n    os.chdir(dataDir)\n\n    # Preallocate sum (mean) and count arrays matching grid dims\n    mean = np.zeros((2321, 4001), np.single)\n    num = np.zeros((2321, 4001), dtype=np.int32)\n\n    # Collect all NetCDF files spanning startDoy..endDoy\n    if (endDoy &gt; startDoy):\n        # Same-year composite\n        doyRange = list(range(startDoy, endDoy + 1))\n        fileList = []\n        for doy in doyRange:\n            doyC = str(doy)\n            doyC = doyC.rjust(3, '0')\n            myString = 'MW' + endyearC + doyC + '*' + dtype + '.nc'\n            fileList.append(glob.glob(myString))\n\n        # Flatten nested lists and sort\n        fileList = list(chain.from_iterable(fileList))\n        fileList.sort()\n        print(fileList)\n\n        filesUsed = \"\"\n        for fName in fileList:\n            # Build comma-separated list of files used\n            if (len(filesUsed) == 0):\n                filesUsed = fName\n            else:\n                filesUsed = filesUsed + ', ' + fName\n\n            # Open NetCDF file and extract 4D SST variable\n            sstFile = Dataset(fName)\n            sst = sstFile.variables[\"MWsstd\"][:, :, :, :]\n            sstFile.close()\n\n            # Squeeze to remove singleton dimensions -&gt; 2D array\n            sst = np.squeeze(sst)\n\n            # Update running mean and count arrays\n            mean, num = meanVar(mean, num, sst)\n\n    else:\n        # Compute directory for start of year by replacing year in path\n        dataDir1 = dataDir\n        dataDir1 = dataDir1.replace(endyearC, startYearC)\n\n        # Determine end-of-year DOY based on leap year\n        if(isleap):\n            endday = 366\n        else:\n            endday = 365\n\n        # From startDoy through end of startYearC\n        fileList = []\n        os.chdir(dataDir1)\n        doyRange = list(range(startDoy, endday + 1))\n        for doy in doyRange:\n            doyC = str(doy)\n            doyC = doyC.rjust(3, '0')\n            myString = 'MW' + startYearC + doyC + '*' + dtype + '.nc'\n            fileList.append(glob.glob(myString))\n\n        fileList = list(chain.from_iterable(fileList))\n        fileList.sort()\n        print(fileList)\n\n        filesUsed = ''\n        for fName in fileList:\n            if (len(filesUsed) == 0):\n                filesUsed = fName\n            else:\n                filesUsed = filesUsed + ', ' + fName\n\n            sstFile = Dataset(fName)\n            sst = sstFile.variables[\"MWsstd\"][:, :, :, :]\n            sstFile.close()\n            sst = np.squeeze(sst)\n            mean, num = meanVar(mean, num, sst)\n\n        # From DOY=1 of endyearC through endDoy\n        os.chdir(dataDir)\n        fileList = []\n        doyRange= list(range(1, endDoy + 1))\n        for doy in doyRange:\n            doyC = str(doy)\n            doyC = doyC.rjust(3, '0')\n            myString = 'MW' + endyearC + doyC + '*' + dtype + '.nc'\n            fileList.append(glob.glob(myString))\n\n        fileList = list(chain.from_iterable(fileList))\n        fileList.sort()\n        print(fileList)\n\n        for fName in fileList:\n            if (len(filesUsed) == 0):\n                filesUsed = fName\n            else:\n                filesUsed = filesUsed + ', ' + fName\n\n            sstFile = Dataset(fName)\n            sst = sstFile.variables[\"MWsstd\"][:, :, :, :]\n            sstFile.close()\n            sst = np.squeeze(sst)\n            mean, num = meanVar(mean, num, sst)\n\n    # Mask out any grid cells with zero observations, setting them to the fill value\n    mean = ma.array(mean, mask=(num == 0), fill_value=-9999999.)\n    print('COmpMWSST finished mean')\n\n    # Switch to the working directory for output operations\n    os.chdir(workDir)\n\n    # Construct the output filename with start and end dates plus data types\n    outFile = 'MW' + startYearC + startDoyC + '_' + endyearC + endDoyC + '_' + dtype + '.nc'\n\n    # Create multi-day NetCDF file using the mean and count arrays\n    ncFile = makeNetcdf(mean, num, interval, outFile, filesUsed, workDir)\n\n    # Directory on the remote server for storing the multi-day SST product\n    remote_dir = '/MW/sstd/'\n\n    # Transfer the generated NetCDF file to the remote server directory, labeling it with the interval\n    send_to_servers(ncFile, remote_dir , str(interval))\n\n\n\n\nCompMWSSTmday\nCompMWSSTmday.py computes a monthly composite SST by reading 1-day composite MW SST NetCDFs over the specified date range and averaging them on a 0.0125° x 0.0125° regular grid covering longitude 205°–255° and latitude 22°–51°. It then masks out any pixels with no valid observations, writes the result as a CF-compliant NetCDF, and uploads it to a directory on a remote server. Find the monthly composite product on ERDDAP here.\nThe following chart summarizes the script’s workflow:\n\n\n\n\n\n%%{init: {\"flowchart\":{\"htmlLabels\":true}}}%%\nflowchart LR\n  I(\"&lt;b&gt;Inputs&lt;/b&gt;&lt;br/&gt;• 1-day composite NetCDFs (dataDir)&lt;br/&gt;• workDir&lt;br/&gt;• endYear & endDoy&lt;br/&gt;• startDoy\")\n    --&gt; P(\"&lt;b&gt;Processing&lt;/b&gt;&lt;br/&gt;• Parse args & compute interval&lt;br/&gt;• Compute start/end dates&lt;br/&gt;• Clear workDir & initialize mean/num arrays&lt;br/&gt;• Gather files spanning startDoy…endDoy&lt;br/&gt;• Loop: open each file & update mean/num&lt;br/&gt;• Mask zero‐observation pixels\")\n    --&gt; O(\"&lt;b&gt;Output&lt;/b&gt;&lt;br/&gt;• CF-compliant monthly composite NetCDF&lt;br/&gt;• Deployed to /MW/sstd/mday/\")\n\n\n\n\n\n\n\n\nView CompMWSSTmday.py\n\"\"\"\nOverview\n--------\nCompute monthly SST composites for the MODIS MW (West Coast) region by averaging\ndaily 1-day SST NetCDF products over a specified interval.\n\nUsage\n-----\n::\n  \n    python CompMWSSTmday.py &lt;dataDir&gt; &lt;workDir&gt; &lt;endYear&gt; &lt;endDoy&gt; &lt;startDoy&gt;\n\nWhere:\n\n- ``dataDir``\n\n  Directory containing 1-day NetCDF files named ``MW&lt;YYYY&gt;&lt;DDD&gt;*sstd.nc``\n\n- ``workDir``\n\n  Temporary working directory for intermediate files\n\n- ``endYear``   \n\n  Four-digit year of the last day in the composite (e.g., ``2025``)\n\n- ``endDoy``\n\n  Three-digit day-of-year of the last day (e.g., ``082``)\n\n- ``startDoy``\n\n  Three-digit day-of-year of the first day (e.g., ``075``)\n\nDescription\n-----------\n1. **Parse command-line arguments** and compute the composite length:\n\n  - Read ``dataDir``, ``workDir``, ``endYear``, ``endDoy``, ``startDoy``.\n\n  - Zero-pad ``endDoy`` and ``startDoy``, compute ``interval = endDoy - startDoy + 1``.\n\n2. **Convert endDoy to a calendar date** (``myDateEnd``) for logging or output paths.\n\n3. **Gather all 1-day SST files** spanning ``startDoy..endDoy``:\n\n  - Build glob patterns ``MW&lt;year&gt;&lt;DDD&gt;*sstd.nc``\n\n  - Handles same-year and year-boundary wrap-around cases.\n\n4. **Initialize accumulators**:\n\n  - ``mean`` (float32) and ``num`` (int32) arrays of shape 2321x4001.\n\n5. **Loop over each NetCDF**:\n\n  - Open with ``Dataset()``.\n\n  - Read the 4D variable ``MWsstd``, squeeze to 2D.\n\n  - Update running ``mean`` and ``num`` via ``meanVar(mean, num, sst)``.\n\n6. **Mask unobserved cells** (``num ==0``) and set fill value ``-9999999``.\n\n7. **Write composite**:\n\n  - Call ``makeNetcdfmDay(mean, num, interval, outFile, filesUsed, workDir)`` to produce a CF-compliant NetCDF.\n\n8. **Transfer result**:\n\n  - Use ``send_to_servers(ncFile, '/MW/sstd/', 'm')`` to copy the composite to the remote directory.\n\nDependencies\n------------\n- **Python 3.x**\n\n- **Standard library:** ``os``, ``sys``, ``glob``, ``itertools.chain``, ``datetime``, ``timedelta``\n\n- **Third-party:** ``numpy``, ``numpy.ma``, ``netCDF4.Dataset``\n\n- **Custom roylib functions:**\n\n  - ``isleap(year)``\n\n  - ``meanVar(sum_array, count_array, data_slice)``\n\n  - ``makeNetcdfmDay(mean, num, interval, outFile, filesUsed, workDir)``\n\n  - ``send_to_servers(ncFile, remote_dir, 'm')``\n\nDirectory Structure\n-------------------\n- **Input directory** (dataDir):\n\n  Contains files like ``MWYYYYDDD*sstd.nc``, one per day.\n\n- **Working directory** (workDir):\n\n  Cleared and used for any temporary artifacts (none persisted).\n\n- **Output Location** (via ``makeNetcdfmDay``):\n\n  Writes ``MW&lt;startYYYY&gt;&lt;startDDD&gt;_&lt;endYYYY&gt;&lt;endDDD&gt;_sstd.nc`` in ``&lt;workDir&gt;`` and then copies it to ``/MW/sstd/``.\n\nUsage Example\n-------------\nCreate a January 2025 composite (DOY 001-031):\n\n::\n \n   python CompMWSSTmday.py /path/to/MW/1day/ /path/to/tmp/ 2025 031 001\n\nThis command will:\n\n  - Read daily SST files ``MW2025001*…MW2025031*`` from ``/path/to/MW/1day/``.\n\n  - Compute the 31-day average for each grid cell.\n\n  - Write ``MW2025001_2025031_sstd.nc`` in ``/path/to/tmp/``. \n\n  - Upload the composite to ``/MW/sstd/`` on the server.\n\"\"\"\nfrom __future__ import print_function\nfrom builtins import str\nfrom builtins import range\n\nif __name__ == \"__main__\":\n    from datetime import datetime, timedelta\n    import glob\n    from itertools import chain\n    from netCDF4 import Dataset\n    import numpy as np\n    import numpy.ma as ma\n    import os\n    import sys\n\n    # Ensure 'roylib' is on the import path\n    sys.path.append('/home/cwatch/pythonLibs')\n    from roylib import *\n\n    # Directory containing daily MW NetCDF files\n    dataDir = sys.argv[1]\n\n    # Temporary working directory\n    workDir = sys.argv[2]\n\n    # End date (year, day-of-year)\n    endyearC = sys.argv[3]\n    endDoyC = sys.argv[4]\n    endDoyC = endDoyC.rjust(3, '0')\n\n    # Start day-of-year (same year)\n    startYearC = endyearC\n    endDoy = int(endDoyC)\n    startDoyC = sys.argv[5]\n    startDoyC = startDoyC.rjust(3, '0')\n    startDoy = int(startDoyC)\n\n    # Number of days in the composite\n    interval = endDoy - startDoy + 1\n\n    # Convert end date to calendar dates\n    myDateEnd = datetime(int(endyearC), 1, 1) + timedelta(endDoy - 1)\n    myDateStart = myDateEnd + timedelta(startDoy - 1)\n\n    # Prepare output directory\n    outDir = '/ERDData1/modisa/data/modiswc/' + endyearC + '/mday'\n    print(dataDir)\n    print(workDir)\n    print(endyearC)\n    print(endDoyC)\n    print(interval)\n\n    ###\n    # dtypeList = ['sstd']\n    # for dtype in dtypeList:\n\n    # Set up for reading MB SST variable (\"MWsstd\") across multiple days\n    dtype = 'sstd'\n\n    # Clear working directory\n    os.chdir(workDir)\n    os.system('rm -f *')\n\n    # Move to data directory\n    os.chdir(dataDir)\n\n    # Preallocate sum (mean) and count arrays matching grid dims\n    mean = np.zeros((2321, 4001), np.single)\n    num = np.zeros((2321, 4001), dtype=np.int32)\n\n    # Build list of NetCDF files spanning startDoy..endDoy\n    if (endDoy &gt; startDoy):\n        # Composite within the same calendar year\n        doyRange = list(range(startDoy, endDoy + 1))\n        fileList = []\n        for doy in doyRange:\n            doyC = str(doy)\n            doyC = doyC.rjust(3, '0')\n            myString = 'MW' + endyearC + doyC + '*' + dtype + '.nc'\n            # find all files matching MWYYYYDD*&lt;dtype&gt;.nc\n            fileList.append(glob.glob(myString))\n\n        # Flatten list of lists and sort alphabetically\n        fileList = list(chain.from_iterable(fileList))\n        fileList.sort()\n        print(fileList)\n\n        # Track which files were used\n        filesUsed = \"\"\n        for fName in fileList:\n            if (len(filesUsed) == 0):\n                filesUsed = fName\n            else:\n                filesUsed = filesUsed + ', ' + fName\n\n            # Open NetCDF, extract the 4D SST array, then squeeze to 2D\n            sstFile = Dataset(fName)\n            sst = sstFile.variables[\"MWsstd\"][:, :, :, :]\n            sstFile.close()\n            sst = np.squeeze(sst)\n\n            # Update running mean and count arrays\n            mean, num = meanVar(mean, num, sst)\n\n    else:\n        # Compute directory for start of year by replacing year in path\n        dataDir1 = dataDir\n        dataDir1 = dataDir1.replace(endyearC, startYearC)\n\n        # Determine end-of-year DOY based on leap year\n        if (isleap):\n            endday = 366\n        else:\n            endday = 365\n\n        # From startDoy through end of start year  \n        fileList = []\n        os.chdir(dataDir1)\n        doyRange = list(range(startDoy, endday + 1))\n        for doy in doyRange:\n            doyC = str(doy)\n            doyC = doyC.rjust(3, '0')\n            myString = 'MW' + startYearC + doyC + '*' + dtype + '.nc'\n            fileList.append(glob.glob(myString))\n\n        fileList = list(chain.from_iterable(fileList))\n        fileList.sort()\n        print(fileList)\n        filesUsed = ''\n        for fName in fileList:\n            if (len(filesUsed) == 0):\n                filesUsed = fName\n            else:\n                filesUsed = filesUsed + ', ' + fName\n\n            sstFile = Dataset(fName)\n            sst = sstFile.variables[\"MWsstd\"][:, :, :, :]\n            sstFile.close()\n            sst = np.squeeze(sst)\n            mean, num = meanVar(mean, num, sst)\n\n        # From DOY 1 of end year through endDoy\n        os.chdir(dataDir)\n        fileList = []\n        doyRange = list(range(1, endDoy + 1))\n        for doy in doyRange:\n            doyC = str(doy)\n            doyC = doyC.rjust(3, '0')\n            myString = 'MW' + endyearC + doyC + '*' + dtype + '.nc'\n            fileList.append(glob.glob(myString))\n\n        fileList = list(chain.from_iterable(fileList))\n        fileList.sort()\n        print(fileList)\n\n        for fName in fileList:\n            if (len(filesUsed) == 0):\n                filesUsed = fName\n            else:\n                filesUsed = filesUsed + ', ' + fName\n\n            sstFile = Dataset(fName)\n            sst = sstFile.variables[\"MWsstd\"][:, :, :, :]\n            sstFile.close()\n            sst = np.squeeze(sst)\n            mean, num = meanVar(mean, num, sst)\n\n    # Mask out pixels with zero observations and set fill value for missing data\n    mean = ma.array(mean, mask=(num == 0), fill_value=-9999999.)\n\n    # Switch to the working directory\n    os.chdir(workDir)\n\n    # Construct output filename\n    outFile = 'MW' + startYearC + startDoyC + '_' + endyearC + endDoyC + '_' + dtype + '.nc'\n\n    # Generate the multi-day NetCDF file\n    ncFile = makeNetcdfmDay(mean, num, interval, outFile, filesUsed, workDir)\n\n    # Directory on the remote server where multi-day SST files are stored\n    remote_dir = '/MW/sstd/'\n\n    # Send the NetCDF to the remote server directory\n    send_to_servers(ncFile, remote_dir , 'm')",
    "crumbs": [
      "MODISA Scripts",
      "MW Datasets"
    ]
  },
  {
    "objectID": "mb_datasets.html",
    "href": "mb_datasets.html",
    "title": "MB Datasets",
    "section": "",
    "text": "Here you’ll find the MB (Pacific Ocean) processing scripts for both chlorophyll-a and SST.\nThey ingest raw Level-2 swath files, apply global coverage and quality filters, and grid valid observations into daily NetCDF products. These daily outputs are then averaged over rolling multi-day windows—automatically rebuilding composites whenever new data arrive.\nBelow are the individual scripts that form the MB workflow. Use the expandable code blocks to explore each stage of the pipeline.",
    "crumbs": [
      "MODISA Scripts",
      "MB Datasets"
    ]
  },
  {
    "objectID": "mb_datasets.html#chlorophyll",
    "href": "mb_datasets.html#chlorophyll",
    "title": "MB Datasets",
    "section": "Chlorophyll",
    "text": "Chlorophyll\n\nmakeChla1daynewMB\nmakeChla1daynewMB.py ingests Level-2 chlorophyll-a swath NetCDFs for the target day (HOD &gt; 10) and early hours of the next day (HOD ≤ 10), filters to “Day” pixels within longitude 120–320° and latitude –45–65°, and interpolates onto a 0.025° × 0.025°regular grid. The resulting 1-day composite is land-masked, written as a CF-compliant NetCDF, and deployed automatically to a remote server directory. Find the 1-day product on ERDDAP here.\nThe following chart summarizes the script’s workflow:\n\n\n\n\n\n%%{init: {\"flowchart\":{\"htmlLabels\":true}}}%%\nflowchart LR\n  I(\"&lt;b&gt;Inputs&lt;/b&gt;&lt;br/&gt;• Raw L2 Chla NetCDF swaths (dataDir)&lt;br/&gt;• workDir&lt;br/&gt;• Year & DOY\")\n    --&gt; P(\"&lt;b&gt;Processing&lt;/b&gt;&lt;br/&gt;• Compute dates & directories&lt;br/&gt;• Load land mask&lt;br/&gt;• Discover swaths for day & next-day&lt;br/&gt;• Stage & filter swaths (day-only, region overlap)&lt;br/&gt;• Extract, reshape & filter variables&lt;br/&gt;• Interpolate onto regular grid&lt;br/&gt;• Apply mask\")\n    --&gt; O(\"&lt;b&gt;Output&lt;/b&gt;&lt;br/&gt;• CF-compliant 1-day composite Chla NetCDF&lt;br/&gt;• Deployed to /MB/chla/1day/\")\n\n\n\n\n\n\n\n\nView makeChla1daynewMB.py\n\"\"\"\nOverview\n--------\nGenerate a one-day ocean-color (Chla) grid for the MODIS “MB” (Pacific Ocean) product by\ncombining swaths from Level-2 chlorophyll-a NetCDF files and gridding them with PyGMT. The\nscript produces a CF-compliant NetCDF file of daily Chla for the region (lon 120-320°, lat -45-65°).\n\nUsage\n-----\n::\n  \n     python makeChla1daynewMB.py &lt;dataDirBase&gt; &lt;workDir&gt; &lt;year&gt; &lt;doy&gt;\n\nWhere:\n\n- ``dataDir``\n\n  Root folder containing raw Level-2 ocean-color NetCDF swath files, organized as ``&lt;dataDirBase&gt;/&lt;YYYY&gt;&lt;MM&gt;/``. Each swath must match: ``AQUA_MODIS.&lt;YYYY&gt;&lt;MM&gt;&lt;DD&gt;*L2.OC.NRT.nc``.\n\n- ``workDir``\n\n  Temporary working directory for staging swath copies and intermediate files.\n\n- ``year``\n\n  Four-digit year string (e.g., ``\"2025\"``).\n\n- ``doy``\n\n  Three-digit day-of-year string (zero-padded, e.g., ``\"082\"`` for March 23).\n\nDescription\n-----------\n1. **Date and Directory Setup**  \n\n     - Computes calendar date from ``year`` + ``doy``, zero-pads month/day, and sets ``datadir = dataDirBase + YYYY + MM + \"/\"``.  \n     \n     - Also computes the next day's directory for early-morning swaths (``HOD ≤ 10``).\n\n2. **Load Land Mask**  \n\n     - Reads a static GRD land-mask from ``/u00/ref/landmasks/LM_120_320_0.025_-45_65_0.025_gridline.grd`` into ``my_mask``.\n\n3. **Swath Discovery & Staging**\n\n     - Globs ``AQUA_MODIS.&lt;YYYY&gt;&lt;MM&gt;&lt;DD&gt;*.L2.OC.NRT.nc`` in both current and next-day folders.\n\n     - Copies swaths into ``workDir``, removing any old ``AQUA_MODIS.*L2.OC*`` or ``MB20*`` files.\n\n4. **Swath Processing**\n\n     For each swath:\n\n         - Extracts navigation (``lon``, ``lat``), converts negative longitudes to 0-360°, and tests overlap with the region (lon 120-320°, lat -45-65°).\n\n     - Filters only daytime pixels (``day_night_flag == \"Day\"``).\n\n     - Reads chlorophyll-a (``chlor_a``), reshapes into column vectors, and stacks into ``(lon, lat, Chla)``.\n\n     - Applies sequential filters:  \n\n         • ``Chla &gt; 0``\n\n         • ``lonmin ≤ lon ≤ lonmax``\n\n         • ``latmin ≤ lat ≤ latmax``\n\n         • Discards any obviously invalid longitudes (``&gt; -400``).\n\n     - Accumulates valid points into ``temp_data`` and tracks provenance in ``filesUsed``.\n\n5. **Gridding & NetCDF Generation**  \n\n     - Uses ``pygmt.xyz2grd`` on ``temp_data`` with region ``120/320/-45/65`` and spacing ``0.025/0.025`` to produce a GMT ``.grd`` file named ``MB&lt;YYYY&gt;&lt;DDD&gt;_&lt;YYYY&gt;&lt;DDD&gt;_chla.grd``.\n     \n     - Converts the grid to a CF-compliant NetCDF via ``roylib.grd2netcdf1``, applying ``my_mask``.\n     \n     - Sends the final NetCDF into ``/MB/chla/1day/`` via ``roylib.send_to_servers``.\n\nDependencies\n------------\n- **Python 3.x**\n\n- **Standard library**: ``os``, ``sys``, ``datetime``, ``timedelta``, ``glob``, ``re``, ``shutil``\n\n- **Third-party**: ``netCDF4.Dataset``, ``numpy``, ``numpy.ma``, ``pygmt``  \n\n- **Custom roylib functions**:  \n\n     - ``myReshape(array)``\n\n     - ``grd2netcdf1(grd, outName, filesUsed, mask, fType)``\n\n     - ``safe_remove(filePath)``  \n\n     - ``send_to_servers(ncFile, destDir, interval)``\n\n     - ``isleap(year)``\n\n     - ``makeNetcdf(mean, nobs, interval, outFile, filesUsed, workDir)``\n\n     - ``meanVar(mean, num, obs)``\n\nLand Mask\n---------\nA static GRD mask is required at:\n\n ``/u00/ref/landmasks/LM_120_320_0.025_-45_65_0.025_gridline.grd``  \n\nto zero-out land pixels in the daily Chla grid.\n\nDirectory Structure\n-------------------\n- **Input directory** (datadir):\n\n  ``&lt;dataDirBase&gt;/&lt;YYYY&gt;&lt;MM&gt;/`` containing raw L2 OC files: ``AQUA_MODIS.&lt;YYYY&gt;&lt;MM&gt;&lt;DD&gt;*.L2.OC.NRT.nc``\n\n- **Working directory** (workDir):\n\n  Temporary staging area; cleared of ``AQUA_MODIS.*L2.OC*`` and ``MB20*`` at start.\n\n- **Output grid** (fileOut):\n\n  GMT “.grd” file named ``MB&lt;YYYY&gt;&lt;DDD&gt;_&lt;YYYY&gt;&lt;DDD&gt;_chla.grd``\n\n- **Final NetCDF**:\n\n  ``MB&lt;YYYY&gt;&lt;DDD&gt;_&lt;YYYY&gt;&lt;DDD&gt;_chla.nc``, copied to: ``/path/to/modis_data/modisgf/chla/1day/``\n\nUsage Example\n-------------\n::  \n  \n     python makeChla1daynewMB.py /Users/you/modis_data/netcdf/ /Users/you/modis_work/ 2025 082\n\nThis will:\n\n  - Download and stage all Level-2 OC swaths for March 23, 2025 (``HOD &gt; 10`` and next day ``HOD ≤ 10``).\n\n  - Build and grid the daily chlorophyll-a point cloud for the region.\n\n  - Generate CF-compliant NetCDF and deploy to ``/modisgf/chla/1day/``.\n\"\"\"\nfrom __future__ import print_function\nfrom builtins import str\n\nif __name__ == \"__main__\":\n    from datetime import datetime, timedelta\n    import glob\n    from itertools import chain\n    from netCDF4 import Dataset\n    import numpy as np\n    import numpy.ma as ma\n    import pygmt\n    import os\n    import re\n    import shutil\n    import sys\n\n    # Ensure 'roylib' is on the import path\n    sys.path.append('/home/cwatch/pythonLibs')\n    from roylib import *\n\n    # Geographic bounds for MB region\n    latmax = 65.\n    latmin = -45.\n    lonmax = 320.\n    lonmin = 120.\n\n    outFile = 'modisgfChlatemp'\n\n    # Set data directory\n    datadirBase = sys.argv[1]\n\n    # Set work directory\n    workdir = sys.argv[2]\n\n    # Get the year and doy from the command line\n    year = sys.argv[3]\n    doy = sys.argv[4]\n    print(year)\n    print(doy)\n\n    # Convert year/doy to a calendar date and zero-pad month/day\n    myDate = datetime(int(year), 1, 1) + timedelta(int(doy) - 1)\n    myMon = str(myDate.month)\n    myMon = myMon.rjust(2, '0')\n    myDay = str(myDate.day).rjust(2, '0')\n\n    # Construct the directory path where raw swaths are stored for this date\n    datadir = datadirBase + year + myMon + '/'\n\n    # Next calendar day (for hours ≤ 10)\n    myDate1 = myDate + timedelta(days=1)\n    year1 = str(myDate1.year)\n    doy1 = myDate1.strftime(\"%j\").zfill(3)\n    myMon1 = str(myDate1.month)\n    myMon1 = myMon1.rjust(2, '0')\n    myDay1 = str(myDate1.day).rjust(2, '0')\n    datadir1 = datadirBase + year1 + myMon1 + '/'\n\n    # Load static land mask from GRD\n    mask_root = Dataset('/u00/ref/landmasks/LM_120_320_0.025_-45_65_0.025_gridline.grd')\n    my_mask = mask_root.variables['z'][:, :]\n    mask_root.close()\n\n    # Now move to the data directory\n    os.chdir(datadir)\n\n    # Set up the string for the file search in the data directory\n    #myString = 'A' + year + doy + '*.L2_LAC_OC.nc'\n    myString = 'AQUA_MODIS.' + year + myMon + myDay + '*.L2.OC.NRT.nc'\n    print(myString)\n\n    # Get list of files in the data directory that match with full path\n    fileList = glob.glob(myString)\n    fileList.sort()\n\n    # Now move to the work directory and clear old files\n    os.chdir(workdir)\n    os.system('rm -f AQUA_MODIS.*L2.OC*')\n    os.system('rm -f MB20*')\n\n    # Initialize variables to accumulate data and track provenance\n    filesUsed = \"\"\n    temp_data = None\n\n    # Loop over swaths for hod &gt; 10 \n    for fName in fileList:\n        fileName = fName\n        # Extract hour of day from filename\n        datetime = re.search('AQUA_MODIS.(.+?).L2.OC.NRT.nc', fileName)\n        hodstr = datetime.group(1)[9:11]\n        hod = int(hodstr)\n\n        if (hod &gt; 10):\n            print(hod)\n            print(fileName)\n\n            # Stage the swath in the working directory\n            shutil.copyfile(datadir + fName, workdir + fName)\n\n            # Try to open the NetCDF; skip if the file is unreadable\n            try:\n                rootgrp = Dataset(fileName, 'r')\n            except IOError:\n                print(\"bad file \" + fileName)\n                continue\n\n            # Extract navigation-group data\n            navDataGroup = rootgrp.groups['navigation_data']\n            latitude = navDataGroup.variables['latitude'][:, :]\n            longitude = navDataGroup.variables['longitude'][:, :]\n\n            # Convert any negative longitudes to the 0-360° domain\n            longitude[longitude &lt; 0] = longitude[longitude &lt; 0] + 360\n\n            # Compute swath extents (min/max) for geographic filtering\n            dataLonMin = np.nanmin(longitude[longitude &gt;= 0])\n            dataLonMax = np.nanmax(longitude[longitude &lt;= 360])\n            dataLatMin = np.nanmin(latitude[latitude &gt;= -90])\n            dataLatMax = np.nanmax(latitude[latitude &lt;= 90])\n\n            # Determine if swath overlaps our the MW region\n            goodLon1 = (dataLonMin &lt; lonmin) and (dataLonMax &gt;= lonmin)\n            goodLon2 = (dataLonMin &gt;= lonmin) and (dataLonMin &lt;= lonmax)\n            goodLon = goodLon1 or goodLon2\n\n            goodLat1 = (dataLatMin &lt; latmin) and (dataLatMax &gt;= latmin)\n            goodLat2 = (dataLatMin &gt;= latmin) and (dataLatMin &lt;= latmax)\n            goodLat = goodLat1 or goodLat2\n\n            # Check if swath is daytime (only keep \"Day\" pixels)\n            dayNightTest = (rootgrp.day_night_flag == 'Day')\n\n            # Only proceed if geography and day-night tests pass\n            if (goodLon and goodLat and dayNightTest):\n            # if(goodLon and goodLat):\n                if (len(filesUsed) == 0):\n                    filesUsed = fileName\n                else:\n                    filesUsed = filesUsed + ', ' + fileName\n\n                # Reshape latitude & longitude arrays into column vectors\n                latitude = myReshape(latitude)\n                longitude = myReshape(longitude)\n\n                # Access geophysical data group\n                geoDataGroup = rootgrp.groups['geophysical_data']\n\n                # Extract chlor_a\n                chlor_a = geoDataGroup.variables['chlor_a'][:, :]\n                chlor_a = myReshape(chlor_a)\n\n                # Stack (lon, lat, chlor_a) into a single 2D array with shape (N, 3)\n                dataOut = np.hstack((longitude, latitude, chlor_a))\n\n                # Filter rows to keep only valid chlor_a (&gt;0)\n                dataOut = dataOut[dataOut[:, 2] &gt; 0]\n                dataOut = dataOut[dataOut[:, 0] &gt; -400]\n                dataOut = dataOut[dataOut[:, 0] &gt;= lonmin]\n                dataOut = dataOut[dataOut[:, 0] &lt;= lonmax]\n                dataOut = dataOut[dataOut[:, 1] &gt;= latmin]\n                dataOut = dataOut[dataOut[:, 1] &lt;= latmax]\n\n                # Accumulate into temp_data\n                if(dataOut.shape[0] &gt; 0):\n                    if(temp_data is None):\n                        temp_data = dataOut\n                    else:\n                        temp_data = np.concatenate((temp_data, dataOut), axis=0)\n\n            # Clean up this swath\n            rootgrp.close()\n            safe_remove(fileName)\n\n    # Repeat for hod ≤ 10 on the next day\n\n    # Move back to data dir\n    os.chdir(datadir1)\n\n    # Set up the string for file matching of doy+1\n    myString = 'AQUA_MODIS.' + year1 + myMon1 + myDay1 + '*.L2.OC.NRT.nc'\n    fileList = glob.glob(myString)\n    fileList.sort()\n\n    # Change back to work directory\n    os.chdir(workdir)\n\n    for fName in fileList:\n        fileName = fName\n\n        # Find the datatime group in the filename\n        datetime = re.search('AQUA_MODIS.(.+?).L2.OC.NRT.nc', fileName)\n        hodstr = datetime.group(1)[9:11]\n        hod = int(hodstr)\n        if (hod &lt;= 10):\n            print(hod)\n            print(fileName)\n\n            # cp file from work directory to\n            shutil.copyfile(datadir1 + fName, workdir + fName)\n            try:\n                rootgrp = Dataset(fileName, 'r')\n            except IOError:\n                print(\"bad file \" + fileName)\n                continue\n\n            #rootgrp = Dataset(fileName, 'r')\n\n            navDataGroup = rootgrp.groups['navigation_data']\n            latitude = navDataGroup.variables['latitude'][:, :]\n            longitude = navDataGroup.variables['longitude'][:, :]\n\n            longitude[longitude &lt; 0] = longitude[longitude &lt; 0] + 360\n\n            dataLonMin = np.nanmin(longitude[longitude &gt;= 0])\n            dataLonMax = np.nanmax(longitude[longitude &lt;= 360])\n            dataLatMin = np.nanmin(latitude[latitude &gt;= -90] )\n            dataLatMax = np.nanmax(latitude[latitude &lt;= 90] )\n\n            goodLon1 = (dataLonMin &lt; lonmin) and ( dataLonMax &gt;= lonmin)\n            goodLon2 = (dataLonMin &gt;= lonmin) and (dataLonMin &lt;= lonmax)\n            goodLon = goodLon1 or goodLon2\n\n            goodLat1 = (dataLatMin &lt; latmin) and (dataLatMax &gt;= latmin)\n            goodLat2 = (dataLatMin &gt;= latmin) and (dataLatMin &lt;= latmax)\n            goodLat = goodLat1 or goodLat2\n\n            dayNightTest = (rootgrp.day_night_flag == 'Day')\n\n            if (goodLon and goodLat and dayNightTest):\n                if (len(filesUsed) == 0):\n                    filesUsed = fileName\n                else:\n                    filesUsed = filesUsed + ', ' + fileName\n\n                latitude = myReshape(latitude)\n                longitude = myReshape(longitude)\n\n                geoDataGroup = rootgrp.groups['geophysical_data']\n\n                chlor_a = geoDataGroup.variables['chlor_a'][:, :]\n                chlor_a = myReshape(chlor_a)\n\n                dataOut = np.hstack((longitude, latitude, chlor_a))\n\n                dataOut = dataOut[dataOut[:, 2] &gt; 0]\n                dataOut = dataOut[dataOut[:, 0] &gt; -400]\n                dataOut = dataOut[dataOut[:, 0] &gt;= lonmin]\n                dataOut = dataOut[dataOut[:, 0] &lt;= lonmax]\n                dataOut = dataOut[dataOut[:, 1] &gt;= latmin]\n                dataOut = dataOut[dataOut[:, 1] &lt;= latmax]\n\n                if(dataOut.shape[0] &gt; 0):\n                    if(temp_data is None):\n                        temp_data = dataOut\n                    else:\n                        temp_data = np.concatenate((temp_data, dataOut), axis=0)\n\n            rootgrp.close()\n            safe_remove(fileName)\n\n    # Grid the combined Chla point cloud and write outputs\n    fileOut = 'MB' + year + doy + '_' + year + doy + '_chla.grd'\n    range = '120/320/-45/65'\n    increment = '0.025/0.025'\n    temp_data1 = pygmt.xyz2grd(\n        data=temp_data,\n        region=range,\n        spacing=increment,\n    )\n\n    # Convert the GMT grid to CF-compliant NetCDF, masking land\n    ncFile = grd2netcdf1(temp_data1, fileOut, filesUsed, my_mask, 'MB')\n\n    #myCmd = \"mv \" + ncFile + \" /home/cwatch/pygmt_test/outfiles\"\n    #os.system(myCmd)\n\n    # Copy to the MB server folder for chla (1-day product)\n    send_to_servers(ncFile, '/MB/chla/' , '1')\n    os.remove(ncFile)\n\n\n\n\nCompMBChla\nCompMBChla.py reads the daily 1-day chlorophyll-a NetCDF files over a user-defined interval, accumulates running sum and count arrays on a 0.025° × 0.025° regular grid spanning longitude 120°–320° and latitude –45°–65°, then masks out cells with no observations and writes the averaged multi-day (3-, 5-, 8-, or 14-day) composite as a CF-compliant NetCDF. It finishes by uploading the final composite to a directory on a remote server. Find the multi-day products on ERDDAP here.\nThe following chart summarizes the script’s workflow:\n\n\n\n\n\n%%{init: {\"flowchart\":{\"htmlLabels\":true}}}%%\nflowchart LR\n  I(\"&lt;b&gt;Inputs&lt;/b&gt;&lt;br/&gt;• Daily 1-day Chla NetCDFs (dataDir)&lt;br/&gt;• workDir&lt;br/&gt;• endYear & endDoy&lt;br/&gt;• interval (days)\")\n    --&gt; P(\"&lt;b&gt;Processing&lt;/b&gt;&lt;br/&gt;• Compute start & end dates&lt;br/&gt;• Initialize mean/num arrays&lt;br/&gt;• Gather daily files over interval&lt;br/&gt;• Loop: open each file, extract & squeeze variable&lt;br/&gt;• Update running mean & count&lt;br/&gt;• Mask pixels with zero observations\")\n    --&gt; O(\"&lt;b&gt;Output&lt;/b&gt;&lt;br/&gt;• CF-compliant multi-day composite Chla NetCDF&lt;br/&gt;• Deployed to /MB/chla/\")\n\n\n\n\n\n\n\n\nView CompMBChla.py\n\"\"\"\nOverview\n--------\nCompute multi-day (m-day) chlorophyll-a composites for the MODIS “MB” (Pacific Ocean)\ndataset by averaging 1-day NetCDF files over a specified range of days.\n\nUsage\n-----\n::\n  \n      python CompMBChla.py &lt;dataDir&gt; &lt;workDir&gt; &lt;endYear&gt; &lt;endDoy&gt; &lt;interval&gt;\n\nWhere:\n\n- ``dataDir``\n\n  Directory containing daily 1-day NetCDFs named ``MB&lt;YYYY&gt;&lt;DDD&gt;*chla.nc``.\n\n- ``workDir``\n\n  Temporary working directory for intermediate files.\n\n- ``endYear``\n\n  Four-digit year of the last day in the composite (e.g., ``2025``).\n\n- ``endDoy``\n\n  Three-digit day-of-year of the last day (e.g., ``082``).\n\n- ``interval``\n\n  Number of days to include (e.g., `3`, `5`, `8`, `14`).\n\n\nDescription\n-----------\n1. **Parse arguments & compute interval**\n\n  - Read ``dataDir``, ``workDir``, ``endYear``, ``endDoy``, and ``interval`` from ``sys.argv``.\n\n  - Compute ``startDoy = endDoy - interval + 1``.\n\n2. **Compute start/end dates**\n\n  - Convert ``endYear`` + ``endDoy`` to a ``datetime``.\n\n  - Derive ``startDoy`` date by subtracting ``interval - 1`` days.\n\n3. **Initialize accumulators**\n\n  - Change to ``workDir`` and clear any old files.\n\n  - Preallocate two arrays of shape (4401x8001):\n\n     - ``mean`` (float32) for the running sum of chlorophyll-a.\n\n     - ``num``  (int32) for the count of valid observations.\n\n4. **Gather daily files**\n\n  - Build a list of all ``MB&lt;YYYY&gt;&lt;DDD&gt;*chla.nc`` files from ``startDoy`` to ``endDoy``.\n\n  - Handle wrap-around year boundaries by splitting into two ranges if needed.\n\n5. **Accumulate daily Chla**\n  \n  - For each file:\n\n     - Open with ``Dataset()``, read the 4-D variable ``MBchla``, squeeze to 2-D.\n\n     - Update ``mean`` and ``num`` via ``meanVar(mean, num, data2d)``.\n\n6. **Mask & finalize**\n\n  - Create a masked array where ``num == 0`` (no observations), fill value ``-9999999.``.\n\n7. **Write & deploy** \n\n  - Change to ``workDir``.\n  \n  - Construct output filename ``MB&lt;startYear&gt;&lt;startDDD&gt;_&lt;endYear&gt;&lt;endDDD&gt;_chla.nc``.\n  \n  - Call ``makeNetcdf(mean, num, interval, outFile, filesUsed, workDir)`` to write the CF-compliant NetCDF.\n  \n  - Transfer it to ``/MB/chla/`` on the server via ``send_to_servers()``.\n\nDependencies\n------------\n- Python 3.x\n\n- Standard library: ``os``, ``sys``, ``glob``, ``itertools.chain``, ``datetime``, ``timedelta``\n\n- Third-party: ``numpy``, ``numpy.ma``, ``netCDF4``\n\n- Custom roylib functions:\n\n  - ``isleap(year)``\n\n  - ``meanVar(mean, num, array)``\n\n  - ``makeNetcdf(mean, num, interval, outFile, filesUsed, workDir)``\n\n  - ``send_to_servers(ncFile, remote_dir, interval_flag)``\n\nDirectory Structure\n-------------------\n- **Input Directory** (``dataDir``):\n\n  ``MB&lt;YYYY&gt;&lt;DDD&gt;*chla.nc`` daily files.\n\n- **Working Directory** (``workDir``):\n\n  Temporary staging and output location.\n\n- **Output location**:\n\n  Copied to ``/MB/chla/`` with the interval flag.\n\nUsage Example\n-------------\nCreate a 5-day chlorophyll composite DOY 001-005 of 2025\n::\n \n   python CompMBChla.py /data/modisgf/1day/ /tmp/mw_work/ 2025 005 5\n\nThis averages files for DOY 001…005, writes ``MB2025001_2025005_chla.nc`` in /tmp/mw_work/, and uploads to /MB/chla/.\n\"\"\"\nfrom __future__ import print_function\nfrom builtins import str\nfrom builtins import range\n\nif __name__ == \"__main__\":\n    from datetime import datetime, timedelta\n    import glob\n    from itertools import chain\n    from netCDF4 import Dataset\n    import numpy as np\n    import numpy.ma as ma\n    import os\n    import sys\n\n    # Ensure 'roylib' is on the import path\n    sys.path.append('/home/cwatch/pythonLibs')\n    from roylib import *\n\n    dataDir = sys.argv[1]\n\n    # Temporary working directory\n    workDir = sys.argv[2]\n\n    # Composite end year (YYYY)\n    endyearC = sys.argv[3]\n\n    # Composite end day-of-year (DDD)\n    endDoyC = sys.argv[4]\n    endDoyC = endDoyC.rjust(3, '0')\n    endDoy = int(endDoyC)\n\n    # Composite length\n    intervalC = sys.argv[5]\n    interval = int(intervalC)\n\n    # Convert end Doy to calendar date\n    myDateEnd = datetime(int(endyearC), 1, 1) + timedelta(int(endDoyC)-1)\n\n    # Start date = end date minus (interval-1) days\n    myDateStart = myDateEnd + timedelta(days=-(interval - 1))\n\n    # Zero-padded start Doy and year\n    startDoyC = myDateStart.strftime(\"%j\").zfill(3)\n    startDoy = int(startDoyC)\n    startYearC = str(myDateStart.year)\n\n    # Prepare output directory\n    outDir = '/ERDData1/modisa/data/modisgf/' + endyearC + '/'+ intervalC + 'day'\n\n    print(dataDir)\n    print(workDir)\n    print(endyearC)\n    print(endDoyC)\n    print(intervalC)\n\n    ###\n    # dtypeList = ['chla']\n    # for dtype in dtypeList:\n\n    # Data type for composites\n    dtype = 'chla'\n\n    # Clean working directory\n    os.chdir(workDir)\n    os.system('rm -f *')\n\n    # Change to data directory\n    os.chdir(dataDir)\n\n    # Preallocate sum (mean) and count arrays matching grid dims\n    mean = np.zeros((4401, 8001), np.single)\n    num = np.zeros((4401, 8001), dtype=np.int32)\n\n    # Composite within the same calendar year\n    if (endDoy &gt; startDoy):\n        # Build range of Doys\n        doyRange = list(range(startDoy, endDoy + 1))\n        fileList = []\n        # Gather filenames for each DOY\n        for doy in doyRange:\n            doyC = str(doy)\n            doyC = doyC.rjust(3, '0')\n            myString = 'MB' + endyearC + doyC + '*' + dtype + '.nc'\n            fileList.append(glob.glob(myString))\n\n        # Flatten and sort file list\n        fileList = list(chain.from_iterable(fileList))\n        fileList.sort()\n        filesUsed = \"\"\n        print(fileList)\n\n        # Loop through files and accumulate data\n        for fName in fileList:\n            if (len(filesUsed) == 0):\n                filesUsed = fName\n            else:\n                filesUsed = filesUsed + ', ' + fName\n\n            chlaFile = Dataset(fName)\n            chla = chlaFile.variables[\"MBchla\"][:, :, :, :]\n            chlaFile.close()\n            chla = np.squeeze(chla)\n            mean, num = meanVar(mean, num, chla)\n    else:\n        # Composite spans year boundary\n        # Determine directory for the start year\n        dataDir1 = dataDir\n        dataDir1 = dataDir1.replace(endyearC, startYearC)\n        # Determine end of start year based on start year\n        if (isleap):\n            endday = 366\n        else:\n            endday = 365\n\n        fileList = []\n        os.chdir(dataDir1)\n        doyRange = list(range(startDoy, endday + 1))\n        for doy in doyRange:\n            doyC = str(doy)\n            doyC = doyC.rjust(3, '0')\n            myString = 'MB' + startYearC + doyC + '*' + dtype + '.nc'\n            fileList.append(glob.glob(myString))\n\n        fileList = list(chain.from_iterable(fileList))\n        fileList.sort()\n        filesUsed = \"\"\n        print(fileList)\n        for fName in fileList:\n            if (len(filesUsed) == 0):\n                filesUsed = fName\n            else:\n                filesUsed = filesUsed + ', ' + fName\n\n            chlaFile = Dataset(fName)\n            chla = chlaFile.variables[\"MBchla\"][:, :, :, :]\n            chlaFile.close()\n            chla = np.squeeze(chla)\n            mean, num = meanVar(mean, num, chla)\n\n        # DOY from 1 to endDoy of end year\n        os.chdir(dataDir)\n        fileList = []\n        doyRange = list(range(1, endDoy + 1))\n        for doy in doyRange:\n            doyC = str(doy)\n            doyC = doyC.rjust(3, '0')\n            myString = 'MB' + endyearC + doyC + '*' + dtype + '.nc'\n            fileList.append(glob.glob(myString))\n\n        fileList = list(chain.from_iterable(fileList))\n        fileList.sort()\n        print(fileList)\n        for fName in fileList:\n            if (len(filesUsed) == 0):\n                filesUsed = fName\n            else:\n                filesUsed = filesUsed + ', ' + fName\n\n            # Read the NetCDF, variable 'MBchla' and squeeze to make 2D array (lat x lon)\n            chlaFile = Dataset(fName)\n            chla = chlaFile.variables[\"MBchla\"][:, :, :, :]\n            chlaFile.close()\n            chla = np.squeeze(chla)\n\n            # Update running sum and count arrays\n            mean, num = meanVar(mean, num, chla)\n\n    # Mask out any grid cells with zero observations, setting them to the fill value\n    mean = ma.array(mean, mask=(num==0), fill_value=-9999999.)\n\n    # Switch to the working directory for output operations\n    os.chdir(workDir)\n\n    # Construct the output filename with start and end dates plus data types\n    outFile = 'MB' + startYearC + startDoyC + '_' + endyearC + endDoyC + '_' + dtype + '.nc'\n\n    # Create multi-day NetCDF file using the mean and count arrays\n    ncFile = makeNetcdf(mean, num, interval, outFile, filesUsed, workDir)\n\n    # Upload composite NetCDF to remote MB chla directory, labeling it with the interval\n    send_to_servers(ncFile, '/MB/chla/', str(interval))\n\n\n\n\nCompMBChlamday\nCompMBChlamday.py ingests 1-day composite Chla NetCDF files over user-defined date range and maps them onto a 0.025° × 0.025° regular grid spanning longitude 120°–320° and latitude –45°–65°, accumulating sums and counts to compute the monthly mean. It then masks out points with no observations, writes the result as a CF-compliant NetCDF, and uploads it to a directory on a remote server. Find the monthly product on ERDDAP here.\nThe following chart summarizes the script’s workflow:\n\n\n\n\n\n%%{init: { \"flowchart\": { \"htmlLabels\": true } }}%%\nflowchart LR\n  I(\"&lt;b&gt;Inputs&lt;/b&gt;&lt;br/&gt;• 1-day Chla NetCDFs (dataDir)&lt;br/&gt;• workDir&lt;br/&gt;• endYear & endDoy&lt;br/&gt;• startDoy\")\n    --&gt; P(\"&lt;b&gt;Processing&lt;/b&gt;&lt;br/&gt;• Parse args & compute interval&lt;br/&gt;• Compute start/end dates&lt;br/&gt;• Clear workDir&lt;br/&gt;• Initialize mean & count arrays&lt;br/&gt;• Gather files over date range&lt;br/&gt;• Loop: read each file & update mean/num&lt;br/&gt;• Mask pixels with no observations\")\n    --&gt; O(\"&lt;b&gt;Output&lt;/b&gt;&lt;br/&gt;• Monthly composite Chla NetCDF&lt;br/&gt;• Deployed to /MB/chla/\")\n\n\n\n\n\n\n\n\nView CompMBChlamday.py\n\"\"\"\nOverview\n--------\nCompute monthly composites of chlorophyll-a (Chla) for the MODIS “MB” (Pacific Ocean)\ndataset by averaging daily 1-day NetCDF files over a specified interval of days.\n\nUsage\n-----\n::\n  \n    python CompMBChlamday.py &lt;dataDir&gt; &lt;workDir&gt; &lt;endYear&gt; &lt;endDoy&gt; &lt;startDoy&gt;\n\nWhere:\n\n- ``dataDir``\n\n  Directory containing daily 1-day NetCDF files named ``MB&lt;YYYY&gt;&lt;DDD&gt;*chla.nc``.\n\n- ``workDir``\n\n  Temporary working directory for intermediate files\n\n- ``endYear`` \n\n  Four-digit year of the last day in the composite (e.g., ``2025``).\n\n- ``endDoy``\n\n  Three-digit day-of-year of the last composite day (e.g., ``031``).\n\n- ``startDoy``\n\n  Three-digit day-of-year of the first composite day (e.g., ``001``).\n\nDescription\n-----------\n1. **Parse arguments & compute interval**\n\n  - Read ``dataDir``, ``workDir``, ``endYear``, ``endDoy``, and ``startDoy``.\n\n  - Compute ``interval = endDoy - startDoy + 1``.\n\n2. **Compute calendar dates**\n\n  - Convert ``endYear`` + ``endDoy`` to a ``datetime`` (``myDateEnd``).\n\n  - Derive ``myDateStart = myDateEnd - (interval - 1) days``.\n\n3. **Initialize accumulators**\n\n  - Clear ``workDir``.\n\n  - Preallocate two arrays of shape (4401x8001):\n\n     - ``mean`` (float32) for the running sum of Chla.\n\n     - ``num``  (int32) for the count of observations.\n\n4. **Gather daily files**\n\n  - Build a list of all ``MB&lt;YYYY&gt;&lt;DDD&gt;*chla.nc`` files from ``startDoy`` to ``endDoy``.\n\n  - Handle year-boundary spans by splitting into two date ranges if necessary.\n\n5. **Accumulate daily Chla**\n\n  - For each file:\n\n     - Open via ``Dataset()``.\n\n     - Read the 4-D variable ``MBchla``, squeeze to 2-D.\n\n     - Update ``mean`` and ``num`` via ``meanVar(mean, num, data2d)``.\n\n6. **Mask & finalize**\n\n  - Create a masked array: mask pixels where ``num == 0`` (no observations) and set fill value ``-9999999.``.\n\n7. **Write & deploy**\n\n  - Change to ``workDir``.\n\n  - Construct output filename ``MB&lt;startYear&gt;&lt;startDoy&gt;_&lt;endYear&gt;&lt;endDoy&gt;_chla.nc``.\n\n  - Call ``makeNetcdfmDay(mean, num, interval, outFile, filesUsed, workDir)`` to produce a CF-compliant NetCDF.\n\n  - Transfer it to ``/MB/chla/`` on the remote server via ``send_to_servers()``.\n\nDependencies\n------------\n- Python 3.x\n\n- Standard library: ``os``, ``sys``, ``glob``, ``itertools.chain``, ``datetime``, ``timedelta``\n\n- Third-party: ``numpy``, ``numpy.ma``, ``netCDF4.Dataset``\n\n- Custom roylib functions:\n\n  - ``isleap(year)``\n\n  - ``meanVar(mean, num, data)``\n\n  - ``makeNetcdfmDay(mean, num, interval, outFile, filesUsed, workDir)``\n\n  - ``send_to_servers(ncFile, remote_dir, interval_flag)``\n\n\nDirectory Structure\n-------------------\n- **Input Directory** (dataDir):\n\n  Contains daily files ``MB&lt;YYYY&gt;&lt;DDD&gt;*chla.nc``.\n\n- **Working Directory** (workDir):\n\n  Temporary space cleared and reused each run.\n\n- **Output Location**:\n\n  Final monthly composite sent to ``/MB/chla/``.\n\nUsage Example\n-------------\nGenerate a January 2025 composite (DOY 001-031):\n\n::\n \n   python CompMBChlamday.py /data/modisgf/1day/ /tmp/mw_work/ 2025 031 001\n\nThis will average daily Chla for DOY 001…031 of 2025, write ``MB2025001_2025031_chla.nc`` in ``/tmp/mw_work/``, and upload to ``/MB/chla/``.\n\"\"\"\nfrom __future__ import print_function\nfrom builtins import str\nfrom builtins import range\n\nif __name__ == \"__main__\":\n    from datetime import datetime, timedelta\n    import glob\n    from itertools import chain\n    from netCDF4 import Dataset\n    import numpy as np\n    import numpy.ma as ma\n    import os\n    import sys\n\n    # Ensure 'roylib' is on the import path\n    sys.path.append('/home/cwatch/pythonLibs')\n    from roylib import *\n\n    # Directory with 1-day MB chl files\n    dataDir = sys.argv[1]\n\n    # Temporary working directory\n    workDir = sys.argv[2]\n\n    # Composite end year\n    endyearC = sys.argv[3]\n\n    # Same year for start\n    startYearC = endyearC\n\n    # Zero-padded end DOY\n    endDoyC = sys.argv[4]\n    endDoyC = endDoyC.rjust(3, '0')\n    endDoy = int(endDoyC)\n\n    # Zero-padded start DOY\n    startDoyC = sys.argv[5]\n    startDoyC = startDoyC.rjust(3, '0')\n    startDoy = int(startDoyC )\n\n    # Number of days in the composite\n    interval = endDoy - startDoy + 1\n\n    # Convert end date to calendar dates\n    myDateEnd = datetime(int(endyearC), 1, 1) + timedelta(endDoy - 1)\n    myDateStart = myDateEnd + timedelta(days=-(interval - 1))\n\n    # Prepare output directory\n    outDir = '/ERDData1/modisa/data/modisgf/' + endyearC + '/mday'\n\n    print(dataDir)\n    print(workDir)\n    print(endyearC)\n    print(endDoyC)\n    print(interval)\n\n    ###\n    # dtypeList = ['chla']\n    # for dtype in dtypeList:\n\n    # Data type for this composite run\n    dtype = 'chla'\n\n    # Clear the working directory\n    os.chdir(workDir)\n    os.system('rm -f *')\n\n    # Move to data directory\n    os.chdir(dataDir)\n\n    # Preallocate sum (mean) and count arrays matching grid dims\n    mean = np.zeros((4401, 8001), np.single)\n    num = np.zeros((4401, 8001), dtype=np.int32)\n\n    # Composite entirely within the same year\n    if (endDoy &gt; startDoy):\n        doyRange = list(range(startDoy, endDoy + 1))\n        fileList = []\n        # Collect all matching files for each DOY\n        for doy in doyRange:\n            doyC = str(doy)\n            doyC = doyC.rjust(3, '0')\n            myString = 'MB' + endyearC + doyC + '*' + dtype + '.nc'\n            fileList.append(glob.glob(myString))\n\n        # Flatten and sort the list of lists\n        fileList = list(chain.from_iterable(fileList))\n        fileList.sort()\n        filesUsed = \"\"\n        print(fileList)\n\n        # Loop through each file\n        for fName in fileList:\n            if (len(filesUsed) == 0):\n                filesUsed = fName\n            else:\n                filesUsed = filesUsed + ', ' + fName\n\n            chlaFile = Dataset(fName)\n            chla = chlaFile.variables[\"MBchla\"][:, :, :, :]\n            chlaFile.close()\n            chla = np.squeeze(chla)\n\n            # Update running mean and count arrays\n            mean, num = meanVar(mean, num, chla)\n    else:\n        # Composite wraps across year boundary\n        dataDir1 = dataDir\n        dataDir1 = dataDir1.replace(endyearC, startYearC)\n        # Determine last DOY of start year based on leap year\n        if(isleap):\n            endday = 366\n        else:\n            endday = 365\n\n        # From startDoy through end of start year  \n        fileList = []\n        os.chdir(dataDir1)\n        doyRange = list(range(startDoy, endday + 1))\n        for doy in doyRange:\n            doyC = str(doy)\n            doyC = doyC.rjust(3, '0')\n            myString = 'MB' + startYearC + doyC + '*' + dtype + '.nc'\n            fileList.append(glob.glob(myString))\n\n        fileList = list(chain.from_iterable(fileList))\n        fileList.sort()\n\n        filesUsed = \"\"\n        print(fileList)\n        for fName in fileList:\n            if (len(filesUsed) == 0):\n                filesUsed = fName\n            else:\n                filesUsed = filesUsed + ', ' + fName\n\n            chlaFile = Dataset(fName)\n            chla = chlaFile.variables[\"MBchla\"][:, :, :, :]\n            chlaFile.close()\n            chla = np.squeeze(chla)\n            mean, num = meanVar(mean, num, chla)\n\n        # From DOY 1 of end year through endDoy\n        os.chdir(dataDir)\n        fileList = []\n        doyRange= list(range(1, endDoy + 1))\n        for doy in doyRange:\n            doyC = str(doy)\n            doyC = doyC.rjust(3, '0')\n            myString = 'MB' + endyearC + doyC + '*' + dtype + '.nc'\n            fileList.append(glob.glob(myString))\n\n        fileList = list(chain.from_iterable(fileList))\n        fileList.sort()\n        print(fileList)\n        for fName in fileList:\n            if (len(filesUsed) == 0):\n                filesUsed = fName\n            else:\n                filesUsed = filesUsed + ', ' + fName\n\n            chlaFile = Dataset(fName)\n            chla = chlaFile.variables[\"MBchla\"][:, :, :, :]\n            chlaFile.close()\n            chla = np.squeeze(chla)\n            mean, num = meanVar(mean, num, chla)\n\n    # Mask out pixels with zero observations and set fill value for missing data\n    mean = ma.array(mean, mask=(num == 0), fill_value=-9999999.)\n\n    # Switch to the working directory\n    os.chdir(workDir)\n\n    # Construct output filename\n    outFile = 'MB' + startYearC + startDoyC + '_' + endyearC + endDoyC + '_' + dtype + '.nc'\n\n    # Generate the monthly NetCDF file\n    ncFile = makeNetcdfmDay(mean, num, interval, outFile, filesUsed, workDir)\n\n    # Upload the composite to remote MB chla directory\n    send_to_servers(ncFile, '/MB/chla/', 'm')",
    "crumbs": [
      "MODISA Scripts",
      "MB Datasets"
    ]
  },
  {
    "objectID": "mb_datasets.html#sst",
    "href": "mb_datasets.html#sst",
    "title": "MB Datasets",
    "section": "SST",
    "text": "SST\n\nmakeSST1daynewMB\nmakeSST1daynewMB makes a 1-day SST composite for the MB dataset by aggregating valid Level-2 swaths from the target day (HOD &gt; 10) and early hours of the following day (HOD ≤ 10), then interpolating onto a 0.025° × 0.025° regular grid spanning longitude 120–320° and latitude –45–65° via PyGMT. It writes out a CF-compliant NetCDF, masks land using a static grid, computes coverage statistics, and uploads the resulting file to a directory on the remote server. Find the 1-day product on ERDDAP here.\nThe following chart summarizes the script’s workflow:\n\n\n\n\n\n%%{init: {\"flowchart\":{\"htmlLabels\":true}}}%%\nflowchart LR\n  I(\"&lt;b&gt;Inputs&lt;/b&gt;&lt;br/&gt;• Raw L2 SST swaths (dataDir)&lt;br/&gt;• workDir&lt;br/&gt;• Year & DOY\")\n    --&gt; P(\"&lt;b&gt;Processing&lt;/b&gt;&lt;br/&gt;• Compute dates & directories&lt;br/&gt;• Load land mask&lt;br/&gt;• Discover swaths for day & next-day&lt;br/&gt;• Stage & filter swaths (day-only, region overlap)&lt;br/&gt;• Extract, reshape & filter variables&lt;br/&gt;• Interpolate onto regular grid&lt;br/&gt;• Apply mask\")\n    --&gt; O(\"&lt;b&gt;Output&lt;/b&gt;&lt;br/&gt;• CF-compliant 1-day composite SST NetCDF&lt;br/&gt;• Deployed to /MB/sstd/1day/\")\n\n\n\n\n\n\n\n\nView makeSST1daynewMB.py\n\"\"\"\nOverview\n--------\nGenerate a one-day SST grid for the MODIS “MB” (Pacific Ocean) dataset by combining swaths\nfrom Level-2 SST NetCDF files over a two-day interval (current day and early hours of the\nnext day). The output is a CF-compliant NetCDF file containing daily SST on a regular grid\n(lon 120-320°, lat -45-65°).\n\nUsage\n-----\n::\n\n    python makeSST1daynewMB.py &lt;dataDir&gt; &lt;workDir&gt; &lt;year&gt; &lt;doy&gt;\n\nWhere:\n\n- ``dataDir``\n\n  Base directory containing raw Level-2 SST NetCDF swaths, organized as ``&lt;dataDirBase&gt;/&lt;YYYY&gt;&lt;MM&gt;/``. Each file must match ``AQUA_MODIS.&lt;YYYY&gt;&lt;MM&gt;&lt;DD&gt;T*.L2.SST.NRT.nc``.\n\n- ``workDir``\n\n  Temporary working directory. Swaths are copied here, intermediate files are created, and then cleaned up.\n\n- ``year``\n\n  Four-digit year string (e.g., ``\"2025\"``).\n\n- ``doy``\n\n  Three-digit day-of-year string (zero-padded, e.g., ``\"082\"`` for March 23).\n\nDescription\n-----------\n1. **Date and Directory Setup**\n\n     - Convert ``year`` + ``doy`` to a calendar date (``myDate``), then extract zero-padded month (``myMon``) and day (``myDay``).\n     \n     - Compute next-day date (``myDate1``) and its year/month/day-of-year (``year1``, ``myMon1``, ``doy1``).\n     \n     Define:\n\n         - ``datadir = dataDirBase + year + myMon + \"/\"``  \n         - ``datadir1 = dataDirBase + year1 + myMon1 + \"/\"``\n\n     These folders must contain the SST swath files for the current day and next day, respectively.\n     \n     - Load a static land-mask grid from ``/u00/ref/landmasks/LM_120_320_0.025_-45_65_0.025_gridline.grd`` into ``my_mask``.\n\n2. **Swath Discovery (Day N, HOD &gt; 10)**  \n\n     - Change directory to ``datadir``.\n     \n     - Build glob pattern: ``AQUA_MODIS.&lt;year&gt;&lt;myMon&gt;&lt;myDay&gt;*.L2.SST.NRT.nc``\n     \n     - Sort matching files into ``fileList``.\n     \n     - Change into ``workDir``, then remove any stale files matching ``AQUA_MODIS.*L2.SST*`` or ``MB20*``.\n\n3. **Loop Over Each Swath for Day N (HOD &gt; 10)**  \n\n     For each ``fName`` in ``fileList``:\n\n     a. Extract hour-of-day (HOD) from the filename.\n     \n     b. If ``HOD &gt; 10``, copy the swath from ``datadir`` to ``workDir`` and open with ``netCDF4.Dataset`` (skip if unreadable).\n      \n     c. Read ``navigation_data`` (``latitude``, ``longitude``), convert negative longitudes to 0-360°. Compute swath extents and test geographic overlap:  \n\n         - Longitude overlaps [120°, 320°]\n         \n         - Latitude overlaps [-45°, 65°]\n         \n         - ``day_night_flag == \"Day\"``\n         \n         If all true, append the filename to ``filesUsed``.\n\n      d. Reshape variables using ``myReshape`` and read from ``geophysical_data``:\n\n         - ``sst`` (sea surface temperature)\n         \n         - ``qual_sst`` (quality flag)\n        \n         Flatten quality flags, stack ``longitude``, ``latitude``, ``sst`` into ``dataOut``, and filter:\n         \n           - ``qual_sst &lt; 3``\n         \n           - SST &gt; -2 °C\n         \n           - Longitude between 120° and 320°\n         \n           - Latitude ≤ 65°\n         \n         Accumulate valid points into ``temp_data``.\n\n   e. Close the NetCDF and remove the copied swath from ``workDir`` using ``safe_remove``.\n\n4. **Swath Discovery (Day N+1, HOD ≤ 10)**\n\n     - Change directory to ``datadir1``.\n     \n     - Build glob pattern: ``AQUA_MODIS.&lt;year1&gt;&lt;myMon1&gt;&lt;myDay1&gt;*.L2.SST.NRT.nc``\n     \n     - Sort into ``fileList``.\n     \n     - Change back into ``workDir``.\n\n5. **Loop Over Each Swath for Day N+1 (HOD ≤ 10)**\n\n     For each ``fName`` in ``fileList``:\n     \n       - Extract HOD; if ``HOD ≤ 10``, copy swath from ``datadir1`` to ``workDir`` and repeat steps 3b-3e (navigation, quality/filter, accumulate into ``temp_data``).\n\n6. **Gridding Swath Point Cloud**\n\n     - After processing both sets of swaths, define output grid filename: ``MB&lt;year&gt;&lt;doy&gt;_&lt;year&gt;&lt;doy&gt;_sstd.grd``\n\n     - Set PyGMT region and spacing:\n\n         - ``region = \"120/320/-45/65\"``\n\n         - ``spacing = \"0.025/0.025\"``\n\n     - Call:\n\n     ::\n\n         pygmt.xyz2grd(\n             data=temp_data,\n             region=region,\n             spacing=spacing\n         )\n\n     to produce a gridded ``xarray.DataArray`` (``temp_data1``).\n\n7. **Convert to NetCDF and Send to Server**\n\n     - Invoke: ``ncFile = grd2netcdf1(temp_data1, fileOut, filesUsed, my_mask, \"MB\")`` from ``roylib``: \n\n     - Masks out land using ``my_mask`` (set land to NaN).\n\n     - Computes coverage metrics (# observations, % coverage).\n\n     - Uses a CDL template to generate an empty CF-compliant NetCDF via ``ncgen``.\n\n     - Populates coordinates, SST values, metadata, and a center-time stamp.\n\n     - Returns the final NetCDF filename (``MB&lt;year&gt;&lt;doy&gt;_&lt;year&gt;&lt;doy&gt;_sstd.nc``).\n\n   - Call:\n\n     ::\n\n          send_to_servers(ncFile, \"/MB/sstd/\", \"1\")\n\n     to copy the NetCDF into ``/MB/sstd/1day/``.\n\n     Finally, remove the local NetCDF copy.\n\nDependencies\n------------\n- **Python 3.x**\n\n- **Standard library:** ``os``, ``glob``, ``re``, ``shutil``, ``sys``, ``datetime``, ``timedelta``, ``chain``\n\n- **Third-party**: ``netCDF4.Dataset``, ``numpy``, ``numpy.ma``, ``pygmt``  \n\n- **Custom roylib functions:**\n\n   - ``myReshape(array)``\n  \n   - ``grd2netcdf1(grd, outName, filesUsed, mask, fType)``\n\n   - ``safe_remove(filePath)``  \n\n   - ``send_to_servers(ncFile, destDir, interval)``\n\n   - ``isleap(year)``\n\n   - ``makeNetcdf(mean, nobs, interval, outFile, filesUsed, workDir)``\n\n   - ``meanVar(mean, num, obs)``\n\nLand Mask\n---------\n- A static GRD file is required at:\n\n  ``/u00/ref/landmasks/LM_120_320_0.025_-45_65_0.025_gridline.grd``\n\nDirectory Structure\n-------------------\n- **Input swaths directory (Day N)**:\n\n  ``&lt;dataDirBase&gt;/&lt;YYYY&gt;&lt;MM&gt;/`` containing files named ``AQUA_MODIS.&lt;YYYY&gt;&lt;MM&gt;&lt;DD&gt;T*.L2.SST.NRT.nc``.\n\n- **Input swaths directory (Day N+1)**:\n  \n  ``&lt;dataDirBase&gt;/&lt;YYYY1&gt;&lt;MM1&gt;/`` containing the early-hour swaths.\n\n- **Working directory** (workDir): \n  \n  Temporary location where swaths are copied, processed, and removed.\n\n- **Output grid** (fileOut):  \n  \n  A GMT “.grd” file named ``MB&lt;year&gt;&lt;doy&gt;_&lt;year&gt;&lt;doy&gt;_sstd.grd``.\n\n- **Final NetCDF** (returned by ``grd2netcdf1``): \n  \n  Named ``MB&lt;year&gt;&lt;doy&gt;_&lt;year&gt;&lt;doy&gt;_sstd.nc``, then copied to ``/MB/sstd/1day/``.\n\nUsage Example\n-------------\nAssume your raw SST swaths are in ``/Users/you/modis_data/netcdf/202503/`` and your working directory is ``/Users/you/modis_work/``\n\n::\n\n    python makeSST1daynewMB.py /Users/you/modis_data/netcdf/ \\\n                              /Users/you/modis_work/ \\\n                              2025 082\n\nThis will:\n\n  - Copy all swaths from March 23 with HOD &gt; 10 into “/Users/you/modis_work/”.\n\n  - Copy early swaths from March 24 with HOD ≤ 10 into “/Users/you/modis_work/”.\n\n  - Build a combined point cloud from valid “Day” pixels for 120-320°, lat -45-65°).\n\n  - Create “MB2025082_2025082_sstd.grd” via PyGMT ``xyz2grd``.\n\n  - Convert the grid to “MB2025082_2025082_sstd.nc” using ``grd2netcdf1``.\n\n  - Copy the NetCDF to “/MB/sstd/1day/” and remove local copies.\n\"\"\"\nfrom __future__ import print_function\nfrom builtins import str\n\nif __name__ == \"__main__\":\n    from datetime import datetime, timedelta\n    import glob\n    from itertools import chain\n    from netCDF4 import Dataset\n    import numpy as np\n    import numpy.ma as ma\n    import pygmt\n    import os\n    import re\n    import shutil\n    import sys\n\n    # Ensure 'roylib' is on the import path\n    sys.path.append('/home/cwatch/pythonLibs')\n    from roylib import *\n\n    # Geographic bounds for MW region\n    latmax = 65.\n    latmin = -45.\n    lonmax = 320.\n    lonmin = 120.\n\n    outFile = 'modisgfSSTtemp'\n\n    # Set data directory\n    datadirBase = sys.argv[1]\n\n    # Set work directory\n    workdir = sys.argv[2]\n\n    # Get the year and doy from the command line\n    year = sys.argv[3]\n    doy = sys.argv[4]\n    print(year)\n    print(doy)\n\n    # Convert year/doy to a calendar date and zero-pad month/day\n    myDate = datetime(int(year), 1, 1) + timedelta(int(doy) - 1)\n    myMon = str(myDate.month)\n    myMon = myMon.rjust(2, '0')\n    myDay = str(myDate.day).rjust(2, '0')\n\n    # Directory for day N's raw swaths: &lt;datadirBase&gt;/&lt;year&gt;/&lt;myMon&gt;/\n    datadir = datadirBase + year + myMon + '/'\n\n    # Compute actual calendar date from year + day\n    myDate1 = myDate + timedelta(days=1)\n    year1 = str(myDate1.year)\n    doy1 = myDate1.strftime(\"%j\").zfill(3)\n    myMon1 = str(myDate1.month)\n    myMon1 = myMon1.rjust(2, '0')\n    myDay1 = str(myDate1.day).rjust(2, '0')\n    print(myDate)\n    print(myDate1)\n    print(year1)\n    print(myMon1)\n\n    # Directory for day N + 1's raw swaths: &lt;datadirBase&gt;/&lt;year1&gt;/&lt;myMon1&gt;/\n    datadir1 = datadirBase + year1 + myMon1 + '/'\n\n    # Load static land mask grid from GRD\n    mask_root = Dataset('/u00/ref/landmasks/LM_120_320_0.025_-45_65_0.025_gridline.grd')\n    my_mask = mask_root.variables['z'][:, :]\n    mask_root.close()\n\n    # Now move to the data directory\n    os.chdir(datadir)\n\n    # Set up the string for the file search in the data directory\n    myString = 'AQUA_MODIS.' + year + myMon + myDay + '*.L2.SST.NRT.nc'\n    print(myString)\n\n    # Get list of files in the data directory that match with full path\n    fileList = glob.glob(myString)\n    # print(fileList)\n    fileList.sort()\n\n    # Now move to the work directory and clear old files\n    os.chdir(workdir)\n    os.system('rm -f AQUA_MODIS.*L2.SST*')\n    os.system('rm -f MB20*')\n\n    # Prepare variables to accumulate point-cloud data and track provenance\n    temp_data = None\n    filesUsed = \"\"\n\n    # Loop through each swath filename for Day N\n    for fName in fileList:\n        fileName = fName\n\n        # Extract Hour-Of-Day (hod) from filename via\n        datetime = re.search('AQUA_MODIS.(.+?).L2.SST.NRT.nc', fileName)\n        hodstr = datetime.group(1)[9:11]\n        hod = int(hodstr)\n\n        # Only process swaths acquired after hod &gt; 10 \n        if (hod &gt; 10):\n            print(hod)\n            print(fileName)\n\n            # Copy swath from datadir into workdir for processing\n            shutil.copyfile(datadir + fName, workdir + fName)\n\n            # Attempt to open the NetCDF; skip if unreadable\n            try:\n                rootgrp = Dataset(fileName, 'r')\n            except IOError:\n                print(\"bad file \" + fileName)\n                continue\n\n            # Extract navigation-group data\n            navDataGroup = rootgrp.groups['navigation_data']\n            latitude = navDataGroup.variables['latitude'][:, :]\n            longitude = navDataGroup.variables['longitude'][:, :]\n\n            # Convert any negative longitudes to the 0-360° domain\n            longitude[longitude &lt; 0] = longitude[longitude &lt; 0] + 360\n\n            # Compute swath extents (min/max) for geographic filtering\n            dataLonMin = np.nanmin(longitude[longitude &gt;= 0])\n            dataLonMax = np.nanmax(longitude[longitude &lt;= 360])\n            dataLatMin = np.nanmin(latitude[latitude &gt;= -90])\n            dataLatMax = np.nanmax(latitude[latitude &lt;= 90])\n\n            # Determine if swath overlaps our MB region (lon 120-320, lat -45-65)\n            goodLon1 = (dataLonMin &lt; lonmin) and (dataLonMax &gt;= lonmin)\n            goodLon2 = (dataLonMin &gt;= lonmin) and (dataLonMin &lt;= lonmax)\n            goodLon = goodLon1 or goodLon2\n\n            goodLat1 = (dataLatMin &lt; latmin) and (dataLatMax &gt;= latmin)\n            goodLat2 = (dataLatMin &gt;= latmin) and (dataLatMin &lt;= latmax)\n            goodLat = goodLat1 or goodLat2\n\n            # Check if swath is daytime (only keep \"Day\" pixels)\n            dayNightTest = (rootgrp.day_night_flag == 'Day')\n\n            # Only proceed if geography and day-night tests pass\n            if (goodLon and goodLat and dayNightTest):\n                # Add filename to provenance list\n                if (len(filesUsed) == 0):\n                    filesUsed = fileName\n                else:\n                    filesUsed = filesUsed + ', ' + fileName\n\n                # Extract geophysical data and reshape\n                longitude = myReshape(longitude)\n                latitude = myReshape(latitude)\n                geoDataGroup = rootgrp.groups['geophysical_data']\n                sst = geoDataGroup.variables['sst'][:, :]\n                sst = myReshape(sst)\n                qual_sst = geoDataGroup.variables['qual_sst'][:, :]\n                qual_sst = myReshape(qual_sst)\n\n                # Filter out low-quality pixels (qual_sst &lt; 3)\n                qualTest = qual_sst &lt; 3\n                qualTest = qual_sst &lt; 3\n                qualTest = qualTest.flatten()\n\n                # Stack (lon, lat, sst) into a single 2D array with shape (N, 3)\n                dataOut = np.hstack((longitude, latitude, sst))\n\n                # Apply quality mask, valid SST &gt; -2°C and geographic bounds\n                dataOut = dataOut[qualTest]\n                dataOut = dataOut[dataOut[:, 2] &gt; -2]\n                dataOut = dataOut[dataOut[:, 0] &gt; -400]\n                dataOut = dataOut[dataOut[:, 0] &gt;= lonmin]\n                dataOut = dataOut[dataOut[:, 0] &lt;= lonmax]\n                dataOut = dataOut[dataOut[:, 1] &lt;= latmax]\n\n                # Accumulate into temp_data array\n                if (dataOut.shape[0] &gt; 0):\n                    if (temp_data is None):\n                        temp_data = dataOut\n                    else:\n                        temp_data = np.concatenate((temp_data, dataOut), axis=0)\n\n            # Close the NetCDF and remove swath from workdir\n            rootgrp.close()\n            safe_remove(fileName)\n\n    # Process swaths for Day N+1 (hod ≤ 10)\n    # Move back to data dir\n    print(datadir1)\n    os.chdir(datadir1)\n\n    # Set up the string for file matching of doy+1\n    myString = 'AQUA_MODIS.' + year1 + myMon1 + myDay1  + '*.L2.SST.NRT.nc'\n    fileList = glob.glob(myString)\n    fileList.sort()\n\n    # Switch back to workdir for processing\n    os.chdir(workdir)\n    for fName in fileList:\n        fileName = fName\n\n        # Extract hod again\n        datetime = re.search('AQUA_MODIS.(.+?).L2.SST.NRT.nc', fileName)\n        hodstr = datetime.group(1)[9:11]\n        hod = int(hodstr)\n\n        # Only process early swaths (hod ≤ 10) from Day N+1\n        if (hod &lt;= 10):\n            print(hod)\n            print(fileName)\n\n            # cp file from work directory to\n            shutil.copyfile(datadir1 + fName, workdir + fName)\n            try:\n                rootgrp = Dataset(fileName, 'r')\n            except IOError:\n                print(\"bad file \" + fileName)\n                continue\n\n            navDataGroup = rootgrp.groups['navigation_data']\n            latitude = navDataGroup.variables['latitude'][:, :]\n            longitude = navDataGroup.variables['longitude'][:, :]\n            longitude[longitude &lt; 0] = longitude[longitude &lt; 0] + 360\n            dataLonMin = np.nanmin(longitude[longitude &gt;= 0])\n            dataLonMax = np.nanmax(longitude[longitude &lt;= 360])\n            dataLatMin = np.nanmin(latitude[latitude &gt;= -90] )\n            dataLatMax = np.nanmax(latitude[latitude &lt;= 90] )\n\n            goodLon1 = (dataLonMin &lt; lonmin) and ( dataLonMax &gt;= lonmin)\n            goodLon2 = (dataLonMin &gt;= lonmin) and (dataLonMin &lt;= lonmax)\n            goodLon = goodLon1 or goodLon2\n            goodLat1 = (dataLatMin &lt; latmin) and (dataLatMax &gt;= latmin)\n            goodLat2 = (dataLatMin &gt;= latmin) and (dataLatMin &lt;= latmax)\n            goodLat = goodLat1 or goodLat2\n\n            dayNightTest = (rootgrp.day_night_flag == 'Day')\n\n            if (goodLon and goodLat and dayNightTest):\n                if (len(filesUsed) == 0):\n                    filesUsed = fileName\n                else:\n                    filesUsed = filesUsed + ', ' + fileName\n\n                longitude = myReshape(longitude)\n                latitude = myReshape(latitude)\n                geoDataGroup = rootgrp.groups['geophysical_data']\n                sst = geoDataGroup.variables['sst'][:, :]\n                sst = myReshape(sst)\n                qual_sst = geoDataGroup.variables['qual_sst'][:, :]\n                qual_sst = myReshape(qual_sst)\n                qualTest = qual_sst &lt; 3\n                qualTest = qualTest.flatten()\n\n                dataOut = np.hstack((longitude, latitude, sst))\n                dataOut = dataOut[qualTest]\n                dataOut = dataOut[dataOut[:, 2] &gt; -2]\n                dataOut = dataOut[dataOut[:, 0] &gt; -400]\n                dataOut = dataOut[dataOut[:, 0] &gt;= lonmin]\n                dataOut = dataOut[dataOut[:, 0] &lt;= lonmax]\n                dataOut = dataOut[dataOut[:, 1] &gt;= latmin]\n                dataOut = dataOut[dataOut[:, 1] &lt;= latmax]\n    \n                if (dataOut.shape[0] &gt; 0):\n                    if (temp_data is None):\n                        temp_data = dataOut\n                    else:\n                        temp_data = np.concatenate((temp_data, dataOut), axis=0)\n\n            # Close the NetCDF and remove swath from workdir\n            rootgrp.close()\n            safe_remove(fileName)\n\n    # Build GMT grid from accumulated point cloud\n    # Define output grid filename for PyGMT\n    fileOut = 'MB' + year + doy + '_' + year + doy + '_sstd.grd'\n    range = '120/320/-45/65'\n    increment = '0.025/0.025'\n\n    # Use PyGMT xyz2grd to interpolate scattered (lon, lat, sst) into a regular grid\n    temp_data1 = pygmt.xyz2grd(\n        data=temp_data,\n        region=range,\n        spacing=increment,\n    )\n\n    # Convert GMT grid to CF-compliant NetCDF and send to server\n    ncFile = grd2netcdf1(temp_data1, fileOut, filesUsed, my_mask, 'MB')\n\n    #myCmd = \"mv \" + ncFile + \" /home/cwatch/pygmt_test/outfiles\"\n    #os.system(myCmd)\n\n    # Copy the resulting NetCDF to the \"MB\" server folder for 1-day products\n    send_to_servers(ncFile, '/MB/sstd/', '1')\n\n    # Remove local NetCDF copy from working directory\n    os.remove(ncFile)\n\n\n\n\nCompMBSST\nCompMBSST.py makes the 3, 5, 8, 14-day SST composites by averaging 1-day MB SST NetCDF files over user-specified interval of days, accumulates running sum and count arrays on a 0.025° × 0.025° regular grid spanning longitude 120°–320° and latitude –45°–65°. It accumulates running sums and counts, masks out unobserved pixels, writes a CF-compliant NetCDF, and uploads the result to a directory on the remote server. Find the multi-day products on ERDDAP here.\nThe following chart summarizes the script’s workflow:\n\n\n\n\n\n%%{init: {\"flowchart\":{\"htmlLabels\":true}}}%%\nflowchart LR\n  I(\"&lt;b&gt;Inputs&lt;/b&gt;&lt;br/&gt;• 1-day SST NetCDFs (dataDir)&lt;br/&gt;• workDir&lt;br/&gt;• endYear & endDoy&lt;br/&gt;• interval (days)\")\n    --&gt; P(\"&lt;b&gt;Processing&lt;/b&gt;&lt;br/&gt;• Compute start/end dates&lt;br/&gt;• Initialize mean/num arrays&lt;br/&gt;• Gather daily files&lt;br/&gt;• Loop: open each file, extract & squeeze variable&lt;br/&gt;• Update running mean & count&lt;br/&gt;• Mask pixels with zero observations\")\n    --&gt; O(\"&lt;b&gt;Output&lt;/b&gt;&lt;br/&gt;• CF-compliant multi-day composite SST NetCDF&lt;br/&gt;• Deployed to /MB/sstd/\")\n\n\n\n\n\n\n\n\nView CompMBSST.py\n\"\"\"\nOverview\n--------\nCompute multi-day SST composites for the MODIS “MB” (Pacific Ocean) dataset by averaging\ndaily 1-day NetCDF SST files over a specified interval of days (e.g., 3, 5, 8, or 14).\n\nUsage\n-----\n::\n  \n    python CompMBSST.py &lt;dataDir&gt; &lt;workDir&gt; &lt;endYear&gt; &lt;endDoy&gt; &lt;interval&gt;\n\nWhere:\n\n- ``dataDir``\n\n  Directory containing daily 1-day SST NetCDF files named ``MB&lt;YYYY&gt;&lt;DDD&gt;*sstd.nc``.\n\n- ``workDir``\n\n  Temporary working directory for intermediate files.\n\n- ``endYear``\n\n  Four-digit year of the last day in the composite (e.g., ``2025``).\n\n- ``endDoy``\n\n  Three-digit day-of-year of the last day (e.g., ``082``).\n\n- ``interval``\n\n  Number of days to include (e.g., `3`, `5`, `8`, `14`).\n\nDescription\n-----------\n1. **Parse arguments & compute interval**\n\n  - Read ``dataDir``, ``workDir``, ``endYear``, ``endDoy``, and ``interval`` from ``sys.argv``.\n\n  - Compute ``startDoy = endDoy - interval + 1`` and zero-pad DOY strings.\n\n2. **Compute calendar dates**\n\n  - Convert ``endYear`` + ``endDoy`` to a ``datetime`` (``myDateEnd``).\n  \n  - Compute ``myDateStart = myDateEnd - (interval - 1)`` days.\n\n3. **Initialize accumulators**\n\n  - Clear ``workDir`` of old files.\n\n  - Preallocate ``mean`` and ``num`` arrays of shape (4401, 8001) for running sum and counts.\n\n4. **Gather daily files**\n\n  - If ``startDoy ≤ endDoy``, collect files for DOYs ``startDoy…endDoy``; else handle wrap-around at year end by collecting ``startDoy…yearEnd`` and ``1…endDoy``.\n\n5. **Accumulate SST**\n\n  - For each NetCDF:\n\n     - Open via ``Dataset()``.\n\n     - Read 4-D variable ``MBsstd``, squeeze to 2-D array.\n\n     - Update ``mean, num`` with ``meanVar(mean, num, sst)``.\n\n6. **Mask and finalize**\n\n  - Mask pixels with zero observations (``num == 0``), fill with ``-9999999.``.\n\n7. **Write & deploy**\n\n  - Change back to ``workDir``.\n\n  - Build ``outFile = MB&lt;startYear&gt;&lt;startDoy&gt;_&lt;endYear&gt;&lt;endDoy&gt;_sstd.nc``.\n\n  - Call ``makeNetcdf(mean, num, interval, outFile, filesUsed, workDir)`` to write CF-compliant NetCDF.\n\n  - Upload via ``send_to_servers(ncFile, '/MB/sstd/', str(interval))``.\n\nDependencies\n------------\n- **Python 3.x**\n\n- **Standard library:** ``os``, ``sys``, ``glob``, ``itertools.chain``, ``datetime``, ``timedelta``\n\n- **Third-party:** ``numpy``, ``numpy.ma``, ``netCDF4.Dataset``\n\n- **Custom roylib functions:**\n\n  - ``isleap(year)``\n\n  - ``meanVar(mean, num, array)``\n\n  - ``makeNetcdf(mean, num, interval, outFile, filesUsed, workDir)``\n\n  - ``send_to_servers(ncFile, remote_dir, interval_flag)``\n\nDirectory Structure\n-------------------\n- **Input Directory** (``dataDir``):\n\n  Contains daily files ``MB&lt;YYYY&gt;&lt;DDD&gt;*sstd.nc``.\n\n- **Working Directory** (``workDir``):\n\n  Temporary staging and output location.\n\n- **Output location** (remote):\n\n  Copied to ``/MB/sstd/`` with the interval flag.\n\nUsage Example\n-------------\nCreate a 5-day SST composite from DOY 001 to 005 of 2025:\n\n::\n \n   python CompMBSST.py /data/modisgf/1day/ /tmp/mw_work/ 2025 005 5\n\nThis will average files MB2025001*… through MB2025005*, write ``MB2025001_2025005_sstd.nc`` in ``/tmp/mw_work/``, and upload to ``/MB/sstd/``.\n\"\"\"\nfrom __future__ import print_function\nfrom builtins import str\nfrom builtins import range\n\nif __name__ == \"__main__\":\n    from datetime import datetime, timedelta\n    import glob\n    from itertools import chain\n    from netCDF4 import Dataset\n    import numpy as np\n    import numpy.ma as ma\n    import os\n    import sys\n\n    # Ensure 'roylib' is on the import path\n    sys.path.append('/home/cwatch/pythonLibs')\n    from roylib import *\n\n    # Directory with 1-day SST NetCDFs\n    dataDir = sys.argv[1]\n\n    # Temporary working directory\n    workDir = sys.argv[2]\n\n    # Composite end year (YYYY)\n    endyearC = sys.argv[3]\n\n    # Composite end day-of-year (DDD)\n    endDoyC = sys.argv[4]\n    endDoyC = endDoyC.rjust(3, '0')\n\n    # Integer form of end day-of-year\n    endDoy = int(endDoyC)\n\n    # Composite length as string\n    intervalC = sys.argv[5]\n    interval = int(intervalC)\n\n    # Convert end Doy to calendar date\n    myDateEnd = datetime(int(endyearC), 1, 1) + timedelta(int(endDoyC) - 1)\n\n    # Start date = end date minus (interval-1) days\n    myDateStart = myDateEnd + timedelta(days=-(interval - 1))\n\n    # Zero-padded start Doy and year\n    startDoyC = myDateStart.strftime(\"%j\").zfill(3)\n    startDoy = int(startDoyC)\n    startYearC = str(myDateStart.year)\n\n    # Prepare output directory\n    outDir = '/ERDData1/modisa/data/modisgf/' + endyearC + '/' + intervalC + 'day'\n\n    print(dataDir)\n    print(workDir)\n    print(endyearC)\n    print(endDoyC)\n    print(intervalC)\n    print(startYearC)\n    print(startDoyC)\n\n    ###\n    #cdtypeList = ['sstd']\n    # for dtype in dtypeList:\n\n    # Data type for SST composites\n    dtype = 'sstd'\n\n    # Clean working directory\n    os.chdir(workDir)\n    os.system('rm -f *')\n\n    # Change to data directory\n    os.chdir(dataDir)\n\n    # Preallocate sum (mean) and count arrays matching grid dims\n    mean =np.zeros((4401, 8001), np.single)\n    num = np.zeros((4401, 8001), dtype=np.int32)\n\n    # Composite within the same calendar year\n    if (endDoy &gt; startDoy):\n        # Build range of Doys\n        doyRange = list(range(startDoy, endDoy + 1))\n        fileList = []\n        # Gather filenames for each DOY\n        for doy in doyRange:\n            doyC = str(doy)\n            doyC = doyC.rjust(3, '0')\n            myString = 'MB' + endyearC + doyC + '*' + dtype + '.nc'\n            fileList.append(glob.glob(myString))\n\n        # Flatten and sort file list\n        fileList = list(chain.from_iterable(fileList))\n        fileList.sort()\n        filesUsed = \"\"\n        print(fileList)\n\n        # Loop through files and accumulate SST\n        for fName in fileList:\n            if (len(filesUsed) == 0):\n                filesUsed = fName\n            else:\n                filesUsed = filesUsed + ', ' + fName\n\n            sstFile = Dataset(fName)\n            sst = sstFile.variables[\"MBsstd\"][:, :, :, :]\n            sstFile.close()\n            sst = np.squeeze(sst)\n            mean, num = meanVar(mean, num, sst)\n    else:\n        # Composite spans year boundary\n        # Determine directory for the start year\n        dataDir1 = dataDir\n        dataDir1 = dataDir1.replace(endyearC, startYearC)\n        # Determine end of start year based on start year\n        if (isleap):\n            endday = 366\n        else:\n            endday = 365\n\n        # A DOY from startDoy to end of start year\n        fileList = []\n        print(dataDir1)\n        os.chdir(dataDir1)\n        doyRange = list(range(startDoy, endday + 1))\n        for doy in doyRange:\n            doyC = str(doy)\n            doyC = doyC.rjust(3, '0')\n            myString = 'MB' + startYearC + doyC + '*' + dtype + '.nc'\n            fileList.append(glob.glob(myString))\n\n        fileList = list(chain.from_iterable(fileList))\n        fileList.sort()\n        filesUsed = \"\"\n        print(fileList)\n        for fName in fileList:\n            if (len(filesUsed) == 0):\n                filesUsed = fName\n            else:\n                filesUsed = filesUsed + ', ' + fName\n\n            sstFile = Dataset(fName)\n            sst = sstFile.variables[\"MBsstd\"][:, :, :, :]\n            sstFile.close()\n            sst = np.squeeze(sst)\n            mean, num = meanVar(mean, num, sst)\n\n        # DOY from 1 to endDoy of end year\n        os.chdir(dataDir)\n        fileList = []\n        doyRange = list(range(1, endDoy + 1))\n        print(doyRange)\n        for doy in doyRange:\n            doyC = str(doy)\n            doyC = doyC.rjust(3, '0')\n            myString = 'MB' + endyearC + doyC + '*' + dtype + '.nc'\n            fileList.append(glob.glob(myString))\n\n        fileList = list(chain.from_iterable(fileList))\n        fileList.sort()\n        print(fileList)\n        for fName in fileList:\n            if (len(filesUsed) == 0):\n                filesUsed = fName\n            else:\n                filesUsed = filesUsed + ', ' + fName\n\n            sstFile = Dataset(fName)\n            sst = sstFile.variables[\"MBsstd\"][:, :, :, :]\n            sstFile.close()\n            sst = np.squeeze(sst)\n            mean, num = meanVar(mean, num, sst)\n\n    # Mask out any grid cells with zero observations, setting them to the fill value\n    mean = ma.array(mean, mask=(num == 0), fill_value=-9999999.)\n\n    # Switch to the working directory for output operations\n    os.chdir(workDir)\n\n    # Construct the output filename with start and end dates plus data types\n    outFile = 'MB' + startYearC + startDoyC + '_' + endyearC + endDoyC + '_' + dtype + '.nc'\n    print(interval)\n\n    # Create multi-day NetCDF file using the mean and count arrays\n    ncFile = makeNetcdf(mean, num, interval, outFile, filesUsed, workDir)\n\n    # Upload composite NetCDF to remote MB SST directory, labeling it with the interval\n    send_to_servers(ncFile, '/MB/sstd/', str(interval))\n\n\n\n\nCompMBSSTmday\nCompMBSSTmday.py reads daily 1-day SST NetCDF files for a user-specified date range and accumulates running sum and count arrays on a 0.025° × 0.025° regular grid spanning longitude 120°–320° and latitude –45°–65°. It then masks out any grid cells with zero observations and writes the averaged monthly composite as a CF-compliant NetCDF, which is automatically uploaded to a directory on the remote server. Find the monthly product on ERDDAP here.\nThe following chart summarizes the script’s workflow:\n\n\n\n\n\n%%{init: { \"flowchart\": { \"htmlLabels\": true } }}%%\nflowchart LR\n  I(\"&lt;b&gt;Inputs&lt;/b&gt;&lt;br/&gt;• 1-day SST NetCDFs (dataDir)&lt;br/&gt;• workDir&lt;br/&gt;• endYear & endDoy&lt;br/&gt;• startDoy\")\n    --&gt; P(\"&lt;b&gt;Processing&lt;/b&gt;&lt;br/&gt;• Parse args & compute interval&lt;br/&gt;• Compute start/end dates&lt;br/&gt;• Clear workDir&lt;br/&gt;• Initialize mean & count arrays&lt;br/&gt;• Gather files&lt;br/&gt;• Loop: read each file & update mean/num&lt;br/&gt;• Mask pixels with no observations\")\n    --&gt; O(\"&lt;b&gt;Output&lt;/b&gt;&lt;br/&gt;• Monthly composite SST NetCDF&lt;br/&gt;• Deployed to /MB/sstd/\")\n\n\n\n\n\n\n\n\nView CompMBSSTmday.py\n\"\"\"\nOverview\n--------\nGenerate **monthly** SST composites for the MODIS “MB” (Pacific Ocean) dataset by averaging\ndaily 1-day NetCDF files over a user-defined range of days (typically a calendar month).\n\nUsage\n-----\n::\n\n    python CompMBSSTmday.py &lt;dataDir&gt; &lt;workDir&gt; &lt;endYear&gt; &lt;endDoy&gt; &lt;startDoy&gt;\n\nWhere:\n\n- ``dataDir``\n\n  Directory containing the daily 1-day SST NetCDFs named ``MB&lt;YYYY&gt;&lt;DDD&gt;*sstd.nc``.\n\n- ``workDir``\n\n  Temporary working directory for intermediate files\n\n- ``endYear``\n\n  Four-digit year of the last day in the composite (e.g., ``2025``).\n\n- ``endDoy``\n\n  Three-digit day-of-year of the last composite day (e.g., ``031``).\n\n- ``startDoy``\n\n  Three-digit day-of-year of the first composite day (e.g., ``001``).\n\nDescription\n-----------\n1. **Parse arguments & compute interval**\n\n  - Read ``dataDir``, ``workDir``, ``endYear``, ``endDoy``, and ``startDoy`` from ``sys.argv``.\n\n  - Compute ``interval = endDoy - startDoy + 1``, the number of days in the composite.\n\n2. **Compute start/end dates**\n\n  - Convert ``endYear`+`endDoy`` to a ``datetime`` for logging.\n\n  - Derive ``startDoy`` date by subtracting ``interval - 1`` days.\n\n3. **Initialize accumulators**\n\n  - Change to ``workDir`` and clear any old files.\n\n  - Preallocate two arrays of shape 4401x8001:\n\n     - ``mean`` (float32) for the running sum of SST.\n\n     - ``num``  (int32) for the count of observations.\n\n4. **Gather daily files**\n\n  - Build a sorted list of all ``MB&lt;YYYY&gt;&lt;DDD&gt;*sstd.nc`` files from ``startDoy`` to ``endDoy``, handling wrap-around at year boundaries if needed.\n\n5. **Accumulate daily SST**\n\n  - For each file:\n\n     - Open via ``Dataset()``, read the 4-D variable ``MBsstd``, squeeze to 2-D.\n\n     - Update ``mean`` and ``num`` via ``meanVar(mean, num, sst2d)``.\n\n6. **Mask and finalize**\n\n  - Create a masked array: mask pixels where ``num==0`` (no observations) and set fill value ``-9999999.``.\n\n7. **Write & deploy**\n\n  - Change to ``workDir``.\n\n  - Construct an output filename ``MB&lt;YYYY&gt;&lt;startDDD&gt;_&lt;YYYY&gt;&lt;endDDD&gt;_sstd_mday.nc``.\n\n  - Call ``makeNetcdfmDay(mean, num, interval, outFile, filesUsed, workDir)`` to produce a CF-compliant NetCDF.\n  \n  - Transfer the file to ``/MB/sstd/`` on the remote server via ``send_to_servers()``.\n\nDependencies\n------------\n- **Python 3.x**\n\n- **Standard library:** ``os``, ``sys``, ``glob``, ``itertools.chain``, ``datetime``, ``timedelta``\n\n- **Third-party:** ``numpy``, ``numpy.ma``, ``netCDF4.Dataset``\n\n- **Custom roylib functions:**\n\n  - ``isleap(year)``\n\n  - ``meanVar(mean, num, data)``\n\n  - ``makeNetcdfmDay(mean, num, interval, outFile, filesUsed, workDir)``\n\n  - ``send_to_servers(ncFile, remote_dir, interval_flag)``\n\nDirectory Structure\n-------------------\n- **Input Directory** (``dataDir``):\n\n  ``MB&lt;YYYY&gt;&lt;DDD&gt;*sstd.nc`` daily SST files for the year.\n\n- **Working Directory**  (``workDir``):\n\n  Temporary staging for intermediate and final files.\n\n- **Output Location**:\n\n  Monthly composite written to ``workDir`` then copied to ``/MB/sstd/``.\n\nUsage Example\n-------------\nCreate a January 2025 composite (DOY 001-031):\n::\n \n   python CompMBSSTmday.py /data/modisgf/1day/ /tmp/mb_work/ 2025 031 001\n\nThis averages daily SST files DOY 001…031 of 2025, writes ``MB2025001_2025031_sstd_mday.nc`` in ``/tmp/mb_work/``, and uploads to ``/MB/sstd/``.\n\"\"\"\nfrom __future__ import print_function\nfrom builtins import str\nfrom builtins import range\n\nif __name__ == \"__main__\":\n    from datetime import datetime, timedelta\n    import glob\n    from itertools import chain\n    from netCDF4 import Dataset\n    import numpy as np\n    import numpy.ma as ma\n    import os\n    import sys\n\n    # Ensure 'roylib' is on the import path\n    sys.path.append('/home/cwatch/pythonLibs')\n    from roylib import *\n\n    # Directory of daily 1-day NetCDFs\n    dataDir = sys.argv[1]\n\n    # Temporary working directory\n    workDir = sys.argv[2]\n\n    # End date (year, day-of-year)\n    endyearC = sys.argv[3]\n    endDoyC = sys.argv[4]\n    endDoyC = endDoyC.rjust(3, '0')\n\n    # Start day-of-year (same year)\n    startYearC = endyearC\n    endDoy = int(endDoyC)\n    startDoyC = sys.argv[5]\n    startDoyC = startDoyC.rjust(3, '0')\n    startDoy = int(startDoyC )\n\n    # Number of days in the composite\n    interval = endDoy - startDoy + 1\n\n    # Convert end date to calendar dates\n    myDateEnd = datetime(int(endyearC), 1, 1) + timedelta(endDoy - 1)\n    myDateStart = myDateEnd + timedelta(days=-(interval - 1))\n\n    # Prepare output directory\n    outDir = '/ERDData1/modisa/data/modisgf/' + endyearC + '/mday'\n\n    print(dataDir)\n    print(workDir)\n    print(endyearC)\n    print(endDoyC)\n    print(interval)\n\n    ###\n    #cdtypeList = ['sstd']\n    # for dtype in dtypeList:\n\n    # Only SST variable for MB composites\n    dtype = 'sstd'\n\n    # Clean working directory\n    os.chdir(workDir)\n    os.system('rm -f *')\n\n    # Move to data directory\n    os.chdir(dataDir)\n\n    # Preallocate sum (mean) and count arrays matching grid dims\n    mean = np.zeros((4401, 8001), np.single)\n    num = np.zeros((4401, 8001), dtype=np.int32)\n\n    # Build list of NetCDF files spanning startDoy..endDoy\n    if (endDoy &gt; startDoy):\n        # Composite entirely within the same year\n        doyRange = list(range(startDoy, endDoy + 1))\n        print(doyRange)\n\n        # Build list matching files for each DOY\n        fileList = []\n        for doy in doyRange:\n            doyC = str(doy)\n            doyC = doyC.rjust(3, '0')\n            myString = 'MB' + endyearC + doyC + '*' + dtype + '.nc'\n            fileList.append(glob.glob(myString))\n\n        # Flatten and sort\n        fileList = list(chain.from_iterable(fileList))\n        fileList.sort()\n        filesUsed = \"\"\n        print(fileList)\n\n        # Loop through each file\n        for fName in fileList:\n            # Build provenance string\n            if (len(filesUsed) == 0):\n                filesUsed = fName\n            else:\n                filesUsed = filesUsed + ', ' + fName\n\n            # Read SST variable and accumulate\n            sstFile = Dataset(fName)\n            sst = sstFile.variables[\"MBsstd\"][:, :, :, :]\n            sstFile.close()\n            sst = np.squeeze(sst)\n\n            # Update running mean and count arrays\n            mean, num = meanVar(mean, num, sst)\n    else:\n        # Composite spans year boundary\n        dataDir1 = dataDir\n        dataDir1 = dataDir1.replace(endyearC, startYearC)\n\n        # Determine last day of start year\n        if (isleap):\n            endday = 366\n        else:\n            endday = 365\n\n        # From startDoy through end of start year  \n        fileList = []\n        os.chdir(dataDir1)\n        doyRange = list(range(startDoy, endday + 1))\n        for doy in doyRange:\n            doyC = str(doy)\n            doyC = doyC.rjust(3, '0')\n            myString = 'MB' + startYearC + doyC + '*' + dtype + '.nc'\n            fileList.append(glob.glob(myString))\n\n        fileList = list(chain.from_iterable(fileList))\n        fileList.sort()\n        fileUsed = \"\"\n        print(fileList)\n        for fName in fileList:\n            if (len(filesUsed) == 0):\n                filesUsed = fName\n            else:\n                filesUsed = filesUsed + ', ' + fName\n\n            sstFile = Dataset(fName)\n            sst = sstFile.variables[\"MBsstd\"][:, :, :, :]\n            sstFile.close()\n            sst = np.squeeze(sst)\n            mean, num = meanVar(mean, num, sst)\n\n        # From DOY 1 of end year through endDoy\n        os.chdir(dataDir)\n        fileList = []\n        doyRange = list(range(1, endDoy + 1))\n        for doy in doyRange:\n            doyC = str(doy)\n            doyC = doyC.rjust(3, '0')\n            myString = 'MB' + endyearC + doyC + '*' + dtype + '.nc'\n            fileList.append(glob.glob(myString))\n\n        fileList = list(chain.from_iterable(fileList))\n        fileList.sort()\n        print(fileList)\n        for fName in fileList:\n            if (len(filesUsed) == 0):\n                filesUsed = fName\n            else:\n                filesUsed = filesUsed + ', ' + fName\n\n            sstFile = Dataset(fName)\n            sst = sstFile.variables[\"MBsstd\"][:, :, :, :]\n            sstFile.close()\n            sst = np.squeeze(sst)\n            mean, num = meanVar(mean,num,sst)\n\n    # Mask out pixels with zero observations and set fill value for missing data\n    mean = ma.array(mean, mask=(num == 0), fill_value=-9999999.)\n\n    # Switch to the working directory\n    os.chdir(workDir)\n\n    # Construct output filename\n    outFile = 'MB' + startYearC + startDoyC + '_' + endyearC + endDoyC + '_' + dtype + '.nc'\n\n    # Generate the multi-day NetCDF file\n    ncFile = makeNetcdfmDay(mean, num, interval, outFile, filesUsed, workDir)\n\n    # Send the NetCDF to the remote server directory\n    send_to_servers(ncFile, '/MB/sstd/' , 'm')",
    "crumbs": [
      "MODISA Scripts",
      "MB Datasets"
    ]
  },
  {
    "objectID": "modisa_scripts.html",
    "href": "modisa_scripts.html",
    "title": "Roy’s MODISA Scripts",
    "section": "",
    "text": "Welcome to Roy’s MODISA Scripts - the central page for the newGetModis.py orchestrator.\nnewGetModis.py leverages the helper functions in roylib.py to download MODISA NRT Level 2 files to create the 1-, 3-, 8-, 14- day composites for the MB (Pacific Ocean) and MW (West Coast) datasets.\nTo find a more detailed explanation of this script, see the API Reference.\nThe ERDDAP products created by this script are located here on the SWFSC ERDDAP Server.\nThe newGetModis.py script downloads the last four days of MODIS Level-2 SST and Chlorophyll-a files, then runs the appropriate processing pipelines to build both daily and rolling composites for MB and MW products. It flags which days actually acquired new data, invokes the 1-day scripts (make*1day*.py), and then triggers the multi-day composites (Comp* scripts) for 3-, 5-, 8-, and 14-day periods. For full script details, see the newGetModis API reference here.\n\n\nView newGetModis.py\n\"\"\"\nOverview\n--------\nDownload MODIS Near-Real-Time (NRT) Level-2 SST and Chla files for the last four days,\nthen generate 1-, 3-, 8-, 14- day composites for both MB (Pacific Ocean) and MW (West Coast) products.\n\nUsage\n-----\n::\n\n    python newGetModis.py &lt;baseDataDir&gt;\n\nWhere:\n\n- ``&lt;baseDataDir&gt;``\n\n  Root directory containing subfolders named ``&lt;YYYY&gt;&lt;MM&gt;`` (e.g., “202503”), each holding raw Level-2 NetCDF files.\n\nDescription\n-----------\n1. **Scan the past four days** (lags -3, -2, -1, 0 relative to now):\n\n  - Compute ``YYYY``, ``MM``, ``DDD`` for each lag.\n\n  - Change into ``&lt;baseDataDir&gt;/&lt;YYYY&gt;&lt;MM&gt;/``.\n\n  - Call ``retrieve_new_files(..., 'SST', ...)`` to fetch missing SST L2 files.\n\n  - Call ``retrieve_new_files(..., 'OC', ...)`` to fetch missing ocean-color (Chla) L2 files.\n\n2. **Record which days had new data**\n\n  - Two boolean arrays ``newSST`` and ``newOC`` track when new files were downloaded.\n\n3. **Generate 1-day composites**\n\n  - For each day flagged true, run external scripts for MB and MW variants:\n\n    - MB (Pacific Ocean): ``makeSST1daynewMB.py`` / ``makeChla1daynewMB.py``\n\n    - MW (West Coast): ``makeSST1daynewMW.py`` / ``makeChla1daynewMW.py``\n\n4. **Generate multi-day composites**\n\n  - For intervals “3”, “5”, “8”, “14”: if any day in the preceding window had new data, invoke MB composite scripts (``CompMBSST.py``, ``CompMBChla.py``) and MW composite scripts (``CompMWSST.py``, ``CompMWChla.py``).\n\nDependencies\n------------\n- **Python 3.x** \n\n- **Standard library:** ``os``, ``sys``, ``datetime``, ``timedelta``\n\n- **Custom roylib functions:**\n\n  - ``retrieve_new_files(dataDir, param, year, doy, flag_list, lag)`` \n\n  - ``update_modis_1day(now, baseDir, param, flag_list)`` \n\n  - ``update_modis_composite(now, baseDir, param, flag_list, composite_interval)``\n\nDirectory Structure\n-------------------\nAssuming ``&lt;baseDataDir&gt;`` is ``/path/to/modis/netcdf/``:\n\n    /path/to/modis/netcdf/\n\n        202501/    # Raw L2 files for January 2025\n\n        202502/    # Raw L2 files for February 2025\n\n        202503/    # Raw L2 files for March 2025\n        …\n\nEach ``&lt;YYYYMM&gt;/`` folder contains files like:\n\n    AQUA_MODIS.20250321T000001.L2.SST.NRT.nc\n    AQUA_MODIS.20250321T000001.L2.OC.NRT.nc\n    …\n\nAfter running:\n\n- **1-day composites** saved in:\n    /path/to/modis/data/modiswc/1day/      # MW-processed SST/Chla\n    /path/to/modis/data/modisgf/1day/      # MB-processed SST/Chla\n\n- **Multi-day composites** saved in:\n    /path/to/modis/data/modiswc/&lt;N&gt;day/    # MW composites (3,5,8,14)\n    /path/to/modis/data/modisgf/&lt;N&gt;day/    # MB composites (3,5,8,14)\n\nUsage Example\n-------------\n::\n  \n   python newGetModis.py /Users/yourname/Roy/modis_data/netcdf\n\nThis will:\n\n  - Download missing L2 SST/Chla files for the last four days.\n\n  - Create 1-day SST and Chla products where new data arrived.\n\n  - Produce 3-, 5-, 8-, and 14-day composites for both MW and MB datasets.\n\"\"\"\nfrom __future__ import print_function\nfrom builtins import str\nfrom builtins import range\nif __name__ == \"__main__\":\n    from datetime import date, datetime, timedelta\n    import os\n    import sys\n\n    # Ensure 'roylib' is on the import path\n    sys.path.append('/home/cwatch/pythonLibs')\n    from roylib import *\n\n    # Base directory for raw L2 NetCDF files (arg 1)\n    basedataDir = sys.argv[1]\n\n    # Current timestamp\n    now = datetime.now()\n\n    # Flags to track which of the last 4 days have new SST or OC data\n    newSST = ([False, False, False, False])\n    newOC = ([False, False, False, False])\n\n    # Loop over the past 4 calendar days: lags -3, -2, -1, 0\n    for lag in list(range(-3, 1)):\n        myDate1 = now + timedelta(days=lag)\n        myYear = str(myDate1.year)\n        myMon = str(myDate1.month)\n        myMon = myMon.rjust(2, '0')\n        doy = myDate1.strftime(\"%j\").zfill(3)\n        print('doy: ' + str(doy))\n\n        # Directory where raw L2 files for this YYYYMM live\n        dataDir = basedataDir + myYear + myMon\n        print(dataDir)\n        os.chdir(dataDir)\n\n        # Fetch SST L2 files if missing\n        print('start retrieve SST')\n        retrieve_new_files(dataDir, 'SST', myYear, doy, newSST, lag)\n        print('done retrieve SST')\n\n        # get SST data\n        # modisSSTURL = '\"https://oceandata.sci.gsfc.nasa.gov/search/file_search.cgi?search=A' + myYear + doy + '*L2_LAC_SST.nc&dtype=L2&sensor=aqua&results_as_file=1\"'\n        # modisSSTURL = 'search=A' + myYear + doy + '*L2_LAC_SST.nc&dtype=L2&sensor=aqua&results_as_file=1'\n        # fileList = url_lines(modisSSTURL)\n        # for fName in fileList[2:]:\n            # fileTest = os.path.isfile(dataDir + '/' + fName)\n            # if(not(fileTest)):\n                # newSST[lag + 3] = True\n                # print(fName)\n                # get_netcdfFile(fName)\n\n        # Fetch Chla (OC) L2 files if missing\n        print('start retrieve OC')\n        retrieve_new_files(dataDir, 'OC', myYear, doy, newOC, lag)\n        print('Done retrieve OC')\n\n        # get OC data\n        # modisOCURL = '\"https://oceandata.sci.gsfc.nasa.gov/search/file_search.cgi?search=A' + myYear + doy + '*L2_LAC_OC.nc&dtype=L2&sensor=aqua&results_as_file=1\"'\n        # modisOCURL = 'search=A' + myYear + doy + '*L2_LAC_OC.nc&dtype=L2&sensor=aqua&results_as_file=1'\n        # fileList = url_lines(modisOCURL)\n        # print(fileList[2:])\n        # for fName in fileList[2:]:\n            # fileTest = os.path.isfile(dataDir + '/' + fName)\n            # if(not(fileTest)):\n                # newOC[lag + 3] = True\n                # print(fName)\n                # get_netcdfFile(fName)\n\n    # Print which days actually had new downloads\n    print('newSST')\n    print(newSST)\n    print('newOC')\n    print(newOC)\n\n    # Generate 1-day composties for SST and Chla\n    update_modis_1day(now, basedataDir, 'SST', newSST)\n    update_modis_1day(now, basedataDir, 'Chla', newOC)\n\n    # Generate 3-day composties for SST and Chla\n    update_modis_composite(now, basedataDir, 'SST', newSST, '3')\n    update_modis_composite(now, basedataDir, 'Chla', newOC, '3')\n\n    # Generate 5-day composties for SST and Chla\n    update_modis_composite(now, basedataDir, 'SST', newSST, '5')\n    update_modis_composite(now, basedataDir, 'Chla', newOC, '5')\n\n    # Generate 8-day composties for SST and Chla\n    update_modis_composite(now, basedataDir, 'SST', newSST, '8')\n    update_modis_composite(now, basedataDir, 'Chla', newOC, '8')\n\n    # Generate 14-day composties for SST and Chla\n    update_modis_composite(now, basedataDir, 'SST', newSST, '14')\n    update_modis_composite(now, basedataDir, 'Chla', newOC, '14')",
    "crumbs": [
      "MODISA Scripts"
    ]
  },
  {
    "objectID": "modisa_scripts.html#data-fetch-orchestration",
    "href": "modisa_scripts.html#data-fetch-orchestration",
    "title": "Roy’s MODISA Scripts",
    "section": "",
    "text": "Welcome to Roy’s MODISA Scripts - the central page for the newGetModis.py orchestrator.\nnewGetModis.py leverages the helper functions in roylib.py to download MODISA NRT Level 2 files to create the 1-, 3-, 8-, 14- day composites for the MB (Pacific Ocean) and MW (West Coast) datasets.\nTo find a more detailed explanation of this script, see the API Reference.\nThe ERDDAP products created by this script are located here on the SWFSC ERDDAP Server.\nThe newGetModis.py script downloads the last four days of MODIS Level-2 SST and Chlorophyll-a files, then runs the appropriate processing pipelines to build both daily and rolling composites for MB and MW products. It flags which days actually acquired new data, invokes the 1-day scripts (make*1day*.py), and then triggers the multi-day composites (Comp* scripts) for 3-, 5-, 8-, and 14-day periods. For full script details, see the newGetModis API reference here.\n\n\nView newGetModis.py\n\"\"\"\nOverview\n--------\nDownload MODIS Near-Real-Time (NRT) Level-2 SST and Chla files for the last four days,\nthen generate 1-, 3-, 8-, 14- day composites for both MB (Pacific Ocean) and MW (West Coast) products.\n\nUsage\n-----\n::\n\n    python newGetModis.py &lt;baseDataDir&gt;\n\nWhere:\n\n- ``&lt;baseDataDir&gt;``\n\n  Root directory containing subfolders named ``&lt;YYYY&gt;&lt;MM&gt;`` (e.g., “202503”), each holding raw Level-2 NetCDF files.\n\nDescription\n-----------\n1. **Scan the past four days** (lags -3, -2, -1, 0 relative to now):\n\n  - Compute ``YYYY``, ``MM``, ``DDD`` for each lag.\n\n  - Change into ``&lt;baseDataDir&gt;/&lt;YYYY&gt;&lt;MM&gt;/``.\n\n  - Call ``retrieve_new_files(..., 'SST', ...)`` to fetch missing SST L2 files.\n\n  - Call ``retrieve_new_files(..., 'OC', ...)`` to fetch missing ocean-color (Chla) L2 files.\n\n2. **Record which days had new data**\n\n  - Two boolean arrays ``newSST`` and ``newOC`` track when new files were downloaded.\n\n3. **Generate 1-day composites**\n\n  - For each day flagged true, run external scripts for MB and MW variants:\n\n    - MB (Pacific Ocean): ``makeSST1daynewMB.py`` / ``makeChla1daynewMB.py``\n\n    - MW (West Coast): ``makeSST1daynewMW.py`` / ``makeChla1daynewMW.py``\n\n4. **Generate multi-day composites**\n\n  - For intervals “3”, “5”, “8”, “14”: if any day in the preceding window had new data, invoke MB composite scripts (``CompMBSST.py``, ``CompMBChla.py``) and MW composite scripts (``CompMWSST.py``, ``CompMWChla.py``).\n\nDependencies\n------------\n- **Python 3.x** \n\n- **Standard library:** ``os``, ``sys``, ``datetime``, ``timedelta``\n\n- **Custom roylib functions:**\n\n  - ``retrieve_new_files(dataDir, param, year, doy, flag_list, lag)`` \n\n  - ``update_modis_1day(now, baseDir, param, flag_list)`` \n\n  - ``update_modis_composite(now, baseDir, param, flag_list, composite_interval)``\n\nDirectory Structure\n-------------------\nAssuming ``&lt;baseDataDir&gt;`` is ``/path/to/modis/netcdf/``:\n\n    /path/to/modis/netcdf/\n\n        202501/    # Raw L2 files for January 2025\n\n        202502/    # Raw L2 files for February 2025\n\n        202503/    # Raw L2 files for March 2025\n        …\n\nEach ``&lt;YYYYMM&gt;/`` folder contains files like:\n\n    AQUA_MODIS.20250321T000001.L2.SST.NRT.nc\n    AQUA_MODIS.20250321T000001.L2.OC.NRT.nc\n    …\n\nAfter running:\n\n- **1-day composites** saved in:\n    /path/to/modis/data/modiswc/1day/      # MW-processed SST/Chla\n    /path/to/modis/data/modisgf/1day/      # MB-processed SST/Chla\n\n- **Multi-day composites** saved in:\n    /path/to/modis/data/modiswc/&lt;N&gt;day/    # MW composites (3,5,8,14)\n    /path/to/modis/data/modisgf/&lt;N&gt;day/    # MB composites (3,5,8,14)\n\nUsage Example\n-------------\n::\n  \n   python newGetModis.py /Users/yourname/Roy/modis_data/netcdf\n\nThis will:\n\n  - Download missing L2 SST/Chla files for the last four days.\n\n  - Create 1-day SST and Chla products where new data arrived.\n\n  - Produce 3-, 5-, 8-, and 14-day composites for both MW and MB datasets.\n\"\"\"\nfrom __future__ import print_function\nfrom builtins import str\nfrom builtins import range\nif __name__ == \"__main__\":\n    from datetime import date, datetime, timedelta\n    import os\n    import sys\n\n    # Ensure 'roylib' is on the import path\n    sys.path.append('/home/cwatch/pythonLibs')\n    from roylib import *\n\n    # Base directory for raw L2 NetCDF files (arg 1)\n    basedataDir = sys.argv[1]\n\n    # Current timestamp\n    now = datetime.now()\n\n    # Flags to track which of the last 4 days have new SST or OC data\n    newSST = ([False, False, False, False])\n    newOC = ([False, False, False, False])\n\n    # Loop over the past 4 calendar days: lags -3, -2, -1, 0\n    for lag in list(range(-3, 1)):\n        myDate1 = now + timedelta(days=lag)\n        myYear = str(myDate1.year)\n        myMon = str(myDate1.month)\n        myMon = myMon.rjust(2, '0')\n        doy = myDate1.strftime(\"%j\").zfill(3)\n        print('doy: ' + str(doy))\n\n        # Directory where raw L2 files for this YYYYMM live\n        dataDir = basedataDir + myYear + myMon\n        print(dataDir)\n        os.chdir(dataDir)\n\n        # Fetch SST L2 files if missing\n        print('start retrieve SST')\n        retrieve_new_files(dataDir, 'SST', myYear, doy, newSST, lag)\n        print('done retrieve SST')\n\n        # get SST data\n        # modisSSTURL = '\"https://oceandata.sci.gsfc.nasa.gov/search/file_search.cgi?search=A' + myYear + doy + '*L2_LAC_SST.nc&dtype=L2&sensor=aqua&results_as_file=1\"'\n        # modisSSTURL = 'search=A' + myYear + doy + '*L2_LAC_SST.nc&dtype=L2&sensor=aqua&results_as_file=1'\n        # fileList = url_lines(modisSSTURL)\n        # for fName in fileList[2:]:\n            # fileTest = os.path.isfile(dataDir + '/' + fName)\n            # if(not(fileTest)):\n                # newSST[lag + 3] = True\n                # print(fName)\n                # get_netcdfFile(fName)\n\n        # Fetch Chla (OC) L2 files if missing\n        print('start retrieve OC')\n        retrieve_new_files(dataDir, 'OC', myYear, doy, newOC, lag)\n        print('Done retrieve OC')\n\n        # get OC data\n        # modisOCURL = '\"https://oceandata.sci.gsfc.nasa.gov/search/file_search.cgi?search=A' + myYear + doy + '*L2_LAC_OC.nc&dtype=L2&sensor=aqua&results_as_file=1\"'\n        # modisOCURL = 'search=A' + myYear + doy + '*L2_LAC_OC.nc&dtype=L2&sensor=aqua&results_as_file=1'\n        # fileList = url_lines(modisOCURL)\n        # print(fileList[2:])\n        # for fName in fileList[2:]:\n            # fileTest = os.path.isfile(dataDir + '/' + fName)\n            # if(not(fileTest)):\n                # newOC[lag + 3] = True\n                # print(fName)\n                # get_netcdfFile(fName)\n\n    # Print which days actually had new downloads\n    print('newSST')\n    print(newSST)\n    print('newOC')\n    print(newOC)\n\n    # Generate 1-day composties for SST and Chla\n    update_modis_1day(now, basedataDir, 'SST', newSST)\n    update_modis_1day(now, basedataDir, 'Chla', newOC)\n\n    # Generate 3-day composties for SST and Chla\n    update_modis_composite(now, basedataDir, 'SST', newSST, '3')\n    update_modis_composite(now, basedataDir, 'Chla', newOC, '3')\n\n    # Generate 5-day composties for SST and Chla\n    update_modis_composite(now, basedataDir, 'SST', newSST, '5')\n    update_modis_composite(now, basedataDir, 'Chla', newOC, '5')\n\n    # Generate 8-day composties for SST and Chla\n    update_modis_composite(now, basedataDir, 'SST', newSST, '8')\n    update_modis_composite(now, basedataDir, 'Chla', newOC, '8')\n\n    # Generate 14-day composties for SST and Chla\n    update_modis_composite(now, basedataDir, 'SST', newSST, '14')\n    update_modis_composite(now, basedataDir, 'Chla', newOC, '14')",
    "crumbs": [
      "MODISA Scripts"
    ]
  },
  {
    "objectID": "modisa_scripts.html#modisa-products-on-swfsc-erddap-server",
    "href": "modisa_scripts.html#modisa-products-on-swfsc-erddap-server",
    "title": "Roy’s MODISA Scripts",
    "section": "MODISA Products on SWFSC ERDDAP Server",
    "text": "MODISA Products on SWFSC ERDDAP Server\n\nMB (Pacific Ocean) Datasets\n\n\nSST\n\n\n\n1-Day Composite (ID: erdMBsstd1day)\n\n\n3-Day Composite (ID: erdMBsstd3day)\n\n\n5-Day Composite (ID: erdMBsstd5day)\n\n\n8-Day Composite (ID: erdMBsstd8day)\n\n\n14-Day Composite (ID: erdMBsstd14day)\n\n\nMonthly Composite (ID: erdMBsstdmday)\n\n\n\nChlorophyll-a\n\n\n\n1-Day Composite (ID: erdMBchla1day)\n\n\n3-Day Composite (ID: erdMBchla3day)\n\n\n5-Day Composite (ID: erdMBchla5day)\n\n\n8-Day Composite (ID: erdMBchla8day)\n\n\n14-Day Composite (ID: erdMBchla14day)\n\n\nMonthly Composite (ID: erdMBchlamday)\n\n\n\n\n\nMW (West Coast) Datasets\n\n\nSST\n\n\n\n1-Day Composite (ID: erdMWsstd1day)\n\n\n3-Day Composite (ID: erdMWsstd3day)\n\n\n8-Day Composite (ID: erdMWsstd8day)\n\n\n14-Day Composite (ID: erdMWsstd14day)\n\n\nMonthly Composite (ID: erdMWsstdmday)\n\n\n\nChlorophyll-a\n\n\n\n1-Day Composite (ID: erdMWchla1day)\n\n\n3-Day Composite (ID: erdMWchla3day)\n\n\n8-Day Composite (ID: erdMWchla8day)\n\n\n14-Day Composite (ID: erdMWchla14day)\n\n\nMonthly Composite (ID: erdMWchlamday)",
    "crumbs": [
      "MODISA Scripts"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Documentation of Roy’s Processing Scripts for SWFSC Data Servers",
    "section": "",
    "text": "This site documents Roy Mendelssohn’s suite of processing scripts for SWFSC Data servers.\nHere you’ll find:\n\nUtility Library (roylib.py): Library used by many of the Python scripts on saltydog.\nMODISA Processing Scripts The MODISA scripts take Level-2 (L2) MODIS-Aqua swath files for both MB (Pacific Ocean) and MW (West Coast) products, automatically downloading, filtering, gridding, and applying land masks and quality flags. They then generate CF-Compliant 1-, 3-, 8-, 14- day or monthly SST or Chlorophyll-a composite NetCDFs ready for deployment to ERDDAP.\n\nData Ingestion & Orchestration (newGetModis.py):\nFetch Level-2 files, track new data, and drive the 1-day, multi-day, and monthly composite workflows for the MB and MW datasets.\nMB Datasets (make*1daynewMB.py, CompMB*.py, CompMW*mday.py): Pacific Ocean SST & Chlorophyll-a scripts for making 1-, 3-, 8-, 14- day or monthly composites.\nMW Datasets (make*1daynewMW.py, CompMW*.py, CompMW*mday.py): West Coast SST & Chlorophyll-a scripts for making 1-, 3-, 8-, 14- day or monthly composites.\n\nFull API & Workflow Documentation: Sphinx-generated reference detailing every function’s docstring, script setup and configuration, command-line options, and an overview of the end-to-end workflow. View the full API Reference\n\nUse the left‐hand menu to jump to any section. Each script is tucked inside a foldable code block for easy browsing.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Documentation of Roy’s Processing Scripts for SWFSC Data Servers",
    "section": "",
    "text": "This site documents Roy Mendelssohn’s suite of processing scripts for SWFSC Data servers.\nHere you’ll find:\n\nUtility Library (roylib.py): Library used by many of the Python scripts on saltydog.\nMODISA Processing Scripts The MODISA scripts take Level-2 (L2) MODIS-Aqua swath files for both MB (Pacific Ocean) and MW (West Coast) products, automatically downloading, filtering, gridding, and applying land masks and quality flags. They then generate CF-Compliant 1-, 3-, 8-, 14- day or monthly SST or Chlorophyll-a composite NetCDFs ready for deployment to ERDDAP.\n\nData Ingestion & Orchestration (newGetModis.py):\nFetch Level-2 files, track new data, and drive the 1-day, multi-day, and monthly composite workflows for the MB and MW datasets.\nMB Datasets (make*1daynewMB.py, CompMB*.py, CompMW*mday.py): Pacific Ocean SST & Chlorophyll-a scripts for making 1-, 3-, 8-, 14- day or monthly composites.\nMW Datasets (make*1daynewMW.py, CompMW*.py, CompMW*mday.py): West Coast SST & Chlorophyll-a scripts for making 1-, 3-, 8-, 14- day or monthly composites.\n\nFull API & Workflow Documentation: Sphinx-generated reference detailing every function’s docstring, script setup and configuration, command-line options, and an overview of the end-to-end workflow. View the full API Reference\n\nUse the left‐hand menu to jump to any section. Each script is tucked inside a foldable code block for easy browsing.",
    "crumbs": [
      "Welcome"
    ]
  }
]